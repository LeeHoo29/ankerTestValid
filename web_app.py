#!/usr/bin/env python3
"""
ä»»åŠ¡æ£€æŸ¥å·¥å…·çœ‹æ¿ - Webç•Œé¢
ç»Ÿä¸€çš„ä»»åŠ¡ç®¡ç†å’Œç›‘æ§å¹³å°
æ”¯æŒå¤šç§å·¥å…·æ¨¡å—ï¼ŒåŒ…æ‹¬Azure Resource Readerç­‰
"""

import os
import subprocess
import threading
import time
import json
import hashlib
from datetime import datetime, timedelta, timezone
from flask import Flask, render_template, request, jsonify, session, send_file
from flask_cors import CORS
from pathlib import Path
import uuid
import logging

# å¯¼å…¥æ•°æ®åº“è¿æ¥å™¨å’Œé…ç½®
from src.db.connector import DatabaseConnector
from config.db_config import DB_CONFIG
from config.task_statistics_config import (
    DATABASE_TABLES, TASK_TYPES, TENANT_CONFIG, 
    get_sql_template, get_all_tenant_ids, TASK_STATISTICS_CONFIG
)

app = Flask(__name__)
app.secret_key = 'azure_resource_reader_web_2024'

# é…ç½®CORSï¼Œå…è®¸å‰ç«¯è·¨åŸŸè®¿é—®
CORS(app, origins=['http://localhost:3000', 'http://localhost:3001'])

# å…¨å±€å˜é‡å­˜å‚¨ä»»åŠ¡çŠ¶æ€
tasks = {}

# ç®€å•çš„å†…å­˜ç¼“å­˜æœºåˆ¶
statistics_cache = {}
CACHE_DURATION = 21600  # ç¼“å­˜6å°æ—¶ï¼ˆ6 * 60 * 60 = 21600ç§’ï¼‰

def get_utc_now():
    """è·å–å½“å‰UTCæ—¶é—´å­—ç¬¦ä¸²"""
    return datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S')

def convert_to_utc_datetime(date_str, time_str="00:00:00"):
    """
    å°†æ—¥æœŸå­—ç¬¦ä¸²è½¬æ¢ä¸ºUTCæ—¶é—´å­—ç¬¦ä¸²
    
    Args:
        date_str: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼å¦‚ "2025-05-27"
        time_str: æ—¶é—´å­—ç¬¦ä¸²ï¼Œæ ¼å¼å¦‚ "00:00:00" æˆ– "23:59:59"
    
    Returns:
        str: UTCæ—¶é—´å­—ç¬¦ä¸²ï¼Œæ ¼å¼å¦‚ "2025-05-27 00:00:00"
    """
    return f"{date_str} {time_str}"

def get_timeout_condition(reference_time=None):
    """
    è·å–è¶…æ—¶æŸ¥è¯¢æ¡ä»¶
    
    Args:
        reference_time: å‚è€ƒæ—¶é—´ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å½“å‰UTCæ—¶é—´
    
    Returns:
        tuple: (condition_sql, reference_time_value)
    """
    if reference_time is None:
        reference_time = get_utc_now()
    
    condition_sql = """break_at < %s
        AND (deliver_at IS NULL OR deliver_at > break_at)
        AND `status` != 'FAILED'"""
    
    return condition_sql.strip(), reference_time

def clean_sql_for_debug(sql_string):
    """
    æ¸…ç†SQLå­—ç¬¦ä¸²ï¼Œå»æ‰æ¢è¡Œç¬¦å’Œå¤šä½™ç©ºæ ¼ï¼Œæ–¹ä¾¿å¤åˆ¶
    """
    if not sql_string:
        return sql_string
    
    # å»æ‰æ¢è¡Œç¬¦ã€åˆ¶è¡¨ç¬¦å’Œå¤šä½™ç©ºæ ¼ï¼Œä¿æŒå•è¡Œæ ¼å¼
    cleaned = ' '.join(sql_string.replace('\n', ' ').replace('\t', ' ').split())
    return cleaned

def execute_command(task_id, command, working_dir):
    """åœ¨åå°æ‰§è¡Œå‘½ä»¤"""
    try:
        tasks[task_id]['status'] = 'running'
        tasks[task_id]['start_time'] = datetime.now()
        
        # æ‰§è¡Œå‘½ä»¤
        process = subprocess.Popen(
            command,
            shell=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            cwd=working_dir,
            bufsize=1,
            universal_newlines=True
        )
        
        # å®æ—¶è¯»å–è¾“å‡º
        output_lines = []
        while True:
            output = process.stdout.readline()
            if output == '' and process.poll() is not None:
                break
            if output:
                line = output.strip()
                output_lines.append(line)
                tasks[task_id]['output'] = '\n'.join(output_lines)
        
        # è·å–è¿”å›ç 
        return_code = process.poll()
        
        tasks[task_id]['status'] = 'completed' if return_code == 0 else 'failed'
        tasks[task_id]['return_code'] = return_code
        tasks[task_id]['end_time'] = datetime.now()
        
    except Exception as e:
        tasks[task_id]['status'] = 'error'
        tasks[task_id]['error'] = str(e)
        tasks[task_id]['end_time'] = datetime.now()

def format_timestamp(iso_timestamp):
    """æ ¼å¼åŒ–ISOæ—¶é—´æˆ³ä¸ºæ˜“è¯»æ ¼å¼"""
    try:
        if not iso_timestamp:
            return 'æœªçŸ¥æ—¶é—´'
        
        # è§£æISOæ ¼å¼æ—¶é—´æˆ³
        dt = datetime.fromisoformat(iso_timestamp.replace('Z', '+00:00'))
        
        # æ ¼å¼åŒ–ä¸ºæ˜“è¯»æ ¼å¼: å¹´-æœˆ-æ—¥ æ—¶:åˆ†:ç§’
        return dt.strftime('%Y-%m-%d %H:%M:%S')
    except Exception as e:
        # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›åŸå§‹å­—ç¬¦ä¸²
        return iso_timestamp

def read_completed_tasks():
    """è¯»å–å·²å®Œæˆä»»åŠ¡çš„æ˜ å°„æ–‡ä»¶"""
    mapping_file = Path('data/output/task_mapping.json')
    
    if not mapping_file.exists():
        return []
    
    try:
        with open(mapping_file, 'r', encoding='utf-8') as f:
            mapping_data = json.load(f)
        
        completed_tasks = []
        for job_id, task_info in mapping_data.items():
            # æ£€æŸ¥ä»»åŠ¡ç›®å½•æ˜¯å¦å­˜åœ¨
            relative_path = task_info.get('relative_path', '')
            if relative_path.startswith('./'):
                relative_path = relative_path[2:]
            
            full_path = Path('data/output') / relative_path
            
            # ç»Ÿè®¡æ–‡ä»¶æ•°é‡
            file_count = 0
            has_parse_file = False
            if full_path.exists():
                files = [f for f in full_path.iterdir() if f.is_file()]
                file_count = len(files)
                has_parse_file = any(f.name == 'parse_result.json' for f in files)
            
            completed_tasks.append({
                'job_id': job_id,
                'task_type': task_info.get('task_type', 'Unknown'),
                'actual_task_id': task_info.get('actual_task_id', ''),
                'last_updated': format_timestamp(task_info.get('last_updated', '')),
                'relative_path': relative_path,
                'full_path': str(full_path),
                'file_count': file_count,
                'has_parse_file': has_parse_file,
                'directory_exists': full_path.exists()
            })
        
        # æŒ‰æœ€åæ›´æ–°æ—¶é—´å€’åºæ’åˆ—
        completed_tasks.sort(key=lambda x: x['last_updated'], reverse=True)
        return completed_tasks
        
    except (json.JSONDecodeError, FileNotFoundError) as e:
        print(f"è¯»å–ä»»åŠ¡æ˜ å°„æ–‡ä»¶å¤±è´¥: {e}")
        return []

@app.route('/')
def index():
    """ä¸»é¡µé¢"""
    completed_tasks = read_completed_tasks()
    return render_template('index.html', completed_tasks=completed_tasks)

@app.route('/api/completed_tasks')
def get_completed_tasks():
    """è·å–å·²å®Œæˆä»»åŠ¡çš„APIæ¥å£"""
    completed_tasks = read_completed_tasks()
    return jsonify({
        'success': True,
        'tasks': completed_tasks,
        'total_count': len(completed_tasks)
    })

@app.route('/api/list_files')
def list_files():
    """åˆ—å‡ºæŒ‡å®šç›®å½•ä¸‹çš„æ–‡ä»¶"""
    try:
        dir_path = request.args.get('path')
        if not dir_path:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘è·¯å¾„å‚æ•°'})
        
        path = Path(dir_path)
        if not path.exists():
            return jsonify({'success': False, 'error': 'ç›®å½•ä¸å­˜åœ¨'})
        
        if not path.is_dir():
            return jsonify({'success': False, 'error': 'è·¯å¾„ä¸æ˜¯ç›®å½•'})
        
        files = []
        for file_path in path.iterdir():
            if file_path.is_file():
                stat = file_path.stat()
                files.append({
                    'name': file_path.name,
                    'path': str(file_path),
                    'size': stat.st_size,
                    'modified': datetime.fromtimestamp(stat.st_mtime).strftime('%Y-%m-%d %H:%M')
                })
        
        # æŒ‰åç§°æ’åº
        files.sort(key=lambda x: x['name'])
        
        return jsonify({
            'success': True,
            'files': files,
            'directory': str(path)
        })
        
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/download_file')
def download_file():
    """ä¸‹è½½æ–‡ä»¶"""
    try:
        file_path = request.args.get('path')
        if not file_path:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘æ–‡ä»¶è·¯å¾„å‚æ•°'})
        
        path = Path(file_path)
        if not path.exists():
            return jsonify({'success': False, 'error': 'æ–‡ä»¶ä¸å­˜åœ¨'})
        
        if not path.is_file():
            return jsonify({'success': False, 'error': 'è·¯å¾„ä¸æ˜¯æ–‡ä»¶'})
        
        # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿æ–‡ä»¶åœ¨data/outputç›®å½•ä¸‹
        data_output_path = Path('data/output').resolve()
        try:
            path.resolve().relative_to(data_output_path)
        except ValueError:
            return jsonify({'success': False, 'error': 'æ–‡ä»¶è®¿é—®æƒé™ä¸è¶³'})
        
        return send_file(path, as_attachment=True, download_name=path.name)
        
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/file_content')
def get_file_content():
    """è·å–æ–‡ä»¶å†…å®¹"""
    try:
        file_path = request.args.get('path')
        if not file_path:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘æ–‡ä»¶è·¯å¾„å‚æ•°'})
        
        path = Path(file_path)
        if not path.exists():
            return jsonify({'success': False, 'error': 'æ–‡ä»¶ä¸å­˜åœ¨'})
        
        if not path.is_file():
            return jsonify({'success': False, 'error': 'è·¯å¾„ä¸æ˜¯æ–‡ä»¶'})
        
        # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿æ–‡ä»¶åœ¨data/outputç›®å½•ä¸‹
        data_output_path = Path('data/output').resolve()
        try:
            path.resolve().relative_to(data_output_path)
        except ValueError:
            return jsonify({'success': False, 'error': 'æ–‡ä»¶è®¿é—®æƒé™ä¸è¶³'})
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹è¯»å–å†…å®¹
        file_ext = path.suffix.lower()
        
        if file_ext in ['.html', '.htm']:
            with open(path, 'r', encoding='utf-8') as f:
                content = f.read()
            return jsonify({
                'success': True,
                'content': content,
                'type': 'html',
                'filename': path.name
            })
        elif file_ext == '.json':
            with open(path, 'r', encoding='utf-8') as f:
                content = f.read()
            return jsonify({
                'success': True,
                'content': content,
                'type': 'json',
                'filename': path.name
            })
        elif file_ext == '.txt':
            with open(path, 'r', encoding='utf-8') as f:
                content = f.read()
            return jsonify({
                'success': True,
                'content': content,
                'type': 'text',
                'filename': path.name
            })
        else:
            return jsonify({'success': False, 'error': 'ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹'})
        
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/compare')
def compare_view():
    """å¯¹æ¯”æŸ¥çœ‹é¡µé¢"""
    task_path = request.args.get('path')
    if not task_path:
        return "ç¼ºå°‘ä»»åŠ¡è·¯å¾„å‚æ•°", 400
    
    # è·å–ä»»åŠ¡ç›®å½•ä¸‹çš„æ–‡ä»¶åˆ—è¡¨
    try:
        path = Path(task_path)
        if not path.exists() or not path.is_dir():
            return "ä»»åŠ¡ç›®å½•ä¸å­˜åœ¨", 404
        
        files = []
        for file_path in path.iterdir():
            if file_path.is_file():
                files.append({
                    'name': file_path.name,
                    'path': str(file_path),
                    'type': file_path.suffix.lower()
                })
        
        # æŒ‰ç±»å‹åˆ†ç»„
        html_files = [f for f in files if f['type'] in ['.html', '.htm']]
        json_files = [f for f in files if f['type'] == '.json']
        
        return render_template('compare.html', 
                             task_path=task_path,
                             html_files=html_files,
                             json_files=json_files)
        
    except Exception as e:
        return f"åŠ è½½å¤±è´¥: {str(e)}", 500

@app.route('/submit', methods=['POST'])
def submit_command():
    """æäº¤å‘½ä»¤æ‰§è¡Œ"""
    try:
        # è·å–è¡¨å•æ•°æ®
        task_type = request.form.get('task_type', '').strip()
        task_id_input = request.form.get('task_id', '').strip()
        output_type = request.form.get('output_type', 'html')
        use_parse = request.form.get('use_parse') == 'on'
        
        # éªŒè¯è¾“å…¥
        if not task_type or not task_id_input:
            return jsonify({
                'success': False,
                'error': 'è¯·å¡«å†™ä»»åŠ¡ç±»å‹å’Œä»»åŠ¡ID'
            })
        
        # æ„å»ºå‘½ä»¤
        command_parts = [
            'python3',
            'src/azure_resource_reader.py',
            task_type,
            task_id_input,
            output_type
        ]
        
        if use_parse:
            command_parts.append('--with-parse')
        
        command = ' '.join(command_parts)
        
        # ç”Ÿæˆä»»åŠ¡ID
        execution_id = str(uuid.uuid4())
        
        # åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€
        tasks[execution_id] = {
            'command': command,
            'status': 'pending',
            'output': '',
            'created_time': datetime.now(),
            'task_type': task_type,
            'task_id': task_id_input,
            'output_type': output_type,
            'use_parse': use_parse
        }
        
        # åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œå‘½ä»¤
        thread = threading.Thread(
            target=execute_command,
            args=(execution_id, command, os.getcwd())
        )
        thread.daemon = True
        thread.start()
        
        return jsonify({
            'success': True,
            'task_id': execution_id,
            'command': command
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'æäº¤å¤±è´¥: {str(e)}'
        })

@app.route('/status/<task_id>')
def get_status(task_id):
    """è·å–ä»»åŠ¡çŠ¶æ€"""
    if task_id not in tasks:
        return jsonify({
            'success': False,
            'error': 'ä»»åŠ¡ä¸å­˜åœ¨'
        })
    
    task = tasks[task_id]
    
    # è®¡ç®—æ‰§è¡Œæ—¶é—´
    duration = None
    if 'start_time' in task:
        end_time = task.get('end_time', datetime.now())
        duration = str(end_time - task['start_time'])
    
    return jsonify({
        'success': True,
        'status': task['status'],
        'output': task.get('output', ''),
        'command': task['command'],
        'created_time': task['created_time'].strftime('%Y-%m-%d %H:%M:%S'),
        'duration': duration,
        'return_code': task.get('return_code'),
        'error': task.get('error')
    })

@app.route('/tasks')
def list_tasks():
    """åˆ—å‡ºæ‰€æœ‰ä»»åŠ¡"""
    task_list = []
    for task_id, task in tasks.items():
        duration = None
        if 'start_time' in task:
            end_time = task.get('end_time', datetime.now())
            duration = str(end_time - task['start_time'])
        
        task_list.append({
            'id': task_id,
            'command': task['command'],
            'status': task['status'],
            'created_time': task['created_time'].strftime('%Y-%m-%d %H:%M:%S'),
            'duration': duration,
            'task_type': task.get('task_type'),
            'task_id': task.get('task_id'),
            'use_parse': task.get('use_parse', False)
        })
    
    # æŒ‰åˆ›å»ºæ—¶é—´å€’åºæ’åˆ—
    task_list.sort(key=lambda x: x['created_time'], reverse=True)
    
    return render_template('tasks.html', tasks=task_list)

@app.route('/clear_tasks', methods=['POST'])
def clear_tasks():
    """æ¸…ç©ºä»»åŠ¡å†å²"""
    global tasks
    tasks = {}
    return jsonify({'success': True})

@app.route('/api/check_task_exists')
def check_task_exists():
    """æ£€æŸ¥ä»»åŠ¡IDæ˜¯å¦å·²å­˜åœ¨"""
    try:
        task_id = request.args.get('task_id')
        if not task_id:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘ä»»åŠ¡IDå‚æ•°'})
        
        mapping_file = Path('data/output/task_mapping.json')
        
        if not mapping_file.exists():
            return jsonify({
                'success': True, 
                'exists': False,
                'task_id': task_id
            })
        
        with open(mapping_file, 'r', encoding='utf-8') as f:
            mapping_data = json.load(f)
        
        # æ£€æŸ¥ä»»åŠ¡IDæ˜¯å¦å­˜åœ¨
        exists = task_id in mapping_data
        task_info = mapping_data.get(task_id, {}) if exists else {}
        
        return jsonify({
            'success': True,
            'exists': exists,
            'task_id': task_id,
            'task_info': task_info
        })
        
    except Exception as e:
        return jsonify({
            'success': False, 
            'error': f'æ£€æŸ¥ä»»åŠ¡æ—¶å‡ºé”™: {str(e)}'
        })

@app.route('/api/statistics/config', methods=['GET'])
def get_statistics_config():
    """è·å–ç»Ÿè®¡é…ç½®ä¿¡æ¯"""
    try:
        return jsonify({
            'success': True,
            'data': {
                'task_types': TASK_TYPES,
                'tenants': TENANT_CONFIG,
                'database_tables': DATABASE_TABLES
            }
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})


@app.route('/api/statistics/data', methods=['POST'])
def get_statistics_data():
    """è·å–ä»»åŠ¡ç»Ÿè®¡æ•°æ®"""
    try:
        data = request.get_json()
        
        # è·å–å‚æ•°
        start_date = data.get('start_date')
        end_date = data.get('end_date')
        tenant_ids = data.get('tenant_ids', [])
        task_type = data.get('task_type', '')  # æ”¹ä¸ºå•ä¸ªä»»åŠ¡ç±»å‹
        refresh_timestamp = data.get('_refresh')  # åˆ·æ–°æ—¶é—´æˆ³
        
        # å‚æ•°éªŒè¯
        if not start_date or not end_date:
            return jsonify({'success': False, 'error': 'å¼€å§‹æ—¥æœŸå’Œç»“æŸæ—¥æœŸä¸èƒ½ä¸ºç©º'})
        
        if not tenant_ids:
            return jsonify({'success': False, 'error': 'è‡³å°‘é€‰æ‹©ä¸€ä¸ªç§Ÿæˆ·'})
        
        if not task_type:
            return jsonify({'success': False, 'error': 'è¯·é€‰æ‹©ä»»åŠ¡ç±»å‹'})
        
        # ç”Ÿæˆç¼“å­˜é”®ï¼ˆå¦‚æœæœ‰åˆ·æ–°æ—¶é—´æˆ³ï¼Œåˆ™åŒ…å«åœ¨ç¼“å­˜é”®ä¸­ä»¥ç»•è¿‡ç¼“å­˜ï¼‰
        cache_key_params = ['statistics_data', start_date, end_date, tenant_ids, task_type]
        if refresh_timestamp:
            cache_key_params.append(refresh_timestamp)
        cache_key = generate_cache_key(*cache_key_params)
        
        # æ£€æŸ¥ç¼“å­˜ï¼ˆå¦‚æœæ˜¯åˆ·æ–°è¯·æ±‚ï¼Œè·³è¿‡ç¼“å­˜æ£€æŸ¥ï¼‰
        if not refresh_timestamp:
            cached_result = get_from_cache(cache_key)
            if cached_result:
                return jsonify({
                    'success': True,
                    'data': cached_result['data'],
                    'from_cache': True,
                    'cache_time': cached_result['cache_time'],
                    '_debug': cached_result['debug_info']
                })
        
        # åˆ›å»ºæ•°æ®åº“è¿æ¥
        db_config = DB_CONFIG.copy()
        db_config['database'] = 'shulex_collector_prod'
        
        db = DatabaseConnector(db_config)
        if not db.connect():
            return jsonify({'success': False, 'error': 'æ•°æ®åº“è¿æ¥å¤±è´¥'})
        
        try:
            # ä½¿ç”¨ä¼˜åŒ–çš„æŸ¥è¯¢ç­–ç•¥
            statistics_data, debug_info = get_optimized_statistics_data(db, start_date, end_date, tenant_ids, task_type)
            
            # å­˜å…¥ç¼“å­˜
            set_to_cache(cache_key, statistics_data, debug_info)
            
            return jsonify({
                'success': True,
                'data': statistics_data,
                'cache_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                '_debug': debug_info
            })
            
        finally:
            db.disconnect()
            
    except Exception as e:
        logging.error(f"è·å–ç»Ÿè®¡æ•°æ®å¤±è´¥: {str(e)}")
        return jsonify({'success': False, 'error': str(e)})


def generate_cache_key(*args):
    """ç”Ÿæˆç¼“å­˜é”®"""
    key_string = '|'.join(str(arg) for arg in args)
    return hashlib.md5(key_string.encode()).hexdigest()


def get_from_cache(cache_key):
    """ä»ç¼“å­˜è·å–æ•°æ®"""
    if cache_key in statistics_cache:
        cache_entry = statistics_cache[cache_key]
        if time.time() - cache_entry['timestamp'] < CACHE_DURATION:
            return {
                'data': cache_entry['data'],
                'cache_time': cache_entry['cache_time'],
                'debug_info': cache_entry['debug_info']
            }
        else:
            # ç¼“å­˜è¿‡æœŸï¼Œåˆ é™¤
            del statistics_cache[cache_key]
    return None


def set_to_cache(cache_key, data, debug_info=None):
    """è®¾ç½®ç¼“å­˜æ•°æ®"""
    cache_time = datetime.now()
    statistics_cache[cache_key] = {
        'data': data,
        'timestamp': time.time(),
        'cache_time': cache_time.strftime('%Y-%m-%d %H:%M:%S'),
        'debug_info': debug_info or []
    }


def get_optimized_statistics_data(db, start_date, end_date, tenant_ids, task_type):
    """
    ä¼˜åŒ–çš„ç»Ÿè®¡æ•°æ®æŸ¥è¯¢
    ä½¿ç”¨å•ä¸ªæŸ¥è¯¢è·å–æ‰€æœ‰ç»Ÿè®¡æ•°æ®ï¼Œå‡å°‘æ•°æ®åº“å¾€è¿”æ¬¡æ•°
    """
    statistics_data = {
        'failed_count': [],
        'timeout_count': [],
        'total_count': [],
        'timeout_but_succeed': [],  # å·²è¶…æ—¶ä½†å·²å®Œæˆ
        'succeed_count': [],        # å·²å®Œæˆæ•°é‡
        'succeed_not_timeout': [],  # æœªè¶…æ—¶ä¸”å·²å®Œæˆæ•°é‡
        'timeout_not_succeed': []   # è¶…æ—¶æœªå®Œæˆæ•°é‡
    }
    
    debug_info = []
    
    # è¶…æ—¶åˆ¤æ–­åº”è¯¥ä½¿ç”¨å½“å‰UTCæ—¶é—´ï¼Œè€Œä¸æ˜¯æŸ¥è¯¢æ—¥æœŸ
    # è¿™æ ·å¯ä»¥åæ˜ æˆªè‡³å½“å‰æ—¶é—´çš„çœŸå®è¶…æ—¶çŠ¶æ€
    current_utc_time = get_utc_now()
    timeout_condition, _ = get_timeout_condition(current_utc_time)
    
    # è½¬æ¢æŸ¥è¯¢æ—¥æœŸä¸ºUTCæ—¶é—´
    utc_start_date = convert_to_utc_datetime(start_date, "00:00:00")
    utc_end_date = convert_to_utc_datetime(end_date, "23:59:59")
    
    # éå†æ‰€æœ‰åˆ†è¡¨
    for table in DATABASE_TABLES:
        # ä½¿ç”¨å•ä¸ªå¤åˆæŸ¥è¯¢è·å–æ‰€æœ‰ç»Ÿè®¡æ•°æ®
        optimized_sql = f"""
            SELECT 
                DATE(created_at) AS date,
                type as task_type,
                COUNT(*) as total_count,
                SUM(CASE WHEN `status` = 'FAILED' THEN 1 ELSE 0 END) as failed_count,
                SUM(CASE WHEN {timeout_condition} THEN 1 ELSE 0 END) as timeout_count,
                SUM(CASE WHEN `status` = 'SUCCEED' THEN 1 ELSE 0 END) as succeed_count,
                SUM(CASE WHEN ({timeout_condition}) AND `status` = 'SUCCEED' THEN 1 ELSE 0 END) as timeout_but_succeed,
                SUM(CASE WHEN NOT ({timeout_condition}) AND `status` = 'SUCCEED' THEN 1 ELSE 0 END) as succeed_not_timeout,
                SUM(CASE WHEN ({timeout_condition}) AND `status` != 'SUCCEED' AND `status` != 'FAILED' THEN 1 ELSE 0 END) as timeout_not_succeed
            FROM {table}
            WHERE created_at >= %s 
                AND created_at <= %s
                AND tenant_id IN ({','.join(['%s'] * len(tenant_ids))})
                AND type = %s
            GROUP BY date, task_type
            ORDER BY date DESC, task_type
        """
        
        # å‚æ•°ï¼šcurrent_utc_time (4æ¬¡), utc_start_date, utc_end_date, tenant_ids, task_type
        params = [current_utc_time, current_utc_time, current_utc_time, current_utc_time, utc_start_date, utc_end_date] + tenant_ids + [task_type]
        
        # è®°å½•è°ƒè¯•ä¿¡æ¯
        debug_info.append({
            'table': table,
            'sql': clean_sql_for_debug(optimized_sql),
            'params': params,
            'query_time': get_utc_now(),
            'timeout_reference_time': current_utc_time
        })
        
        # æ‰§è¡ŒæŸ¥è¯¢
        results = db.execute_query(optimized_sql, params)
        
        # åˆ†è§£ç»“æœåˆ°ä¸åŒçš„ç»Ÿè®¡ç±»å‹
        for result in results:
            date_str = str(result['date'])
            task_type = result['task_type']
            
            # æ·»åŠ è¡¨åä¿¡æ¯
            base_record = {
                'date': date_str,
                'task_type': task_type,
                'table': table
            }
            
            # åˆ†åˆ«æ·»åŠ åˆ°ä¸åŒçš„ç»Ÿè®¡ç±»å‹
            statistics_data['total_count'].append({
                **base_record,
                'count': result['total_count']
            })
            
            statistics_data['failed_count'].append({
                **base_record,
                'count': result['failed_count']
            })
            
            statistics_data['timeout_count'].append({
                **base_record,
                'count': result['timeout_count']
            })
            
            statistics_data['succeed_count'].append({
                **base_record,
                'count': result['succeed_count']
            })
            
            statistics_data['timeout_but_succeed'].append({
                **base_record,
                'count': result['timeout_but_succeed']
            })
            
            statistics_data['succeed_not_timeout'].append({
                **base_record,
                'count': result['succeed_not_timeout']
            })
            
            statistics_data['timeout_not_succeed'].append({
                **base_record,
                'count': result['timeout_not_succeed']
            })
    
    # æ•°æ®èšåˆå¤„ç†
    return aggregate_statistics_data(statistics_data), debug_info


def aggregate_statistics_data(raw_data):
    """
    èšåˆç»Ÿè®¡æ•°æ®
    
    Args:
        raw_data: åŸå§‹æ•°æ®å­—å…¸
        
    Returns:
        dict: èšåˆåçš„æ•°æ®
    """
    aggregated = {}
    
    for stat_type, records in raw_data.items():
        # æŒ‰æ—¥æœŸå’Œä»»åŠ¡ç±»å‹èšåˆ
        date_task_map = {}
        
        for record in records:
            date_str = str(record['date'])
            task_type = record['task_type']
            count = record['count']
            
            if date_str not in date_task_map:
                date_task_map[date_str] = {}
            
            if task_type not in date_task_map[date_str]:
                date_task_map[date_str][task_type] = 0
            
            date_task_map[date_str][task_type] += count
        
        # è½¬æ¢ä¸ºå‰ç«¯éœ€è¦çš„æ ¼å¼
        aggregated[stat_type] = []
        for date_str in sorted(date_task_map.keys(), reverse=True):
            for task_type, count in date_task_map[date_str].items():
                aggregated[stat_type].append({
                    'date': date_str,
                    'task_type': task_type,
                    'count': count
                })
    
    return aggregated


@app.route('/api/statistics/summary', methods=['POST'])
def get_statistics_summary():
    """è·å–ç»Ÿè®¡æ±‡æ€»æ•°æ®"""
    try:
        data = request.get_json()
        
        # è·å–å‚æ•°
        start_date = data.get('start_date')
        end_date = data.get('end_date')
        tenant_ids = data.get('tenant_ids', [])
        task_type = data.get('task_type', '')  # æ”¹ä¸ºå•ä¸ªä»»åŠ¡ç±»å‹
        refresh_timestamp = data.get('_refresh')  # åˆ·æ–°æ—¶é—´æˆ³
        
        # å‚æ•°éªŒè¯
        if not start_date or not end_date:
            return jsonify({'success': False, 'error': 'å¼€å§‹æ—¥æœŸå’Œç»“æŸæ—¥æœŸä¸èƒ½ä¸ºç©º'})
        
        if not tenant_ids:
            return jsonify({'success': False, 'error': 'è‡³å°‘é€‰æ‹©ä¸€ä¸ªç§Ÿæˆ·'})
        
        if not task_type:
            return jsonify({'success': False, 'error': 'è¯·é€‰æ‹©ä»»åŠ¡ç±»å‹'})
        
        # ç”Ÿæˆç¼“å­˜é”®ï¼ˆå¦‚æœæœ‰åˆ·æ–°æ—¶é—´æˆ³ï¼Œåˆ™åŒ…å«åœ¨ç¼“å­˜é”®ä¸­ä»¥ç»•è¿‡ç¼“å­˜ï¼‰
        cache_key_params = ['statistics_summary', start_date, end_date, tenant_ids, task_type]
        if refresh_timestamp:
            cache_key_params.append(refresh_timestamp)
        cache_key = generate_cache_key(*cache_key_params)
        
        # æ£€æŸ¥ç¼“å­˜ï¼ˆå¦‚æœæ˜¯åˆ·æ–°è¯·æ±‚ï¼Œè·³è¿‡ç¼“å­˜æ£€æŸ¥ï¼‰
        if not refresh_timestamp:
            cached_result = get_from_cache(cache_key)
            if cached_result:
                return jsonify({
                    'success': True,
                    'data': cached_result['data'],
                    'from_cache': True,
                    'cache_time': cached_result['cache_time'],
                    '_debug': cached_result['debug_info']
                })
        
        # åˆ›å»ºæ•°æ®åº“è¿æ¥
        db_config = DB_CONFIG.copy()
        db_config['database'] = 'shulex_collector_prod'
        
        db = DatabaseConnector(db_config)
        if not db.connect():
            return jsonify({'success': False, 'error': 'æ•°æ®åº“è¿æ¥å¤±è´¥'})
        
        try:
            summary_data = {}
            debug_info = []
            
            # è¶…æ—¶åˆ¤æ–­åº”è¯¥ä½¿ç”¨å½“å‰UTCæ—¶é—´ï¼Œè€Œä¸æ˜¯æŸ¥è¯¢æ—¥æœŸ
            # è¿™æ ·å¯ä»¥åæ˜ æˆªè‡³å½“å‰æ—¶é—´çš„çœŸå®è¶…æ—¶çŠ¶æ€
            current_utc_time = get_utc_now()
            timeout_condition, _ = get_timeout_condition(current_utc_time)
            
            # è½¬æ¢æŸ¥è¯¢æ—¥æœŸä¸ºUTCæ—¶é—´
            utc_start_date = convert_to_utc_datetime(start_date, "00:00:00")
            utc_end_date = convert_to_utc_datetime(end_date, "23:59:59")
            
            # éå†æ‰€æœ‰åˆ†è¡¨è·å–æ±‡æ€»æ•°æ®
            for table in DATABASE_TABLES:
                # æ±‡æ€»SQLæ¨¡æ¿ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
                summary_sql = f"""
                    SELECT 
                        type as task_type,
                        COUNT(*) as total_count,
                        SUM(CASE WHEN `status` = 'FAILED' THEN 1 ELSE 0 END) as failed_count,
                        SUM(CASE WHEN {timeout_condition} THEN 1 ELSE 0 END) as timeout_count,
                        SUM(CASE WHEN `status` = 'SUCCEED' THEN 1 ELSE 0 END) as succeed_count,
                        SUM(CASE WHEN ({timeout_condition}) AND `status` = 'SUCCEED' THEN 1 ELSE 0 END) as timeout_but_succeed,
                        SUM(CASE WHEN NOT ({timeout_condition}) AND `status` = 'SUCCEED' THEN 1 ELSE 0 END) as succeed_not_timeout,
                        SUM(CASE WHEN ({timeout_condition}) AND `status` != 'SUCCEED' AND `status` != 'FAILED' THEN 1 ELSE 0 END) as timeout_not_succeed
                    FROM {table}
                    WHERE created_at >= %s 
                        AND created_at <= %s
                        AND tenant_id IN ({','.join(['%s'] * len(tenant_ids))})
                        AND type = %s
                    GROUP BY task_type
                """
                
                # å‚æ•°ï¼šcurrent_utc_time (4æ¬¡), utc_start_date, utc_end_date, tenant_ids, task_type
                params = [current_utc_time, current_utc_time, current_utc_time, current_utc_time, utc_start_date, utc_end_date] + tenant_ids + [task_type]
                
                # è®°å½•è°ƒè¯•ä¿¡æ¯
                debug_info.append({
                    'table': table,
                    'sql': clean_sql_for_debug(summary_sql),
                    'params': params,
                    'query_time': get_utc_now(),
                    'timeout_reference_time': current_utc_time
                })
                
                results = db.execute_query(summary_sql, params)
                
                # èšåˆæ•°æ®
                for result in results:
                    task_type = result['task_type']
                    if task_type not in summary_data:
                        summary_data[task_type] = {
                            'total_count': 0,
                            'failed_count': 0,
                            'timeout_count': 0,
                            'succeed_count': 0,
                            'timeout_but_succeed': 0,
                            'succeed_not_timeout': 0,
                            'timeout_not_succeed': 0
                        }
                    
                    summary_data[task_type]['total_count'] += result['total_count']
                    summary_data[task_type]['failed_count'] += result['failed_count']
                    summary_data[task_type]['timeout_count'] += result['timeout_count']
                    summary_data[task_type]['succeed_count'] += result['succeed_count']
                    summary_data[task_type]['timeout_but_succeed'] += result['timeout_but_succeed']
                    summary_data[task_type]['succeed_not_timeout'] += result['succeed_not_timeout']
                    summary_data[task_type]['timeout_not_succeed'] += result['timeout_not_succeed']
            
            # å­˜å…¥ç¼“å­˜
            set_to_cache(cache_key, summary_data, debug_info)
            
            return jsonify({
                'success': True,
                'data': summary_data,
                'cache_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                '_debug': debug_info
            })
            
        finally:
            db.disconnect()
            
    except Exception as e:
        logging.error(f"è·å–æ±‡æ€»æ•°æ®å¤±è´¥: {str(e)}")
        return jsonify({'success': False, 'error': str(e)})


@app.route('/api/statistics/details', methods=['POST'])
def get_statistics_details():
    """è·å–ç»Ÿè®¡è¯¦ç»†æ•°æ®"""
    try:
        data = request.get_json()
        
        # éªŒè¯å¿…éœ€å‚æ•°
        required_fields = ['start_date', 'end_date', 'tenant_ids', 'task_type', 'detail_type', 'date']
        for field in required_fields:
            if field not in data:
                return jsonify({
                    'success': False,
                    'message': f'ç¼ºå°‘å¿…éœ€å‚æ•°: {field}'
                }), 400
        
        start_date = data['start_date']
        end_date = data['end_date']
        tenant_ids = data['tenant_ids']
        task_type = data['task_type']
        detail_type = data['detail_type']  # 'failed', 'timeout', 'failed_timeout'
        target_date = data['date']
        page = data.get('page', 1)
        page_size = data.get('page_size', 20)
        target_table = data.get('table')  # æ–°å¢ï¼šæŒ‡å®šæŸ¥è¯¢çš„è¡¨ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™æŸ¥è¯¢æ‰€æœ‰è¡¨
        
        # éªŒè¯å‚æ•°
        if not isinstance(tenant_ids, list) or len(tenant_ids) == 0:
            return jsonify({
                'success': False,
                'message': 'ç§Ÿæˆ·IDåˆ—è¡¨ä¸èƒ½ä¸ºç©º'
            }), 400
        
        # è·å–é…ç½®
        tables = TASK_STATISTICS_CONFIG['tables']
        tenants_config = TASK_STATISTICS_CONFIG['tenants']
        
        # å¦‚æœæŒ‡å®šäº†è¡¨ï¼ŒåªæŸ¥è¯¢è¯¥è¡¨
        if target_table:
            if target_table not in tables:
                return jsonify({
                    'success': False,
                    'message': f'æ— æ•ˆçš„è¡¨å: {target_table}'
                }), 400
            tables = [target_table]
        
        # éªŒè¯ç§Ÿæˆ·ID
        valid_tenant_ids = [t['id'] for t in tenants_config]
        for tenant_id in tenant_ids:
            if tenant_id not in valid_tenant_ids:
                return jsonify({
                    'success': False,
                    'message': f'æ— æ•ˆçš„ç§Ÿæˆ·ID: {tenant_id}'
                }), 400
        
        # åˆ›å»ºæ•°æ®åº“è¿æ¥ï¼ˆä¿®å¤ï¼šæ­£ç¡®é…ç½®æ•°æ®åº“ï¼‰
        db_config = DB_CONFIG.copy()
        db_config['database'] = 'shulex_collector_prod'
        
        all_details = []
        debug_info = []
        
        # æŸ¥è¯¢æ¯ä¸ªè¡¨çš„è¯¦ç»†æ•°æ®
        for table in tables:
            job_table = table
            log_table = table.replace('job_', 'log_')
            
            # æ„å»ºSQLæŸ¥è¯¢
            tenant_placeholders = ', '.join(['%s'] * len(tenant_ids))
            
            # æ ¹æ®è¯¦æƒ…ç±»å‹æ„å»ºä¸åŒçš„WHEREæ¡ä»¶
            # è½¬æ¢æ—¥æœŸä¸ºUTCæ—¶é—´
            target_date_start = convert_to_utc_datetime(target_date, "00:00:00")
            target_date_end = convert_to_utc_datetime(target_date, "23:59:59")
            
            # è®¡ç®—æœ€ç»ˆçš„æ—¥æœŸèŒƒå›´ï¼ˆå–äº¤é›†ï¼‰
            final_start_date = max(convert_to_utc_datetime(start_date, "00:00:00"), target_date_start)
            final_end_date = min(convert_to_utc_datetime(end_date, "23:59:59"), target_date_end)
            
            if detail_type == 'failed':
                where_condition = f"""
                WHERE a.created_at >= %s 
                    AND a.created_at <= %s 
                    AND a.tenant_id IN ({tenant_placeholders})
                    AND a.type = %s
                    AND a.status = 'FAILED'
                """
                params = [
                    final_start_date,
                    final_end_date
                ] + tenant_ids + [
                    task_type
                ]
            elif detail_type == 'timeout':
                # è¶…æ—¶åˆ¤æ–­åº”è¯¥ä½¿ç”¨å½“å‰UTCæ—¶é—´ï¼Œè€Œä¸æ˜¯ç›®æ ‡æ—¥æœŸ
                # è¿™æ ·å¯ä»¥åæ˜ æˆªè‡³å½“å‰æ—¶é—´çš„çœŸå®è¶…æ—¶çŠ¶æ€
                current_utc_time = get_utc_now()
                timeout_condition, _ = get_timeout_condition(current_utc_time)
                
                where_condition = f"""
                WHERE a.created_at >= %s 
                    AND a.created_at <= %s 
                    AND a.tenant_id IN ({tenant_placeholders})
                    AND a.type = %s
                    AND {timeout_condition}
                """
                params = [
                    final_start_date,
                    final_end_date
                ] + tenant_ids + [
                    task_type,
                    current_utc_time
                ]
            elif detail_type == 'timeout_but_succeed':
                # å·²è¶…æ—¶ä½†å·²å®Œæˆ
                current_utc_time = get_utc_now()
                timeout_condition, _ = get_timeout_condition(current_utc_time)
                
                where_condition = f"""
                WHERE a.created_at >= %s 
                    AND a.created_at <= %s 
                    AND a.tenant_id IN ({tenant_placeholders})
                    AND a.type = %s
                    AND {timeout_condition}
                    AND a.status = 'SUCCEED'
                """
                params = [
                    final_start_date,
                    final_end_date
                ] + tenant_ids + [
                    task_type,
                    current_utc_time
                ]
            elif detail_type == 'succeed':
                # å·²å®Œæˆ
                where_condition = f"""
                WHERE a.created_at >= %s 
                    AND a.created_at <= %s 
                    AND a.tenant_id IN ({tenant_placeholders})
                    AND a.type = %s
                    AND a.status = 'SUCCEED'
                """
                params = [
                    final_start_date,
                    final_end_date
                ] + tenant_ids + [
                    task_type
                ]
            elif detail_type == 'succeed_not_timeout':
                # æœªè¶…æ—¶ä¸”å·²å®Œæˆ
                current_utc_time = get_utc_now()
                timeout_condition, _ = get_timeout_condition(current_utc_time)
                
                where_condition = f"""
                WHERE a.created_at >= %s 
                    AND a.created_at <= %s 
                    AND a.tenant_id IN ({tenant_placeholders})
                    AND a.type = %s
                    AND NOT ({timeout_condition})
                    AND a.status = 'SUCCEED'
                """
                params = [
                    final_start_date,
                    final_end_date
                ] + tenant_ids + [
                    task_type,
                    current_utc_time
                ]
            elif detail_type == 'timeout_not_succeed':
                # è¶…æ—¶æœªå®Œæˆ
                current_utc_time = get_utc_now()
                timeout_condition, _ = get_timeout_condition(current_utc_time)
                
                where_condition = f"""
                WHERE a.created_at >= %s 
                    AND a.created_at <= %s 
                    AND a.tenant_id IN ({tenant_placeholders})
                    AND a.type = %s
                    AND {timeout_condition}
                    AND a.status != 'SUCCEED'
                    AND a.status != 'FAILED'
                """
                params = [
                    final_start_date,
                    final_end_date
                ] + tenant_ids + [
                    task_type,
                    current_utc_time
                ]
            else:
                return jsonify({
                    'success': False,
                    'message': f'ä¸æ”¯æŒçš„è¯¦æƒ…ç±»å‹: {detail_type}'
                }), 400
            
            sql = f"""
            SELECT 
                a.created_at,
                a.break_at,
                a.deliver_at,
                a.req_ssn,
                a.payload,
                a.result,
                b.ext_ssn,
                b.analysis_response,
                b.response,
                a.status,
                b.state as log_state,
                '{table}' as source_table
            FROM {job_table} a 
            LEFT JOIN {log_table} b ON b.req_ssn = a.req_ssn
            {where_condition}
            ORDER BY a.created_at DESC
            LIMIT {page_size} OFFSET {(page - 1) * page_size}
            """
            
            # è®°å½•è°ƒè¯•ä¿¡æ¯
            debug_info.append({
                'table': table,
                'sql': clean_sql_for_debug(sql),
                'params': params,
                'detail_type': detail_type,
                'query_time': get_utc_now()
            })
            
            try:
                # æ‰§è¡ŒæŸ¥è¯¢
                connector = DatabaseConnector(db_config)
                if not connector.connect():
                    print(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {table}")
                    continue
                
                results = connector.execute_query(sql, params)
                
                # å¤„ç†ç»“æœ
                for row in results:
                    # å¤„ç†å­—å…¸æ ¼å¼çš„ç»“æœ
                    if isinstance(row, dict):
                        # æ ¼å¼åŒ–JSONå­—æ®µ
                        result_formatted = format_json_field(row.get('result'))
                        response_formatted = format_json_field(row.get('response'))
                        
                        # åˆ¤æ–­æ˜¯å¦æ˜¾ç¤ºé‡çˆ¬æŒ‰é’®
                        status = row.get('status') or ''
                        show_recrawl = should_show_recrawl_button(status, result_formatted)
                        
                        detail_item = {
                            'created_at': row.get('created_at').strftime('%Y-%m-%d %H:%M:%S') if row.get('created_at') else '',
                            'break_at': row.get('break_at').strftime('%Y-%m-%d %H:%M:%S') if row.get('break_at') else '',
                            'deliver_at': row.get('deliver_at').strftime('%Y-%m-%d %H:%M:%S') if row.get('deliver_at') else '',
                            'req_ssn': row.get('req_ssn') or '',
                            'payload': row.get('payload') or '',
                            'result': result_formatted,
                            'ext_ssn': row.get('ext_ssn') or '',
                            'analysis_response': row.get('analysis_response') or '',
                            'response': response_formatted,
                            'status': status,
                            'log_state': row.get('log_state') or '',
                            'source_table': row.get('source_table') or table,
                            'show_recrawl_button': show_recrawl
                        }
                    else:
                        # å¤„ç†å…ƒç»„æ ¼å¼çš„ç»“æœ
                        # æ ¼å¼åŒ–JSONå­—æ®µ
                        result_formatted = format_json_field(row[5] if len(row) > 5 else '')
                        response_formatted = format_json_field(row[8] if len(row) > 8 else '')
                        
                        # åˆ¤æ–­æ˜¯å¦æ˜¾ç¤ºé‡çˆ¬æŒ‰é’®
                        status = row[9] if len(row) > 9 else ''
                        show_recrawl = should_show_recrawl_button(status, result_formatted)
                        
                        detail_item = {
                            'created_at': row[0].strftime('%Y-%m-%d %H:%M:%S') if row[0] else '',
                            'break_at': row[1].strftime('%Y-%m-%d %H:%M:%S') if row[1] else '',
                            'deliver_at': row[2].strftime('%Y-%m-%d %H:%M:%S') if row[2] else '',
                            'req_ssn': row[3] or '',
                            'payload': row[4] or '',
                            'result': result_formatted,
                            'ext_ssn': row[6] or '',
                            'analysis_response': row[7] or '',
                            'response': response_formatted,
                            'status': status,
                            'log_state': row[10] or '',
                            'source_table': row[11] or table,
                            'show_recrawl_button': show_recrawl
                        }
                    all_details.append(detail_item)
                
                connector.disconnect()
                    
            except Exception as e:
                print(f"æŸ¥è¯¢è¡¨ {table} è¯¦ç»†æ•°æ®å¤±è´¥: {str(e)}")
                debug_info[-1]['error'] = str(e)
                continue
        
        # æŒ‰åˆ›å»ºæ—¶é—´æ’åº
        all_details.sort(key=lambda x: x['created_at'], reverse=True)
        
        # åˆ†é¡µå¤„ç†ï¼ˆå·²åœ¨SQLä¸­å¤„ç†ï¼Œè¿™é‡Œåªæ˜¯ä¸ºäº†è®¡ç®—æ€»æ•°ï¼‰
        total_count = len(all_details)
        
        return jsonify({
            'success': True,
            'data': {
                'details': all_details,
                'total_count': total_count,
                'page': page,
                'page_size': page_size,
                'total_pages': (total_count + page_size - 1) // page_size if total_count > 0 else 0,
                'queried_tables': tables  # è¿”å›æŸ¥è¯¢çš„è¡¨åˆ—è¡¨
            },
            '_debug': debug_info
        })
        
    except Exception as e:
        logging.error(f"è·å–ç»Ÿè®¡è¯¦ç»†æ•°æ®å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'message': f'è·å–è¯¦ç»†æ•°æ®å¤±è´¥: {str(e)}'
        }), 500


def should_show_recrawl_button(status, result_data):
    """
    åˆ¤æ–­æ˜¯å¦åº”è¯¥æ˜¾ç¤ºé‡çˆ¬æŒ‰é’®
    
    Args:
        status: ä»»åŠ¡çŠ¶æ€
        result_data: æ ¼å¼åŒ–åçš„resultæ•°æ®å­—å…¸
        
    Returns:
        bool: æ˜¯å¦æ˜¾ç¤ºé‡çˆ¬æŒ‰é’®
    """
    # æ£€æŸ¥çŠ¶æ€æ˜¯å¦ä¸ºFAILED
    if status != 'FAILED':
        return False
    
    # æ£€æŸ¥resultæ•°æ®æ˜¯å¦æœ‰æ•ˆ
    if not result_data or not result_data.get('is_valid_json'):
        return False
    
    try:
        # è§£æJSONæ•°æ®
        if isinstance(result_data.get('raw'), str):
            result_json = json.loads(result_data.get('raw'))
        else:
            result_json = result_data.get('raw')
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç‰¹å®šçš„é”™è¯¯ä¿¡æ¯
        expected_error = "ä½œä¸šæäº¤å¤±è´¥: 408 IORuntimeException - SocketTimeoutException: connect timed out"
        
        if isinstance(result_json, dict) and 'E3001' in result_json:
            actual_error = result_json.get('E3001', '')
            return actual_error == expected_error
        
        return False
        
    except (json.JSONDecodeError, TypeError, AttributeError) as e:
        print(f"âš ï¸ è§£æresultæ•°æ®å¤±è´¥: {e}")
        return False


def format_json_field(json_str):
    """
    æ ¼å¼åŒ–JSONå­—æ®µ
    
    Args:
        json_str: JSONå­—ç¬¦ä¸²
        
    Returns:
        dict: åŒ…å«æ ¼å¼åŒ–ä¿¡æ¯çš„å­—å…¸
    """
    if not json_str:
        return {
            'raw': '',
            'formatted': '',
            'is_valid_json': False,
            'error': ''
        }
    
    try:
        # å°è¯•è§£æJSON
        if isinstance(json_str, str):
            parsed_json = json.loads(json_str)
        else:
            parsed_json = json_str
        
        # æ ¼å¼åŒ–JSON
        formatted_json = json.dumps(parsed_json, indent=2, ensure_ascii=False)
        
        return {
            'raw': json_str,
            'formatted': formatted_json,
            'is_valid_json': True,
            'error': ''
        }
    except (json.JSONDecodeError, TypeError) as e:
        return {
            'raw': json_str,
            'formatted': str(json_str),
            'is_valid_json': False,
            'error': str(e)
        }


@app.route('/api/resubmit_crawler', methods=['POST'])
def resubmit_crawler():
    """é‡æ–°æäº¤çˆ¬è™«ä»»åŠ¡"""
    try:
        data = request.get_json()
        
        # éªŒè¯å¿…éœ€å‚æ•°
        if 'req_ssn' not in data:
            return jsonify({
                'success': False,
                'message': 'ç¼ºå°‘å¿…éœ€å‚æ•°: req_ssn'
            }), 400
        
        req_ssn = data['req_ssn']
        
        # æ„å»ºå‘½ä»¤
        job_id = f"SL{req_ssn}" if not req_ssn.startswith('SL') else req_ssn
        command = f"python3 src/main.py resubmit_crawler_jobs --job-ids {job_id}"
        
        print(f"ğŸ”„ æ‰§è¡Œé‡çˆ¬å‘½ä»¤: {command}")
        logging.info(f"æ‰§è¡Œé‡çˆ¬å‘½ä»¤: {command} (req_ssn: {req_ssn})")
        
        # å¼‚æ­¥æ‰§è¡Œå‘½ä»¤
        def run_resubmit_command():
            try:
                process = subprocess.Popen(
                    command,
                    shell=True,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.STDOUT,
                    text=True,
                    cwd=os.getcwd(),
                    bufsize=1,
                    universal_newlines=True
                )
                
                # è¯»å–è¾“å‡º
                output_lines = []
                while True:
                    output = process.stdout.readline()
                    if output == '' and process.poll() is not None:
                        break
                    if output:
                        line = output.strip()
                        output_lines.append(line)
                        print(f"ğŸ“ é‡çˆ¬è¾“å‡º: {line}")
                
                return_code = process.poll()
                
                final_output = '\n'.join(output_lines)
                success = return_code == 0
                
                print(f"âœ… é‡çˆ¬å‘½ä»¤å®Œæˆï¼Œè¿”å›ç : {return_code}")
                print(f"ğŸ“‹ å®Œæ•´è¾“å‡º:\n{final_output}")
                
                logging.info(f"é‡çˆ¬å‘½ä»¤å®Œæˆ - req_ssn: {req_ssn}, è¿”å›ç : {return_code}, è¾“å‡º: {final_output}")
                
                return {
                    'success': success,
                    'return_code': return_code,
                    'output': final_output,
                    'job_id': job_id,
                    'req_ssn': req_ssn
                }
                
            except Exception as e:
                error_msg = f"æ‰§è¡Œé‡çˆ¬å‘½ä»¤æ—¶å‡ºé”™: {str(e)}"
                print(f"âŒ {error_msg}")
                logging.error(error_msg)
                return {
                    'success': False,
                    'error': error_msg,
                    'job_id': job_id,
                    'req_ssn': req_ssn
                }
        
        # åœ¨æ–°çº¿ç¨‹ä¸­æ‰§è¡Œå‘½ä»¤
        thread = threading.Thread(target=run_resubmit_command)
        thread.daemon = True
        thread.start()
        
        return jsonify({
            'success': True,
            'message': f'é‡çˆ¬ä»»åŠ¡å·²æäº¤ï¼Œä»»åŠ¡ID: {job_id}',
            'job_id': job_id,
            'req_ssn': req_ssn,
            'command': command
        })
        
    except Exception as e:
        error_msg = f"æäº¤é‡çˆ¬ä»»åŠ¡å¤±è´¥: {str(e)}"
        print(f"âŒ {error_msg}")
        logging.error(error_msg)
        return jsonify({
            'success': False,
            'message': error_msg
        }), 500


if __name__ == '__main__':
    # ç¡®ä¿æ¨¡æ¿å’Œé™æ€æ–‡ä»¶ç›®å½•å­˜åœ¨
    os.makedirs('templates', exist_ok=True)
    os.makedirs('static', exist_ok=True)
    
    print("ğŸš€ ä»»åŠ¡æ£€æŸ¥å·¥å…·çœ‹æ¿å¯åŠ¨ä¸­...")
    print("ğŸ“Š ç»Ÿä¸€çš„ä»»åŠ¡ç®¡ç†å’Œç›‘æ§å¹³å°")
    print("ğŸ“‹ è®¿é—®åœ°å€: http://localhost:5001")
    print("=" * 50)
    
    app.run(debug=True, host='0.0.0.0', port=5001) 