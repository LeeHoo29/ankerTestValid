#!/usr/bin/env python3
"""
ä»»åŠ¡æ£€æŸ¥å·¥å…·çœ‹æ¿ - Webç•Œé¢
ç»Ÿä¸€çš„ä»»åŠ¡ç®¡ç†å’Œç›‘æ§å¹³å°
æ”¯æŒå¤šç§å·¥å…·æ¨¡å—ï¼ŒåŒ…æ‹¬Azure Resource Readerç­‰
"""

import os
import subprocess
import threading
import time
import json
import hashlib
from datetime import datetime, timedelta, timezone
from flask import Flask, render_template, request, jsonify, session, send_file
from flask_cors import CORS
from pathlib import Path
import uuid
import logging

# é…ç½®æ—¥å¿—ç³»ç»Ÿ
def setup_logger():
    """è®¾ç½®åº”ç”¨ç¨‹åºæ—¥å¿—é…ç½®"""
    # åˆ›å»ºlogsç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    log_dir = Path('logs')
    log_dir.mkdir(exist_ok=True)
    
    # é…ç½®æ—¥å¿—æ ¼å¼
    log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    
    # é…ç½®æ ¹logger
    logging.basicConfig(
        level=logging.INFO,
        format=log_format,
        handlers=[
            # æ§åˆ¶å°è¾“å‡º
            logging.StreamHandler(),
            # æ–‡ä»¶è¾“å‡º
            logging.FileHandler(log_dir / 'web_app.log', encoding='utf-8')
        ]
    )
    
    # åˆ›å»ºåº”ç”¨ä¸“ç”¨logger
    logger = logging.getLogger('web_app')
    return logger

# åˆå§‹åŒ–logger
logger = setup_logger()

# å¯¼å…¥æ•°æ®åº“è¿æ¥å™¨å’Œé…ç½®
from src.db.connector import DatabaseConnector
from config.db_config import DB_CONFIG
from config.task_statistics_config import (
    DATABASE_TABLES, TASK_TYPES, TENANT_CONFIG, 
    get_sql_template, get_all_tenant_ids, TASK_STATISTICS_CONFIG
)

# æ·»åŠ æœ¬åœ°æ•°æ®åº“è¿æ¥å™¨å¯¼å…¥
try:
    from src.db.local_connector import LocalDatabaseConnector
    LOCAL_DB_AVAILABLE = True
except ImportError:
    LOCAL_DB_AVAILABLE = False
    logger.warning("âš ï¸  æœ¬åœ°æ•°æ®åº“æ¨¡å—ä¸å¯ç”¨ï¼Œéƒ¨åˆ†åŠŸèƒ½å°†è¢«ç¦ç”¨")

app = Flask(__name__)
app.secret_key = 'azure_resource_reader_web_2024'

# é…ç½®CORSï¼Œå…è®¸å‰ç«¯è·¨åŸŸè®¿é—®
CORS(app, origins=['http://localhost:3000', 'http://localhost:3001'])

# å…¨å±€å˜é‡å­˜å‚¨ä»»åŠ¡çŠ¶æ€
tasks = {}

# ç®€å•çš„å†…å­˜ç¼“å­˜æœºåˆ¶
statistics_cache = {}
CACHE_DURATION = 21600  # ç¼“å­˜6å°æ—¶ï¼ˆ6 * 60 * 60 = 21600ç§’ï¼‰

def get_utc_now():
    """è·å–å½“å‰UTCæ—¶é—´å­—ç¬¦ä¸²"""
    return datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S')

def convert_to_utc_datetime(date_str, time_str="00:00:00"):
    """
    å°†æ—¥æœŸå­—ç¬¦ä¸²è½¬æ¢ä¸ºUTCæ—¶é—´å­—ç¬¦ä¸²
    
    Args:
        date_str: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼å¦‚ "2025-05-27"
        time_str: æ—¶é—´å­—ç¬¦ä¸²ï¼Œæ ¼å¼å¦‚ "00:00:00" æˆ– "23:59:59"
    
    Returns:
        str: UTCæ—¶é—´å­—ç¬¦ä¸²ï¼Œæ ¼å¼å¦‚ "2025-05-27 00:00:00"
    """
    return f"{date_str} {time_str}"

def get_timeout_condition(reference_time=None):
    """
    è·å–è¶…æ—¶æŸ¥è¯¢æ¡ä»¶
    
    Args:
        reference_time: å‚è€ƒæ—¶é—´ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å½“å‰UTCæ—¶é—´
    
    Returns:
        tuple: (condition_sql, reference_time_value)
    """
    if reference_time is None:
        reference_time = get_utc_now()
    
    condition_sql = """break_at < %s
        AND (deliver_at IS NULL OR deliver_at > break_at)
        AND `status` != 'FAILED'"""
    
    return condition_sql.strip(), reference_time

def clean_sql_for_debug(sql_string):
    """
    æ¸…ç†SQLå­—ç¬¦ä¸²ï¼Œå»æ‰æ¢è¡Œç¬¦å’Œå¤šä½™ç©ºæ ¼ï¼Œæ–¹ä¾¿å¤åˆ¶
    """
    if not sql_string:
        return sql_string
    
    # å»æ‰æ¢è¡Œç¬¦ã€åˆ¶è¡¨ç¬¦å’Œå¤šä½™ç©ºæ ¼ï¼Œä¿æŒå•è¡Œæ ¼å¼
    cleaned = ' '.join(sql_string.replace('\n', ' ').replace('\t', ' ').split())
    return cleaned

def execute_command(task_id, command, working_dir):
    """åœ¨åå°æ‰§è¡Œå‘½ä»¤"""
    try:
        tasks[task_id]['status'] = 'running'
        tasks[task_id]['start_time'] = datetime.now()
        
        # æ‰§è¡Œå‘½ä»¤
        process = subprocess.Popen(
            command,
            shell=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            cwd=working_dir,
            bufsize=1,
            universal_newlines=True
        )
        
        # å®æ—¶è¯»å–è¾“å‡º
        output_lines = []
        while True:
            output = process.stdout.readline()
            if output == '' and process.poll() is not None:
                break
            if output:
                line = output.strip()
                output_lines.append(line)
                tasks[task_id]['output'] = '\n'.join(output_lines)
        
        # è·å–è¿”å›ç 
        return_code = process.poll()
        
        tasks[task_id]['status'] = 'completed' if return_code == 0 else 'failed'
        tasks[task_id]['return_code'] = return_code
        tasks[task_id]['end_time'] = datetime.now()
        
    except Exception as e:
        tasks[task_id]['status'] = 'error'
        tasks[task_id]['error'] = str(e)
        tasks[task_id]['end_time'] = datetime.now()

def format_timestamp(iso_timestamp):
    """æ ¼å¼åŒ–ISOæ—¶é—´æˆ³ä¸ºæ˜“è¯»æ ¼å¼"""
    try:
        if not iso_timestamp:
            return 'æœªçŸ¥æ—¶é—´'
        
        # è§£æISOæ ¼å¼æ—¶é—´æˆ³
        dt = datetime.fromisoformat(iso_timestamp.replace('Z', '+00:00'))
        
        # æ ¼å¼åŒ–ä¸ºæ˜“è¯»æ ¼å¼: å¹´-æœˆ-æ—¥ æ—¶:åˆ†:ç§’
        return dt.strftime('%Y-%m-%d %H:%M:%S')
    except Exception as e:
        # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›åŸå§‹å­—ç¬¦ä¸²
        return iso_timestamp

def read_completed_tasks():
    """è¯»å–å·²å®Œæˆä»»åŠ¡ï¼ˆä»…ä»æ•°æ®åº“è·å–ï¼‰"""
    try:
        if not LOCAL_DB_AVAILABLE:
            logger.warning("æœ¬åœ°æ•°æ®åº“ä¸å¯ç”¨ï¼Œè¿”å›ç©ºä»»åŠ¡åˆ—è¡¨")
            return []
        
        db = LocalDatabaseConnector()
        
        # è·å–æ‰€æœ‰ä»»åŠ¡æ˜ å°„
        cursor = db.cursor
        query = """
            SELECT job_id, task_type, actual_task_id, relative_path, 
                   full_path, file_count, has_parse_file, 
                   download_method, status, created_at, updated_at
            FROM task_mapping 
            ORDER BY updated_at DESC
        """
        cursor.execute(query)
        mappings = cursor.fetchall()
        
        completed_tasks = []
        for mapping in mappings:
            # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
            full_path = Path(mapping['full_path']) if mapping['full_path'] else None
            directory_exists = full_path.exists() if full_path else False
            
            # æ ¼å¼åŒ–ç›¸å¯¹è·¯å¾„
            relative_path = mapping['relative_path']
            if relative_path.startswith('./'):
                relative_path = relative_path[2:]
            
            completed_tasks.append({
                'job_id': mapping['job_id'],
                'task_type': mapping['task_type'],
                'actual_task_id': mapping['actual_task_id'],
                'last_updated': format_timestamp(mapping['updated_at'].isoformat() if mapping['updated_at'] else ''),
                'relative_path': relative_path,
                'full_path': str(full_path) if full_path else '',
                'file_count': mapping['file_count'],
                'has_parse_file': mapping['has_parse_file'],
                'directory_exists': directory_exists
            })
        
        db.disconnect()
        return completed_tasks
        
    except Exception as e:
        logger.warning(f"è¯»å–ä»»åŠ¡å¤±è´¥: {str(e)}")
        return []

@app.route('/')
def index():
    """ä¸»é¡µé¢"""
    completed_tasks = read_completed_tasks()
    return render_template('index.html', completed_tasks=completed_tasks)

@app.route('/api/completed_tasks')
def get_completed_tasks():
    """è·å–å·²å®Œæˆä»»åŠ¡çš„APIæ¥å£ï¼ˆä»…ä»æ•°æ®åº“è·å–ï¼‰"""
    logger.info("ğŸ“¥ æ”¶åˆ°è·å–å·²å®Œæˆä»»åŠ¡åˆ—è¡¨è¯·æ±‚")
    try:
        if not LOCAL_DB_AVAILABLE:
            logger.warning("æœ¬åœ°æ•°æ®åº“ä¸å¯ç”¨")
            return jsonify({
                'success': False,
                'error': 'æœ¬åœ°æ•°æ®åº“ä¸å¯ç”¨',
                'tasks': [],
                'total_count': 0
            }), 500
        
        db = LocalDatabaseConnector()
        
        # ç¡®ä¿æ•°æ®åº“è¿æ¥æˆåŠŸ
        if not db.connect():
            logger.warning("æ•°æ®åº“è¿æ¥å¤±è´¥")
            return jsonify({
                'success': False,
                'error': 'æ•°æ®åº“è¿æ¥å¤±è´¥',
                'tasks': [],
                'total_count': 0
            }), 500
        
        # è·å–æ‰€æœ‰ä»»åŠ¡æ˜ å°„
        cursor = db.cursor
        if cursor is None:
            logger.warning("è·å–æ•°æ®åº“æ¸¸æ ‡å¤±è´¥")
            db.disconnect()
            return jsonify({
                'success': False,
                'error': 'è·å–æ•°æ®åº“æ¸¸æ ‡å¤±è´¥',
                'tasks': [],
                'total_count': 0
            }), 500
            
        query = """
            SELECT job_id, task_type, actual_task_id, relative_path, 
                   full_path, file_count, has_parse_file, 
                   download_method, status, created_at, updated_at
            FROM task_mapping 
            ORDER BY updated_at DESC
        """
        cursor.execute(query)
        mappings = cursor.fetchall()
        
        tasks = []
        for mapping in mappings:
            task = {
                'job_id': mapping['job_id'],
                'task_type': mapping['task_type'],
                'actual_task_id': mapping['actual_task_id'],
                'relative_path': mapping['relative_path'],
                'full_path': mapping['full_path'],
                'file_count': mapping['file_count'],
                'has_parse_file': mapping['has_parse_file'],
                'download_method': mapping['download_method'],
                'status': mapping['status'],
                'last_updated': mapping['updated_at'].isoformat() if mapping['updated_at'] else None,
                'created_at': mapping['created_at'].isoformat() if mapping['created_at'] else None,
                'directory_exists': True  # å…¼å®¹å‰ç«¯å­—æ®µ
            }
            tasks.append(task)
        
        db.disconnect()
        
        logger.info(f"âœ… æˆåŠŸè¿”å› {len(tasks)} ä¸ªå·²å®Œæˆä»»åŠ¡")
        return jsonify({
            'success': True,
            'tasks': tasks,
            'total_count': len(tasks),
            'source': 'database'
        })
        
    except Exception as e:
        logger.error(f"è·å–å·²å®Œæˆä»»åŠ¡å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e),
            'tasks': [],
            'total_count': 0
        }), 500

@app.route('/api/list_files')
def list_files():
    """åˆ—å‡ºæŒ‡å®šç›®å½•ä¸‹çš„æ–‡ä»¶"""
    try:
        dir_path = request.args.get('path')
        if not dir_path:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘è·¯å¾„å‚æ•°'})
        
        path = Path(dir_path)
        if not path.exists():
            return jsonify({'success': False, 'error': 'ç›®å½•ä¸å­˜åœ¨'})
        
        if not path.is_dir():
            return jsonify({'success': False, 'error': 'è·¯å¾„ä¸æ˜¯ç›®å½•'})
        
        files = []
        for file_path in path.iterdir():
            if file_path.is_file():
                stat = file_path.stat()
                files.append({
                    'name': file_path.name,
                    'path': str(file_path),
                    'size': stat.st_size,
                    'modified': datetime.fromtimestamp(stat.st_mtime).strftime('%Y-%m-%d %H:%M')
                })
        
        # æŒ‰åç§°æ’åº
        files.sort(key=lambda x: x['name'])
        
        return jsonify({
            'success': True,
            'files': files,
            'directory': str(path)
        })
        
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/download_file')
def download_file():
    """ä¸‹è½½æ–‡ä»¶"""
    try:
        file_path = request.args.get('path')
        if not file_path:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘æ–‡ä»¶è·¯å¾„å‚æ•°'})
        
        path = Path(file_path)
        if not path.exists():
            return jsonify({'success': False, 'error': 'æ–‡ä»¶ä¸å­˜åœ¨'})
        
        if not path.is_file():
            return jsonify({'success': False, 'error': 'è·¯å¾„ä¸æ˜¯æ–‡ä»¶'})
        
        # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿æ–‡ä»¶åœ¨data/outputç›®å½•ä¸‹
        data_output_path = Path('data/output').resolve()
        try:
            path.resolve().relative_to(data_output_path)
        except ValueError:
            return jsonify({'success': False, 'error': 'æ–‡ä»¶è®¿é—®æƒé™ä¸è¶³'})
        
        return send_file(path, as_attachment=True, download_name=path.name)
        
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/file_content')
def get_file_content():
    """è·å–æ–‡ä»¶å†…å®¹"""
    try:
        file_path = request.args.get('path')
        if not file_path:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘æ–‡ä»¶è·¯å¾„å‚æ•°'})
        
        path = Path(file_path)
        if not path.exists():
            return jsonify({'success': False, 'error': 'æ–‡ä»¶ä¸å­˜åœ¨'})
        
        if not path.is_file():
            return jsonify({'success': False, 'error': 'è·¯å¾„ä¸æ˜¯æ–‡ä»¶'})
        
        # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿æ–‡ä»¶åœ¨data/outputç›®å½•ä¸‹
        data_output_path = Path('data/output').resolve()
        try:
            path.resolve().relative_to(data_output_path)
        except ValueError:
            return jsonify({'success': False, 'error': 'æ–‡ä»¶è®¿é—®æƒé™ä¸è¶³'})
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹è¯»å–å†…å®¹
        file_ext = path.suffix.lower()
        
        def read_file_with_encoding_detection(file_path):
            """æ™ºèƒ½è¯»å–æ–‡ä»¶ï¼Œè‡ªåŠ¨æ£€æµ‹ç¼–ç """
            # å°è¯•å¤šç§ç¼–ç æ–¹å¼è¯»å–æ–‡ä»¶
            encodings_to_try = ['utf-8', 'shift_jis', 'gbk', 'big5', 'iso-8859-1', 'cp1252']
            content = None
            used_encoding = None
            
            for encoding in encodings_to_try:
                try:
                    with open(file_path, 'r', encoding=encoding) as f:
                        content = f.read()
                        used_encoding = encoding
                        break
                except UnicodeDecodeError:
                    continue
            
            if content is None:
                # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œå°è¯•äºŒè¿›åˆ¶æ¨¡å¼è¯»å–
                try:
                    with open(file_path, 'rb') as f:
                        raw_data = f.read()
                        # å°è¯•è‡ªåŠ¨æ£€æµ‹ç¼–ç 
                        try:
                            import chardet
                            detected = chardet.detect(raw_data)
                            if detected['encoding'] and detected['confidence'] > 0.7:
                                content = raw_data.decode(detected['encoding'])
                                used_encoding = f"{detected['encoding']} (detected)"
                        except ImportError:
                            pass
                        
                        # å¦‚æœæ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨UTF-8å¹¶å¿½ç•¥é”™è¯¯
                        if content is None:
                            content = raw_data.decode('utf-8', errors='ignore')
                            used_encoding = 'utf-8 (with errors ignored)'
                except Exception:
                    raise Exception('æ— æ³•è¯»å–æ–‡ä»¶å†…å®¹')
            
            return content, used_encoding
        
        if file_ext in ['.html', '.htm']:
            content, encoding = read_file_with_encoding_detection(path)
            return jsonify({
                'success': True,
                'content': content,
                'type': 'html',
                'filename': path.name,
                'encoding': encoding
            })
        elif file_ext == '.json':
            content, encoding = read_file_with_encoding_detection(path)
            
            # æ£€æµ‹JSONå†…å®¹æ˜¯å¦ä¸ºç©ºæˆ–æ— æ„ä¹‰
            is_empty_json = False
            empty_json_type = None
            
            try:
                parsed_content = json.loads(content)
                # æ£€æµ‹å„ç§"ç©º"æƒ…å†µ
                if parsed_content == "":
                    is_empty_json = True
                    empty_json_type = "empty_string"
                elif parsed_content == {}:
                    is_empty_json = True
                    empty_json_type = "empty_object"
                elif parsed_content == []:
                    is_empty_json = True
                    empty_json_type = "empty_array"
                elif parsed_content is None:
                    is_empty_json = True
                    empty_json_type = "null"
            except json.JSONDecodeError:
                pass
            
            return jsonify({
                'success': True,
                'content': content,
                'type': 'json',
                'filename': path.name,
                'encoding': encoding,
                'is_empty_json': is_empty_json,
                'empty_json_type': empty_json_type
            })
        elif file_ext == '.txt':
            content, encoding = read_file_with_encoding_detection(path)
            return jsonify({
                'success': True,
                'content': content,
                'type': 'text',
                'filename': path.name,
                'encoding': encoding
            })
        else:
            return jsonify({'success': False, 'error': 'ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹'})
        
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/compare')
def compare_view():
    """å¯¹æ¯”æŸ¥çœ‹é¡µé¢"""
    task_path = request.args.get('path')
    if not task_path:
        return "ç¼ºå°‘ä»»åŠ¡è·¯å¾„å‚æ•°", 400
    
    # è·å–ä»»åŠ¡ç›®å½•ä¸‹çš„æ–‡ä»¶åˆ—è¡¨
    try:
        path = Path(task_path)
        if not path.exists() or not path.is_dir():
            return "ä»»åŠ¡ç›®å½•ä¸å­˜åœ¨", 404
        
        files = []
        for file_path in path.iterdir():
            if file_path.is_file():
                files.append({
                    'name': file_path.name,
                    'path': str(file_path),
                    'type': file_path.suffix.lower()
                })
        
        # æŒ‰ç±»å‹åˆ†ç»„
        html_files = [f for f in files if f['type'] in ['.html', '.htm']]
        json_files = [f for f in files if f['type'] == '.json']
        
        return render_template('compare.html', 
                             task_path=task_path,
                             html_files=html_files,
                             json_files=json_files)
        
    except Exception as e:
        return f"åŠ è½½å¤±è´¥: {str(e)}", 500

@app.route('/submit', methods=['POST'])
def submit_command():
    """æäº¤å‘½ä»¤æ‰§è¡Œ"""
    try:
        # è·å–è¡¨å•æ•°æ®
        task_id_input = request.form.get('task_id', '').strip()
        output_type = request.form.get('output_type', 'html')
        use_parse = request.form.get('use_parse') == 'on'
        
        # éªŒè¯è¾“å…¥
        if not task_id_input:
            return jsonify({
                'success': False,
                'error': 'è¯·å¡«å†™ä»»åŠ¡ID'
            })
        
        # è‡ªåŠ¨è·å–ä»»åŠ¡ç±»å‹
        from src.azure_resource_reader import get_task_type_by_job_id
        task_type = get_task_type_by_job_id(task_id_input)
        
        if not task_type:
            return jsonify({
                'success': False,
                'error': f'æ— æ³•æ‰¾åˆ°ä»»åŠ¡ID {task_id_input} å¯¹åº”çš„ä»»åŠ¡ç±»å‹ï¼Œè¯·æ£€æŸ¥ä»»åŠ¡IDæ˜¯å¦æ­£ç¡®'
            })
        
        # æ„å»ºæ™ºèƒ½æ¨¡å¼å‘½ä»¤ï¼ˆç›´æ¥ä½¿ç”¨job_idï¼Œè®©è„šæœ¬è‡ªåŠ¨è¯†åˆ«ä»»åŠ¡ç±»å‹ï¼‰
        command_parts = [
            'python3',
            'src/azure_resource_reader.py',
            task_id_input,  # ç›´æ¥ä½¿ç”¨job_id
            output_type
        ]
        
        if use_parse:
            command_parts.append('--with-parse')
        
        command = ' '.join(command_parts)
        
        # ç”Ÿæˆä»»åŠ¡ID
        execution_id = str(uuid.uuid4())
        
        # åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€
        tasks[execution_id] = {
            'command': command,
            'status': 'pending',
            'output': '',
            'created_time': datetime.now(),
            'task_type': task_type,
            'task_id': task_id_input,
            'output_type': output_type,
            'use_parse': use_parse
        }
        
        # åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œå‘½ä»¤
        thread = threading.Thread(
            target=execute_command,
            args=(execution_id, command, os.getcwd())
        )
        thread.daemon = True
        thread.start()
        
        return jsonify({
            'success': True,
            'task_id': execution_id,
            'command': command
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'æäº¤å¤±è´¥: {str(e)}'
        })

@app.route('/status/<task_id>')
def get_status(task_id):
    """è·å–ä»»åŠ¡çŠ¶æ€"""
    if task_id not in tasks:
        return jsonify({
            'success': False,
            'error': 'ä»»åŠ¡ä¸å­˜åœ¨'
        })
    
    task = tasks[task_id]
    
    # è®¡ç®—æ‰§è¡Œæ—¶é—´
    duration = None
    if 'start_time' in task:
        end_time = task.get('end_time', datetime.now())
        duration = str(end_time - task['start_time'])
    
    return jsonify({
        'success': True,
        'status': task['status'],
        'output': task.get('output', ''),
        'command': task['command'],
        'created_time': task['created_time'].strftime('%Y-%m-%d %H:%M:%S'),
        'duration': duration,
        'return_code': task.get('return_code'),
        'error': task.get('error')
    })

@app.route('/tasks')
def list_tasks():
    """åˆ—å‡ºæ‰€æœ‰ä»»åŠ¡"""
    task_list = []
    for task_id, task in tasks.items():
        duration = None
        if 'start_time' in task:
            end_time = task.get('end_time', datetime.now())
            duration = str(end_time - task['start_time'])
        
        task_list.append({
            'id': task_id,
            'command': task['command'],
            'status': task['status'],
            'created_time': task['created_time'].strftime('%Y-%m-%d %H:%M:%S'),
            'duration': duration,
            'task_type': task.get('task_type'),
            'task_id': task.get('task_id'),
            'use_parse': task.get('use_parse', False)
        })
    
    # æŒ‰åˆ›å»ºæ—¶é—´å€’åºæ’åˆ—
    task_list.sort(key=lambda x: x['created_time'], reverse=True)
    
    return render_template('tasks.html', tasks=task_list)

@app.route('/clear_tasks', methods=['POST'])
def clear_tasks():
    """æ¸…ç©ºä»»åŠ¡å†å²"""
    global tasks
    tasks = {}
    return jsonify({'success': True})

@app.route('/api/get_task_type')
def get_task_type():
    """æ ¹æ®ä»»åŠ¡IDè·å–ä»»åŠ¡ç±»å‹"""
    try:
        task_id = request.args.get('task_id')
        if not task_id:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘ä»»åŠ¡IDå‚æ•°'})
        
        # å¯¼å…¥å‡½æ•°
        from src.azure_resource_reader import get_task_type_by_job_id
        
        # æŸ¥è¯¢ä»»åŠ¡ç±»å‹
        task_type = get_task_type_by_job_id(task_id)
        
        if task_type:
            return jsonify({
                'success': True,
                'task_type': task_type,
                'task_id': task_id
            })
        else:
            return jsonify({
                'success': False,
                'error': f'æœªæ‰¾åˆ°ä»»åŠ¡ID {task_id} å¯¹åº”çš„ä»»åŠ¡ç±»å‹',
                'task_id': task_id
            })
        
    except Exception as e:
        return jsonify({
            'success': False, 
            'error': f'æŸ¥è¯¢ä»»åŠ¡ç±»å‹æ—¶å‡ºé”™: {str(e)}'
        })

@app.route('/api/check_task_exists')
def check_task_exists():
    """æ£€æŸ¥ä»»åŠ¡IDæ˜¯å¦å·²å­˜åœ¨ï¼ˆä»…ä½¿ç”¨æ•°æ®åº“ï¼‰"""
    try:
        task_id = request.args.get('task_id')
        if not task_id:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘ä»»åŠ¡IDå‚æ•°'})
        
        if not LOCAL_DB_AVAILABLE:
            return jsonify({
                'success': True, 
                'exists': False,
                'task_id': task_id,
                'message': 'æ•°æ®åº“ä¸å¯ç”¨ï¼Œæ— æ³•æ£€æŸ¥ä»»åŠ¡'
            })
        
        db = LocalDatabaseConnector()
        mapping = db.get_task_mapping_by_job_id(task_id)
        db.disconnect()
        
        # æ£€æŸ¥ä»»åŠ¡IDæ˜¯å¦å­˜åœ¨
        exists = mapping is not None
        task_info = mapping if exists else {}
        
        return jsonify({
            'success': True,
            'exists': exists,
            'task_id': task_id,
            'task': task_info  # ä¸å‰ç«¯æœŸæœ›çš„å­—æ®µåä¿æŒä¸€è‡´
        })
        
    except Exception as e:
        logger.warning(f"æ£€æŸ¥ä»»åŠ¡å­˜åœ¨æ€§å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False, 
            'error': f'æ£€æŸ¥ä»»åŠ¡æ—¶å‡ºé”™: {str(e)}'
        })

@app.route('/api/statistics/config', methods=['GET'])
def get_statistics_config():
    """è·å–ç»Ÿè®¡é…ç½®ä¿¡æ¯"""
    try:
        return jsonify({
            'success': True,
            'data': {
                'task_types': TASK_TYPES,
                'tenants': TENANT_CONFIG,
                'database_tables': DATABASE_TABLES
            }
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})


@app.route('/api/statistics/data', methods=['POST'])
def get_statistics_data():
    """è·å–ä»»åŠ¡ç»Ÿè®¡æ•°æ®"""
    try:
        data = request.get_json()
        
        # è·å–å‚æ•°
        start_date = data.get('start_date')
        end_date = data.get('end_date')
        tenant_ids = data.get('tenant_ids', [])
        task_type = data.get('task_type', '')  # æ”¹ä¸ºå•ä¸ªä»»åŠ¡ç±»å‹
        refresh_timestamp = data.get('_refresh')  # åˆ·æ–°æ—¶é—´æˆ³
        
        # å‚æ•°éªŒè¯
        if not start_date or not end_date:
            return jsonify({'success': False, 'error': 'å¼€å§‹æ—¥æœŸå’Œç»“æŸæ—¥æœŸä¸èƒ½ä¸ºç©º'})
        
        if not tenant_ids:
            return jsonify({'success': False, 'error': 'è‡³å°‘é€‰æ‹©ä¸€ä¸ªç§Ÿæˆ·'})
        
        if not task_type:
            return jsonify({'success': False, 'error': 'è¯·é€‰æ‹©ä»»åŠ¡ç±»å‹'})
        
        # ç”Ÿæˆç¼“å­˜é”®ï¼ˆå¦‚æœæœ‰åˆ·æ–°æ—¶é—´æˆ³ï¼Œåˆ™åŒ…å«åœ¨ç¼“å­˜é”®ä¸­ä»¥ç»•è¿‡ç¼“å­˜ï¼‰
        cache_key_params = ['statistics_data', start_date, end_date, tenant_ids, task_type]
        if refresh_timestamp:
            cache_key_params.append(refresh_timestamp)
        cache_key = generate_cache_key(*cache_key_params)
        
        # æ£€æŸ¥ç¼“å­˜ï¼ˆå¦‚æœæ˜¯åˆ·æ–°è¯·æ±‚ï¼Œè·³è¿‡ç¼“å­˜æ£€æŸ¥ï¼‰
        if not refresh_timestamp:
            cached_result = get_from_cache(cache_key)
            if cached_result:
                return jsonify({
                    'success': True,
                    'data': cached_result['data'],
                    'from_cache': True,
                    'cache_time': cached_result['cache_time'],
                    '_debug': cached_result['debug_info']
                })
        
        # åˆ›å»ºæ•°æ®åº“è¿æ¥
        db_config = DB_CONFIG.copy()
        db_config['database'] = 'shulex_collector_prod'
        
        db = DatabaseConnector(db_config)
        if not db.connect():
            return jsonify({'success': False, 'error': 'æ•°æ®åº“è¿æ¥å¤±è´¥'})
        
        try:
            # ä½¿ç”¨ä¼˜åŒ–çš„æŸ¥è¯¢ç­–ç•¥
            statistics_data, debug_info = get_optimized_statistics_data(db, start_date, end_date, tenant_ids, task_type)
            
            # å­˜å…¥ç¼“å­˜
            set_to_cache(cache_key, statistics_data, debug_info)
            
            return jsonify({
                'success': True,
                'data': statistics_data,
                'cache_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                '_debug': debug_info
            })
            
        finally:
            db.disconnect()
            
    except Exception as e:
        logger.error(f"è·å–ç»Ÿè®¡æ•°æ®å¤±è´¥: {str(e)}")
        return jsonify({'success': False, 'error': str(e)})


def generate_cache_key(*args):
    """ç”Ÿæˆç¼“å­˜é”®"""
    key_string = '|'.join(str(arg) for arg in args)
    return hashlib.md5(key_string.encode()).hexdigest()


def get_from_cache(cache_key):
    """ä»ç¼“å­˜è·å–æ•°æ®"""
    if cache_key in statistics_cache:
        cache_entry = statistics_cache[cache_key]
        if time.time() - cache_entry['timestamp'] < CACHE_DURATION:
            return {
                'data': cache_entry['data'],
                'cache_time': cache_entry['cache_time'],
                'debug_info': cache_entry['debug_info']
            }
        else:
            # ç¼“å­˜è¿‡æœŸï¼Œåˆ é™¤
            del statistics_cache[cache_key]
    return None


def set_to_cache(cache_key, data, debug_info=None):
    """è®¾ç½®ç¼“å­˜æ•°æ®"""
    cache_time = datetime.now()
    statistics_cache[cache_key] = {
        'data': data,
        'timestamp': time.time(),
        'cache_time': cache_time.strftime('%Y-%m-%d %H:%M:%S'),
        'debug_info': debug_info or []
    }


def get_optimized_statistics_data(db, start_date, end_date, tenant_ids, task_type):
    """
    ä¼˜åŒ–çš„ç»Ÿè®¡æ•°æ®æŸ¥è¯¢
    ä½¿ç”¨å•ä¸ªæŸ¥è¯¢è·å–æ‰€æœ‰ç»Ÿè®¡æ•°æ®ï¼Œå‡å°‘æ•°æ®åº“å¾€è¿”æ¬¡æ•°
    """
    statistics_data = {
        'failed_count': [],
        'timeout_count': [],
        'total_count': [],
        'timeout_but_succeed': [],  # å·²è¶…æ—¶ä½†å·²å®Œæˆ
        'succeed_count': [],        # å·²å®Œæˆæ•°é‡
        'succeed_not_timeout': [],  # æœªè¶…æ—¶ä¸”å·²å®Œæˆæ•°é‡
        'timeout_not_succeed': []   # è¶…æ—¶æœªå®Œæˆæ•°é‡
    }
    
    debug_info = []
    
    # è¶…æ—¶åˆ¤æ–­åº”è¯¥ä½¿ç”¨å½“å‰UTCæ—¶é—´ï¼Œè€Œä¸æ˜¯æŸ¥è¯¢æ—¥æœŸ
    # è¿™æ ·å¯ä»¥åæ˜ æˆªè‡³å½“å‰æ—¶é—´çš„çœŸå®è¶…æ—¶çŠ¶æ€
    current_utc_time = get_utc_now()
    timeout_condition, _ = get_timeout_condition(current_utc_time)
    
    # è½¬æ¢æŸ¥è¯¢æ—¥æœŸä¸ºUTCæ—¶é—´
    utc_start_date = convert_to_utc_datetime(start_date, "00:00:00")
    utc_end_date = convert_to_utc_datetime(end_date, "23:59:59")
    
    # éå†æ‰€æœ‰åˆ†è¡¨
    for table in DATABASE_TABLES:
        # ä½¿ç”¨å•ä¸ªå¤åˆæŸ¥è¯¢è·å–æ‰€æœ‰ç»Ÿè®¡æ•°æ®
        optimized_sql = f"""
            SELECT 
                DATE(created_at) AS date,
                type as task_type,
                COUNT(*) as total_count,
                SUM(CASE WHEN `status` = 'FAILED' THEN 1 ELSE 0 END) as failed_count,
                SUM(CASE WHEN {timeout_condition} THEN 1 ELSE 0 END) as timeout_count,
                SUM(CASE WHEN `status` = 'SUCCEED' THEN 1 ELSE 0 END) as succeed_count,
                SUM(CASE WHEN ({timeout_condition}) AND `status` = 'SUCCEED' THEN 1 ELSE 0 END) as timeout_but_succeed,
                SUM(CASE WHEN NOT ({timeout_condition}) AND `status` = 'SUCCEED' THEN 1 ELSE 0 END) as succeed_not_timeout,
                SUM(CASE WHEN ({timeout_condition}) AND `status` != 'SUCCEED' AND `status` != 'FAILED' THEN 1 ELSE 0 END) as timeout_not_succeed
            FROM {table}
            WHERE created_at >= %s 
                AND created_at <= %s
                AND tenant_id IN ({','.join(['%s'] * len(tenant_ids))})
                AND type = %s
            GROUP BY date, task_type
            ORDER BY date DESC, task_type
        """
        
        # å‚æ•°ï¼šcurrent_utc_time (4æ¬¡), utc_start_date, utc_end_date, tenant_ids, task_type
        params = [current_utc_time, current_utc_time, current_utc_time, current_utc_time, utc_start_date, utc_end_date] + tenant_ids + [task_type]
        
        # è®°å½•è°ƒè¯•ä¿¡æ¯
        debug_info.append({
            'table': table,
            'sql': clean_sql_for_debug(optimized_sql),
            'params': params,
            'query_time': get_utc_now(),
            'timeout_reference_time': current_utc_time
        })
        
        # æ‰§è¡ŒæŸ¥è¯¢
        results = db.execute_query(optimized_sql, params)
        
        # åˆ†è§£ç»“æœåˆ°ä¸åŒçš„ç»Ÿè®¡ç±»å‹
        for result in results:
            date_str = str(result['date'])
            task_type = result['task_type']
            
            # æ·»åŠ è¡¨åä¿¡æ¯
            base_record = {
                'date': date_str,
                'task_type': task_type,
                'table': table
            }
            
            # åˆ†åˆ«æ·»åŠ åˆ°ä¸åŒçš„ç»Ÿè®¡ç±»å‹
            statistics_data['total_count'].append({
                **base_record,
                'count': result['total_count']
            })
            
            statistics_data['failed_count'].append({
                **base_record,
                'count': result['failed_count']
            })
            
            statistics_data['timeout_count'].append({
                **base_record,
                'count': result['timeout_count']
            })
            
            statistics_data['succeed_count'].append({
                **base_record,
                'count': result['succeed_count']
            })
            
            statistics_data['timeout_but_succeed'].append({
                **base_record,
                'count': result['timeout_but_succeed']
            })
            
            statistics_data['succeed_not_timeout'].append({
                **base_record,
                'count': result['succeed_not_timeout']
            })
            
            statistics_data['timeout_not_succeed'].append({
                **base_record,
                'count': result['timeout_not_succeed']
            })
    
    # æ•°æ®èšåˆå¤„ç†
    return aggregate_statistics_data(statistics_data), debug_info


def aggregate_statistics_data(raw_data):
    """
    èšåˆç»Ÿè®¡æ•°æ®
    
    Args:
        raw_data: åŸå§‹æ•°æ®å­—å…¸
        
    Returns:
        dict: èšåˆåçš„æ•°æ®
    """
    aggregated = {}
    
    for stat_type, records in raw_data.items():
        # æŒ‰æ—¥æœŸå’Œä»»åŠ¡ç±»å‹èšåˆ
        date_task_map = {}
        
        for record in records:
            date_str = str(record['date'])
            task_type = record['task_type']
            count = record['count']
            
            if date_str not in date_task_map:
                date_task_map[date_str] = {}
            
            if task_type not in date_task_map[date_str]:
                date_task_map[date_str][task_type] = 0
            
            date_task_map[date_str][task_type] += count
        
        # è½¬æ¢ä¸ºå‰ç«¯éœ€è¦çš„æ ¼å¼
        aggregated[stat_type] = []
        for date_str in sorted(date_task_map.keys(), reverse=True):
            for task_type, count in date_task_map[date_str].items():
                aggregated[stat_type].append({
                    'date': date_str,
                    'task_type': task_type,
                    'count': count
                })
    
    return aggregated


@app.route('/api/statistics/summary', methods=['POST'])
def get_statistics_summary():
    """è·å–ç»Ÿè®¡æ±‡æ€»æ•°æ®"""
    try:
        data = request.get_json()
        
        # è·å–å‚æ•°
        start_date = data.get('start_date')
        end_date = data.get('end_date')
        tenant_ids = data.get('tenant_ids', [])
        task_type = data.get('task_type', '')  # æ”¹ä¸ºå•ä¸ªä»»åŠ¡ç±»å‹
        refresh_timestamp = data.get('_refresh')  # åˆ·æ–°æ—¶é—´æˆ³
        
        # å‚æ•°éªŒè¯
        if not start_date or not end_date:
            return jsonify({'success': False, 'error': 'å¼€å§‹æ—¥æœŸå’Œç»“æŸæ—¥æœŸä¸èƒ½ä¸ºç©º'})
        
        if not tenant_ids:
            return jsonify({'success': False, 'error': 'è‡³å°‘é€‰æ‹©ä¸€ä¸ªç§Ÿæˆ·'})
        
        if not task_type:
            return jsonify({'success': False, 'error': 'è¯·é€‰æ‹©ä»»åŠ¡ç±»å‹'})
        
        # ç”Ÿæˆç¼“å­˜é”®ï¼ˆå¦‚æœæœ‰åˆ·æ–°æ—¶é—´æˆ³ï¼Œåˆ™åŒ…å«åœ¨ç¼“å­˜é”®ä¸­ä»¥ç»•è¿‡ç¼“å­˜ï¼‰
        cache_key_params = ['statistics_summary', start_date, end_date, tenant_ids, task_type]
        if refresh_timestamp:
            cache_key_params.append(refresh_timestamp)
        cache_key = generate_cache_key(*cache_key_params)
        
        # æ£€æŸ¥ç¼“å­˜ï¼ˆå¦‚æœæ˜¯åˆ·æ–°è¯·æ±‚ï¼Œè·³è¿‡ç¼“å­˜æ£€æŸ¥ï¼‰
        if not refresh_timestamp:
            cached_result = get_from_cache(cache_key)
            if cached_result:
                return jsonify({
                    'success': True,
                    'data': cached_result['data'],
                    'from_cache': True,
                    'cache_time': cached_result['cache_time'],
                    '_debug': cached_result['debug_info']
                })
        
        # åˆ›å»ºæ•°æ®åº“è¿æ¥
        db_config = DB_CONFIG.copy()
        db_config['database'] = 'shulex_collector_prod'
        
        db = DatabaseConnector(db_config)
        if not db.connect():
            return jsonify({'success': False, 'error': 'æ•°æ®åº“è¿æ¥å¤±è´¥'})
        
        try:
            summary_data = {}
            debug_info = []
            
            # è¶…æ—¶åˆ¤æ–­åº”è¯¥ä½¿ç”¨å½“å‰UTCæ—¶é—´ï¼Œè€Œä¸æ˜¯æŸ¥è¯¢æ—¥æœŸ
            # è¿™æ ·å¯ä»¥åæ˜ æˆªè‡³å½“å‰æ—¶é—´çš„çœŸå®è¶…æ—¶çŠ¶æ€
            current_utc_time = get_utc_now()
            timeout_condition, _ = get_timeout_condition(current_utc_time)
            
            # è½¬æ¢æŸ¥è¯¢æ—¥æœŸä¸ºUTCæ—¶é—´
            utc_start_date = convert_to_utc_datetime(start_date, "00:00:00")
            utc_end_date = convert_to_utc_datetime(end_date, "23:59:59")
            
            # éå†æ‰€æœ‰åˆ†è¡¨è·å–æ±‡æ€»æ•°æ®
            for table in DATABASE_TABLES:
                # æ±‡æ€»SQLæ¨¡æ¿ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
                summary_sql = f"""
                    SELECT 
                        type as task_type,
                        COUNT(*) as total_count,
                        SUM(CASE WHEN `status` = 'FAILED' THEN 1 ELSE 0 END) as failed_count,
                        SUM(CASE WHEN {timeout_condition} THEN 1 ELSE 0 END) as timeout_count,
                        SUM(CASE WHEN `status` = 'SUCCEED' THEN 1 ELSE 0 END) as succeed_count,
                        SUM(CASE WHEN ({timeout_condition}) AND `status` = 'SUCCEED' THEN 1 ELSE 0 END) as timeout_but_succeed,
                        SUM(CASE WHEN NOT ({timeout_condition}) AND `status` = 'SUCCEED' THEN 1 ELSE 0 END) as succeed_not_timeout,
                        SUM(CASE WHEN ({timeout_condition}) AND `status` != 'SUCCEED' AND `status` != 'FAILED' THEN 1 ELSE 0 END) as timeout_not_succeed
                    FROM {table}
                    WHERE created_at >= %s 
                        AND created_at <= %s
                        AND tenant_id IN ({','.join(['%s'] * len(tenant_ids))})
                        AND type = %s
                    GROUP BY task_type
                """
                
                # å‚æ•°ï¼šcurrent_utc_time (4æ¬¡), utc_start_date, utc_end_date, tenant_ids, task_type
                params = [current_utc_time, current_utc_time, current_utc_time, current_utc_time, utc_start_date, utc_end_date] + tenant_ids + [task_type]
                
                # è®°å½•è°ƒè¯•ä¿¡æ¯
                debug_info.append({
                    'table': table,
                    'sql': clean_sql_for_debug(summary_sql),
                    'params': params,
                    'query_time': get_utc_now(),
                    'timeout_reference_time': current_utc_time
                })
                
                results = db.execute_query(summary_sql, params)
                
                # èšåˆæ•°æ®
                for result in results:
                    task_type = result['task_type']
                    if task_type not in summary_data:
                        summary_data[task_type] = {
                            'total_count': 0,
                            'failed_count': 0,
                            'timeout_count': 0,
                            'succeed_count': 0,
                            'timeout_but_succeed': 0,
                            'succeed_not_timeout': 0,
                            'timeout_not_succeed': 0
                        }
                    
                    summary_data[task_type]['total_count'] += result['total_count']
                    summary_data[task_type]['failed_count'] += result['failed_count']
                    summary_data[task_type]['timeout_count'] += result['timeout_count']
                    summary_data[task_type]['succeed_count'] += result['succeed_count']
                    summary_data[task_type]['timeout_but_succeed'] += result['timeout_but_succeed']
                    summary_data[task_type]['succeed_not_timeout'] += result['succeed_not_timeout']
                    summary_data[task_type]['timeout_not_succeed'] += result['timeout_not_succeed']
            
            # å­˜å…¥ç¼“å­˜
            set_to_cache(cache_key, summary_data, debug_info)
            
            return jsonify({
                'success': True,
                'data': summary_data,
                'cache_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                '_debug': debug_info
            })
            
        finally:
            db.disconnect()
            
    except Exception as e:
        logger.error(f"è·å–æ±‡æ€»æ•°æ®å¤±è´¥: {str(e)}")
        return jsonify({'success': False, 'error': str(e)})


@app.route('/api/statistics/details', methods=['POST'])
def get_statistics_details():
    """è·å–ç»Ÿè®¡è¯¦ç»†æ•°æ®"""
    try:
        data = request.get_json()
        
        # éªŒè¯å¿…éœ€å‚æ•°
        required_fields = ['start_date', 'end_date', 'tenant_ids', 'task_type', 'detail_type', 'date']
        for field in required_fields:
            if field not in data:
                return jsonify({
                    'success': False,
                    'message': f'ç¼ºå°‘å¿…éœ€å‚æ•°: {field}'
                }), 400
        
        start_date = data['start_date']
        end_date = data['end_date']
        tenant_ids = data['tenant_ids']
        task_type = data['task_type']
        detail_type = data['detail_type']  # 'failed', 'timeout', 'failed_timeout'
        target_date = data['date']
        page = data.get('page', 1)
        page_size = data.get('page_size', 20)
        target_table = data.get('table')  # æ–°å¢ï¼šæŒ‡å®šæŸ¥è¯¢çš„è¡¨ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™æŸ¥è¯¢æ‰€æœ‰è¡¨
        
        # éªŒè¯å‚æ•°
        if not isinstance(tenant_ids, list) or len(tenant_ids) == 0:
            return jsonify({
                'success': False,
                'message': 'ç§Ÿæˆ·IDåˆ—è¡¨ä¸èƒ½ä¸ºç©º'
            }), 400
        
        # è·å–é…ç½®
        tables = TASK_STATISTICS_CONFIG['tables']
        tenants_config = TASK_STATISTICS_CONFIG['tenants']
        
        # å¦‚æœæŒ‡å®šäº†è¡¨ï¼ŒåªæŸ¥è¯¢è¯¥è¡¨
        if target_table:
            if target_table not in tables:
                return jsonify({
                    'success': False,
                    'message': f'æ— æ•ˆçš„è¡¨å: {target_table}'
                }), 400
            tables = [target_table]
        
        # éªŒè¯ç§Ÿæˆ·ID
        valid_tenant_ids = [t['id'] for t in tenants_config]
        for tenant_id in tenant_ids:
            if tenant_id not in valid_tenant_ids:
                return jsonify({
                    'success': False,
                    'message': f'æ— æ•ˆçš„ç§Ÿæˆ·ID: {tenant_id}'
                }), 400
        
        # åˆ›å»ºæ•°æ®åº“è¿æ¥ï¼ˆä¿®å¤ï¼šæ­£ç¡®é…ç½®æ•°æ®åº“ï¼‰
        db_config = DB_CONFIG.copy()
        db_config['database'] = 'shulex_collector_prod'
        
        all_details = []
        debug_info = []
        
        # æŸ¥è¯¢æ¯ä¸ªè¡¨çš„è¯¦ç»†æ•°æ®
        for table in tables:
            job_table = table
            log_table = table.replace('job_', 'log_')
            
            # æ„å»ºSQLæŸ¥è¯¢
            tenant_placeholders = ', '.join(['%s'] * len(tenant_ids))
            
            # æ ¹æ®è¯¦æƒ…ç±»å‹æ„å»ºä¸åŒçš„WHEREæ¡ä»¶
            # è½¬æ¢æ—¥æœŸä¸ºUTCæ—¶é—´
            target_date_start = convert_to_utc_datetime(target_date, "00:00:00")
            target_date_end = convert_to_utc_datetime(target_date, "23:59:59")
            
            # è®¡ç®—æœ€ç»ˆçš„æ—¥æœŸèŒƒå›´ï¼ˆå–äº¤é›†ï¼‰
            final_start_date = max(convert_to_utc_datetime(start_date, "00:00:00"), target_date_start)
            final_end_date = min(convert_to_utc_datetime(end_date, "23:59:59"), target_date_end)
            
            if detail_type == 'failed':
                where_condition = f"""
                WHERE a.created_at >= %s 
                    AND a.created_at <= %s 
                    AND a.tenant_id IN ({tenant_placeholders})
                    AND a.type = %s
                    AND a.status = 'FAILED'
                """
                params = [
                    final_start_date,
                    final_end_date
                ] + tenant_ids + [
                    task_type
                ]
            elif detail_type == 'timeout':
                # è¶…æ—¶åˆ¤æ–­åº”è¯¥ä½¿ç”¨å½“å‰UTCæ—¶é—´ï¼Œè€Œä¸æ˜¯ç›®æ ‡æ—¥æœŸ
                # è¿™æ ·å¯ä»¥åæ˜ æˆªè‡³å½“å‰æ—¶é—´çš„çœŸå®è¶…æ—¶çŠ¶æ€
                current_utc_time = get_utc_now()
                timeout_condition, _ = get_timeout_condition(current_utc_time)
                
                where_condition = f"""
                WHERE a.created_at >= %s 
                    AND a.created_at <= %s 
                    AND a.tenant_id IN ({tenant_placeholders})
                    AND a.type = %s
                    AND {timeout_condition}
                """
                params = [
                    final_start_date,
                    final_end_date
                ] + tenant_ids + [
                    task_type,
                    current_utc_time
                ]
            elif detail_type == 'timeout_but_succeed':
                # å·²è¶…æ—¶ä½†å·²å®Œæˆ
                current_utc_time = get_utc_now()
                timeout_condition, _ = get_timeout_condition(current_utc_time)
                
                where_condition = f"""
                WHERE a.created_at >= %s 
                    AND a.created_at <= %s 
                    AND a.tenant_id IN ({tenant_placeholders})
                    AND a.type = %s
                    AND {timeout_condition}
                    AND a.status = 'SUCCEED'
                """
                params = [
                    final_start_date,
                    final_end_date
                ] + tenant_ids + [
                    task_type,
                    current_utc_time
                ]
            elif detail_type == 'succeed':
                # å·²å®Œæˆ
                where_condition = f"""
                WHERE a.created_at >= %s 
                    AND a.created_at <= %s 
                    AND a.tenant_id IN ({tenant_placeholders})
                    AND a.type = %s
                    AND a.status = 'SUCCEED'
                """
                params = [
                    final_start_date,
                    final_end_date
                ] + tenant_ids + [
                    task_type
                ]
            elif detail_type == 'succeed_not_timeout':
                # æœªè¶…æ—¶ä¸”å·²å®Œæˆ
                current_utc_time = get_utc_now()
                timeout_condition, _ = get_timeout_condition(current_utc_time)
                
                where_condition = f"""
                WHERE a.created_at >= %s 
                    AND a.created_at <= %s 
                    AND a.tenant_id IN ({tenant_placeholders})
                    AND a.type = %s
                    AND NOT ({timeout_condition})
                    AND a.status = 'SUCCEED'
                """
                params = [
                    final_start_date,
                    final_end_date
                ] + tenant_ids + [
                    task_type,
                    current_utc_time
                ]
            elif detail_type == 'timeout_not_succeed':
                # è¶…æ—¶æœªå®Œæˆ
                current_utc_time = get_utc_now()
                timeout_condition, _ = get_timeout_condition(current_utc_time)
                
                where_condition = f"""
                WHERE a.created_at >= %s 
                    AND a.created_at <= %s 
                    AND a.tenant_id IN ({tenant_placeholders})
                    AND a.type = %s
                    AND {timeout_condition}
                    AND a.status != 'SUCCEED'
                    AND a.status != 'FAILED'
                """
                params = [
                    final_start_date,
                    final_end_date
                ] + tenant_ids + [
                    task_type,
                    current_utc_time
                ]
            else:
                return jsonify({
                    'success': False,
                    'message': f'ä¸æ”¯æŒçš„è¯¦æƒ…ç±»å‹: {detail_type}'
                }), 400
            
            sql = f"""
            SELECT 
                a.created_at,
                a.break_at,
                a.deliver_at,
                a.req_ssn,
                a.payload,
                a.result,
                b.ext_ssn,
                b.analysis_response,
                b.response,
                a.status,
                b.state as log_state,
                '{table}' as source_table
            FROM {job_table} a 
            LEFT JOIN {log_table} b ON b.req_ssn = a.req_ssn
            {where_condition}
            ORDER BY a.created_at DESC
            LIMIT {page_size} OFFSET {(page - 1) * page_size}
            """
            
            # è®°å½•è°ƒè¯•ä¿¡æ¯
            debug_info.append({
                'table': table,
                'sql': clean_sql_for_debug(sql),
                'params': params,
                'detail_type': detail_type,
                'query_time': get_utc_now()
            })
            
            try:
                # æ‰§è¡ŒæŸ¥è¯¢
                connector = DatabaseConnector(db_config)
                if not connector.connect():
                    logger.warning(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {table}")
                    continue
                
                results = connector.execute_query(sql, params)
                
                # å¤„ç†ç»“æœ
                for row in results:
                    # å¤„ç†å­—å…¸æ ¼å¼çš„ç»“æœ
                    if isinstance(row, dict):
                        # æ ¼å¼åŒ–JSONå­—æ®µ
                        result_formatted = format_json_field(row.get('result'))
                        response_formatted = format_json_field(row.get('response'))
                        
                        # åˆ¤æ–­æ˜¯å¦æ˜¾ç¤ºé‡çˆ¬æŒ‰é’®
                        status = row.get('status') or ''
                        show_recrawl = should_show_recrawl_button(status, result_formatted)
                        
                        detail_item = {
                            'created_at': row.get('created_at').strftime('%Y-%m-%d %H:%M:%S') if row.get('created_at') else '',
                            'break_at': row.get('break_at').strftime('%Y-%m-%d %H:%M:%S') if row.get('break_at') else '',
                            'deliver_at': row.get('deliver_at').strftime('%Y-%m-%d %H:%M:%S') if row.get('deliver_at') else '',
                            'req_ssn': row.get('req_ssn') or '',
                            'payload': row.get('payload') or '',
                            'result': result_formatted,
                            'ext_ssn': row.get('ext_ssn') or '',
                            'analysis_response': row.get('analysis_response') or '',
                            'response': response_formatted,
                            'status': status,
                            'log_state': row.get('log_state') or '',
                            'source_table': row.get('source_table') or table,
                            'show_recrawl_button': show_recrawl
                        }
                    else:
                        # å¤„ç†å…ƒç»„æ ¼å¼çš„ç»“æœ
                        # æ ¼å¼åŒ–JSONå­—æ®µ
                        result_formatted = format_json_field(row[5] if len(row) > 5 else '')
                        response_formatted = format_json_field(row[8] if len(row) > 8 else '')
                        
                        # åˆ¤æ–­æ˜¯å¦æ˜¾ç¤ºé‡çˆ¬æŒ‰é’®
                        status = row[9] if len(row) > 9 else ''
                        show_recrawl = should_show_recrawl_button(status, result_formatted)
                        
                        detail_item = {
                            'created_at': row[0].strftime('%Y-%m-%d %H:%M:%S') if row[0] else '',
                            'break_at': row[1].strftime('%Y-%m-%d %H:%M:%S') if row[1] else '',
                            'deliver_at': row[2].strftime('%Y-%m-%d %H:%M:%S') if row[2] else '',
                            'req_ssn': row[3] or '',
                            'payload': row[4] or '',
                            'result': result_formatted,
                            'ext_ssn': row[6] or '',
                            'analysis_response': row[7] or '',
                            'response': response_formatted,
                            'status': status,
                            'log_state': row[10] or '',
                            'source_table': row[11] or table,
                            'show_recrawl_button': show_recrawl
                        }
                    all_details.append(detail_item)
                
                connector.disconnect()
                    
            except Exception as e:
                logger.warning(f"æŸ¥è¯¢è¡¨ {table} è¯¦ç»†æ•°æ®å¤±è´¥: {str(e)}")
                debug_info[-1]['error'] = str(e)
                continue
        
        # æŒ‰åˆ›å»ºæ—¶é—´æ’åº
        all_details.sort(key=lambda x: x['created_at'], reverse=True)
        
        # åˆ†é¡µå¤„ç†ï¼ˆå·²åœ¨SQLä¸­å¤„ç†ï¼Œè¿™é‡Œåªæ˜¯ä¸ºäº†è®¡ç®—æ€»æ•°ï¼‰
        total_count = len(all_details)
        
        return jsonify({
            'success': True,
            'data': {
                'details': all_details,
                'total_count': total_count,
                'page': page,
                'page_size': page_size,
                'total_pages': (total_count + page_size - 1) // page_size if total_count > 0 else 0,
                'queried_tables': tables  # è¿”å›æŸ¥è¯¢çš„è¡¨åˆ—è¡¨
            },
            '_debug': debug_info
        })
        
    except Exception as e:
        logger.error(f"è·å–ç»Ÿè®¡è¯¦ç»†æ•°æ®å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'message': f'è·å–è¯¦ç»†æ•°æ®å¤±è´¥: {str(e)}'
        }), 500


def should_show_recrawl_button(status, result_data):
    """
    åˆ¤æ–­æ˜¯å¦åº”è¯¥æ˜¾ç¤ºé‡çˆ¬æŒ‰é’®
    
    Args:
        status: ä»»åŠ¡çŠ¶æ€
        result_data: æ ¼å¼åŒ–åçš„resultæ•°æ®å­—å…¸
        
    Returns:
        bool: æ˜¯å¦æ˜¾ç¤ºé‡çˆ¬æŒ‰é’®
    """
    # æ£€æŸ¥çŠ¶æ€æ˜¯å¦ä¸ºFAILED
    if status != 'FAILED':
        return False
    
    # æ£€æŸ¥resultæ•°æ®æ˜¯å¦æœ‰æ•ˆ
    if not result_data or not result_data.get('is_valid_json'):
        return False
    
    try:
        # è§£æJSONæ•°æ®
        if isinstance(result_data.get('raw'), str):
            result_json = json.loads(result_data.get('raw'))
        else:
            result_json = result_data.get('raw')
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç‰¹å®šçš„é”™è¯¯ä¿¡æ¯
        expected_error = "ä½œä¸šæäº¤å¤±è´¥: 408 IORuntimeException - SocketTimeoutException: connect timed out"
        
        if isinstance(result_json, dict) and 'E3001' in result_json:
            actual_error = result_json.get('E3001', '')
            return actual_error == expected_error
        
        return False
        
    except (json.JSONDecodeError, TypeError, AttributeError) as e:
        logger.warning(f"âš ï¸ è§£æresultæ•°æ®å¤±è´¥: {e}")
        return False


def format_json_field(json_str):
    """
    æ ¼å¼åŒ–JSONå­—æ®µ
    
    Args:
        json_str: JSONå­—ç¬¦ä¸²
        
    Returns:
        dict: åŒ…å«æ ¼å¼åŒ–ä¿¡æ¯çš„å­—å…¸
    """
    if not json_str:
        return {
            'raw': '',
            'formatted': '',
            'is_valid_json': False,
            'error': ''
        }
    
    try:
        # å°è¯•è§£æJSON
        if isinstance(json_str, str):
            parsed_json = json.loads(json_str)
        else:
            parsed_json = json_str
        
        # æ ¼å¼åŒ–JSON
        formatted_json = json.dumps(parsed_json, indent=2, ensure_ascii=False)
        
        return {
            'raw': json_str,
            'formatted': formatted_json,
            'is_valid_json': True,
            'error': ''
        }
    except (json.JSONDecodeError, TypeError) as e:
        return {
            'raw': json_str,
            'formatted': str(json_str),
            'is_valid_json': False,
            'error': str(e)
        }


@app.route('/api/resubmit_crawler', methods=['POST'])
def resubmit_crawler():
    """é‡æ–°æäº¤çˆ¬è™«ä»»åŠ¡"""
    try:
        data = request.get_json()
        
        # éªŒè¯å¿…éœ€å‚æ•°
        if 'req_ssn' not in data:
            return jsonify({
                'success': False,
                'message': 'ç¼ºå°‘å¿…éœ€å‚æ•°: req_ssn'
            }), 400
        
        req_ssn = data['req_ssn']
        
        # æ„å»ºå‘½ä»¤
        job_id = f"SL{req_ssn}" if not req_ssn.startswith('SL') else req_ssn
        command = f"python3 src/main.py resubmit_crawler_jobs --job-ids {job_id}"
        
        logger.info(f"ğŸ”„ æ‰§è¡Œé‡çˆ¬å‘½ä»¤: {command} (req_ssn: {req_ssn})")
        
        # å¼‚æ­¥æ‰§è¡Œå‘½ä»¤
        def run_resubmit_command():
            try:
                process = subprocess.Popen(
                    command,
                    shell=True,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.STDOUT,
                    text=True,
                    cwd=os.getcwd(),
                    bufsize=1,
                    universal_newlines=True
                )
                
                # è¯»å–è¾“å‡º
                output_lines = []
                while True:
                    output = process.stdout.readline()
                    if output == '' and process.poll() is not None:
                        break
                    if output:
                        line = output.strip()
                        output_lines.append(line)
                        logger.info(f"ğŸ“ é‡çˆ¬è¾“å‡º: {line}")
                
                return_code = process.poll()
                
                final_output = '\n'.join(output_lines)
                success = return_code == 0
                
                logger.info(f"âœ… é‡çˆ¬å‘½ä»¤å®Œæˆï¼Œè¿”å›ç : {return_code}")
                logger.info(f"ğŸ“‹ å®Œæ•´è¾“å‡º:\n{final_output}")
                
                return {
                    'success': success,
                    'return_code': return_code,
                    'output': final_output,
                    'job_id': job_id,
                    'req_ssn': req_ssn
                }
                
            except Exception as e:
                error_msg = f"æ‰§è¡Œé‡çˆ¬å‘½ä»¤æ—¶å‡ºé”™: {str(e)}"
                logger.error(error_msg)
                return {
                    'success': False,
                    'error': error_msg,
                    'job_id': job_id,
                    'req_ssn': req_ssn
                }
        
        # åœ¨æ–°çº¿ç¨‹ä¸­æ‰§è¡Œå‘½ä»¤
        thread = threading.Thread(target=run_resubmit_command)
        thread.daemon = True
        thread.start()
        
        return jsonify({
            'success': True,
            'message': f'é‡çˆ¬ä»»åŠ¡å·²æäº¤ï¼Œä»»åŠ¡ID: {job_id}',
            'job_id': job_id,
            'req_ssn': req_ssn,
            'command': command
        })
        
    except Exception as e:
        error_msg = f"æäº¤é‡çˆ¬ä»»åŠ¡å¤±è´¥: {str(e)}"
        logger.error(error_msg)
        return jsonify({
            'success': False,
            'message': error_msg
        }), 500


@app.route('/api/task_mappings', methods=['GET'])
def get_task_mappings():
    """
    è·å–ä»»åŠ¡æ˜ å°„è®°å½•ï¼ˆä¼˜å…ˆä»æ•°æ®åº“ï¼Œå¤‡ç”¨JSONæ–‡ä»¶ï¼‰
    æ”¯æŒåˆ†é¡µå’Œæœç´¢
    """
    try:
        # è·å–æŸ¥è¯¢å‚æ•°
        page = int(request.args.get('page', 1))
        per_page = int(request.args.get('per_page', 50))
        search = request.args.get('search', '').strip()
        
        # ğŸ†• ä¼˜å…ˆå°è¯•ä»æ•°æ®åº“è·å–
        if LOCAL_DB_AVAILABLE:
            try:
                db = LocalDatabaseConnector()
                offset = (page - 1) * per_page
                
                if search:
                    # ç®€å•çš„æœç´¢å®ç°ï¼ˆåœ¨job_idå’Œtask_typeä¸­æœç´¢ï¼‰
                    # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…å¯ä»¥æ‰©å±•æ›´å¤æ‚çš„æœç´¢é€»è¾‘
                    mappings = []
                    all_mappings = db.get_all_task_mappings(limit=1000, offset=0)
                    for mapping in all_mappings:
                        if (search.lower() in mapping.get('job_id', '').lower() or 
                            search.lower() in mapping.get('task_type', '').lower()):
                            mappings.append(mapping)
                    
                    # æ‰‹åŠ¨åˆ†é¡µ
                    total = len(mappings)
                    mappings = mappings[offset:offset + per_page]
                else:
                    mappings = db.get_all_task_mappings(limit=per_page, offset=offset)
                    # è¿™é‡Œåº”è¯¥ä¹ŸæŸ¥è¯¢æ€»æ•°ï¼Œç®€åŒ–å¤„ç†
                    total = len(mappings) + offset  # ç®€åŒ–çš„æ€»æ•°ä¼°ç®—
                
                db.disconnect()
                
                # ä¸ºæ¯ä¸ªæ˜ å°„æ·»åŠ æ–‡ä»¶è¯¦æƒ…
                for mapping in mappings:
                    mapping_id = mapping.get('id')
                    if mapping_id:
                        db = LocalDatabaseConnector()
                        file_details = db.get_file_details_by_mapping_id(mapping_id)
                        mapping['files'] = file_details
                        db.disconnect()
                
                return jsonify({
                    'success': True,
                    'data': mappings,
                    'pagination': {
                        'page': page,
                        'per_page': per_page,
                        'total': total,
                        'pages': (total + per_page - 1) // per_page
                    },
                    'source': 'database'
                })
                
            except Exception as db_error:
                logger.warning(f"æ•°æ®åº“æŸ¥è¯¢å¤±è´¥ï¼Œå›é€€åˆ°JSONæ–‡ä»¶: {str(db_error)}")
        
        # ğŸ”„ å›é€€åˆ°JSONæ–‡ä»¶
        mapping_file = 'data/output/task_mapping.json'
        if not os.path.exists(mapping_file):
            return jsonify({
                'success': True,
                'data': [],
                'pagination': {
                    'page': page,
                    'per_page': per_page,
                    'total': 0,
                    'pages': 0
                },
                'source': 'json_file'
            })
        
        with open(mapping_file, 'r', encoding='utf-8') as f:
            mapping_data = json.load(f)
        
        # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼å¹¶æ·»åŠ æ–‡ä»¶ç»Ÿè®¡
        mappings = []
        for job_id, info in mapping_data.items():
            if search and search.lower() not in job_id.lower() and search.lower() not in info.get('task_type', '').lower():
                continue
                
            # ç»Ÿè®¡æ–‡ä»¶ä¿¡æ¯
            relative_path = info.get('relative_path', '')
            if relative_path.startswith('./'):
                full_path = f"data/output/{relative_path[2:]}"
            else:
                full_path = f"data/output/{relative_path}"
            
            file_count = 0
            has_parse_file = False
            if os.path.exists(full_path):
                files = [f for f in os.listdir(full_path) if os.path.isfile(os.path.join(full_path, f))]
                file_count = len(files)
                has_parse_file = 'parse_result.json' in files
            
            mapping = {
                'job_id': job_id,
                'task_type': info.get('task_type', ''),
                'actual_task_id': info.get('actual_task_id', ''),
                'relative_path': relative_path,
                'full_path': full_path,
                'file_count': file_count,
                'has_parse_file': has_parse_file,
                'status': 'success',
                'last_updated': info.get('last_updated', ''),
                'source': 'json_file'
            }
            mappings.append(mapping)
        
        # æ’åºï¼ˆæŒ‰æœ€åæ›´æ–°æ—¶é—´å€’åºï¼‰
        mappings.sort(key=lambda x: x.get('last_updated', ''), reverse=True)
        
        # åˆ†é¡µ
        total = len(mappings)
        start_idx = (page - 1) * per_page
        end_idx = start_idx + per_page
        mappings = mappings[start_idx:end_idx]
        
        return jsonify({
            'success': True,
            'data': mappings,
            'pagination': {
                'page': page,
                'per_page': per_page,
                'total': total,
                'pages': (total + per_page - 1) // per_page
            },
            'source': 'json_file'
        })
        
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡æ˜ å°„å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/task_mapping/<job_id>', methods=['GET'])
def get_task_mapping_detail(job_id):
    """
    è·å–å•ä¸ªä»»åŠ¡æ˜ å°„çš„è¯¦ç»†ä¿¡æ¯
    """
    try:
        # ğŸ†• ä¼˜å…ˆå°è¯•ä»æ•°æ®åº“è·å–
        if LOCAL_DB_AVAILABLE:
            try:
                db = LocalDatabaseConnector()
                mapping = db.get_task_mapping_by_job_id(job_id)
                
                if mapping:
                    # è·å–æ–‡ä»¶è¯¦æƒ…
                    file_details = db.get_file_details_by_mapping_id(mapping['id'])
                    mapping['files'] = file_details
                    mapping['source'] = 'database'
                    
                    db.disconnect()
                    return jsonify({
                        'success': True,
                        'data': mapping
                    })
                
                db.disconnect()
                
            except Exception as db_error:
                logger.warning(f"æ•°æ®åº“æŸ¥è¯¢å¤±è´¥ï¼Œå›é€€åˆ°JSONæ–‡ä»¶: {str(db_error)}")
        
        # ğŸ”„ å›é€€åˆ°JSONæ–‡ä»¶
        mapping_file = 'data/output/task_mapping.json'
        if not os.path.exists(mapping_file):
            return jsonify({
                'success': False,
                'error': 'ä»»åŠ¡æ˜ å°„æ–‡ä»¶ä¸å­˜åœ¨'
            }), 404
        
        with open(mapping_file, 'r', encoding='utf-8') as f:
            mapping_data = json.load(f)
        
        if job_id not in mapping_data:
            return jsonify({
                'success': False,
                'error': 'ä»»åŠ¡æ˜ å°„ä¸å­˜åœ¨'
            }), 404
        
        info = mapping_data[job_id]
        
        # ç»Ÿè®¡æ–‡ä»¶ä¿¡æ¯
        relative_path = info.get('relative_path', '')
        if relative_path.startswith('./'):
            full_path = f"data/output/{relative_path[2:]}"
        else:
            full_path = f"data/output/{relative_path}"
        
        files = []
        if os.path.exists(full_path):
            for filename in os.listdir(full_path):
                file_path = os.path.join(full_path, filename)
                if os.path.isfile(file_path):
                    file_type = 'parse' if filename == 'parse_result.json' else 'original'
                    files.append({
                        'file_name': filename,
                        'file_type': file_type,
                        'file_size': os.path.getsize(file_path),
                        'file_path': file_path
                    })
        
        mapping = {
            'job_id': job_id,
            'task_type': info.get('task_type', ''),
            'actual_task_id': info.get('actual_task_id', ''),
            'relative_path': relative_path,
            'full_path': full_path,
            'file_count': len(files),
            'has_parse_file': any(f['file_name'] == 'parse_result.json' for f in files),
            'status': 'success',
            'last_updated': info.get('last_updated', ''),
            'files': files,
            'source': 'json_file'
        }
        
        return jsonify({
            'success': True,
            'data': mapping
        })
        
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡æ˜ å°„è¯¦æƒ…å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/database_status', methods=['GET'])
def get_database_status():
    """
    è·å–æ•°æ®åº“çŠ¶æ€ä¿¡æ¯
    """
    try:
        if not LOCAL_DB_AVAILABLE:
            return jsonify({
                'success': True,
                'data': {
                    'available': False,
                    'reason': 'æœ¬åœ°æ•°æ®åº“æ¨¡å—ä¸å¯ç”¨'
                }
            })
        
        db = LocalDatabaseConnector()
        connected = db.test_connection()
        
        if connected:
            # è·å–ç»Ÿè®¡ä¿¡æ¯
            db.connect()
            cursor = db.cursor
            
            # è·å–ä»»åŠ¡æ˜ å°„æ•°é‡
            cursor.execute("SELECT COUNT(*) as total FROM task_mapping")
            total_mappings = cursor.fetchone()['total']
            
            # è·å–ä»»åŠ¡ç±»å‹åˆ†å¸ƒ
            cursor.execute("SELECT task_type, COUNT(*) as count FROM task_mapping GROUP BY task_type")
            task_type_distribution = cursor.fetchall()
            
            # è·å–æœ€è¿‘æ›´æ–°æ—¶é—´
            cursor.execute("SELECT MAX(updated_at) as last_updated FROM task_mapping")
            last_updated = cursor.fetchone()['last_updated']
            
            db.disconnect()
            
            return jsonify({
                'success': True,
                'data': {
                    'available': True,
                    'connected': True,
                    'statistics': {
                        'total_mappings': total_mappings,
                        'task_type_distribution': task_type_distribution,
                        'last_updated': last_updated.isoformat() if last_updated else None
                    }
                }
            })
        else:
            return jsonify({
                'success': True,
                'data': {
                    'available': True,
                    'connected': False,
                    'reason': 'æ— æ³•è¿æ¥åˆ°æ•°æ®åº“'
                }
            })
        
    except Exception as e:
        logger.error(f"è·å–æ•°æ®åº“çŠ¶æ€å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/delete_task/<job_id>', methods=['DELETE'])
def delete_task(job_id):
    """åˆ é™¤ä»»åŠ¡ï¼ˆä»…ä»æ•°æ®åº“åˆ é™¤ï¼‰"""
    logger.info(f"ğŸ“¥ æ”¶åˆ°åˆ é™¤ä»»åŠ¡è¯·æ±‚: job_id={job_id}")
    try:
        if not LOCAL_DB_AVAILABLE:
            return jsonify({
                'success': False,
                'error': 'æœ¬åœ°æ•°æ®åº“ä¸å¯ç”¨'
            }), 500
        
        db = LocalDatabaseConnector()
        
        # é¦–å…ˆè·å–ä»»åŠ¡ä¿¡æ¯
        mapping = db.get_task_mapping_by_job_id(job_id)
        if not mapping:
            db.disconnect()
            return jsonify({
                'success': False,
                'error': 'ä»»åŠ¡è®°å½•ä¸å­˜åœ¨'
            }), 404
        
        # åˆ é™¤ç‰©ç†æ–‡ä»¶
        full_path = mapping.get('full_path', '')
        if full_path and os.path.exists(full_path):
            try:
                import shutil
                shutil.rmtree(full_path)
                logger.info(f"å·²åˆ é™¤æ–‡ä»¶ç›®å½•: {full_path}")
            except Exception as file_error:
                logger.warning(f"åˆ é™¤æ–‡ä»¶ç›®å½•å¤±è´¥: {str(file_error)}")
        
        # åˆ é™¤æ•°æ®åº“è®°å½•ï¼ˆç”±äºå¤–é”®çº¦æŸï¼Œä¼šè‡ªåŠ¨åˆ é™¤ç›¸å…³çš„æ–‡ä»¶è¯¦æƒ…è®°å½•ï¼‰
        cursor = db.cursor
        delete_query = "DELETE FROM task_mapping WHERE job_id = %s"
        cursor.execute(delete_query, (job_id,))
        db.connection.commit()
        
        deleted_rows = cursor.rowcount
        db.disconnect()
        
        if deleted_rows > 0:
            logger.info(f"æˆåŠŸåˆ é™¤ä»»åŠ¡è®°å½•: {job_id}")
            return jsonify({
                'success': True,
                'message': f'ä»»åŠ¡ {job_id} åˆ é™¤æˆåŠŸ'
            })
        else:
            return jsonify({
                'success': False,
                'error': 'åˆ é™¤å¤±è´¥ï¼Œè®°å½•ä¸å­˜åœ¨'
            }), 404
        
    except Exception as e:
        logger.error(f"åˆ é™¤ä»»åŠ¡å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/task_detail/<job_id>', methods=['GET'])
def get_task_detail(job_id):
    """è·å–ä»»åŠ¡è¯¦æƒ…ï¼ˆä»…ä»æ•°æ®åº“è·å–ï¼‰"""
    logger.info(f"ğŸ“¥ æ”¶åˆ°è·å–ä»»åŠ¡è¯¦æƒ…è¯·æ±‚: job_id={job_id}")
    try:
        if not LOCAL_DB_AVAILABLE:
            return jsonify({
                'success': False,
                'error': 'æœ¬åœ°æ•°æ®åº“ä¸å¯ç”¨'
            }), 500
        
        db = LocalDatabaseConnector()
        mapping = db.get_task_mapping_by_job_id(job_id)
        
        if not mapping:
            db.disconnect()
            return jsonify({
                'success': False,
                'error': 'ä»»åŠ¡è®°å½•ä¸å­˜åœ¨'
            }), 404
        
        # è·å–æ–‡ä»¶è¯¦æƒ…
        file_details = db.get_file_details_by_mapping_id(mapping['id'])
        mapping['files'] = file_details
        mapping['source'] = 'database'
        
        db.disconnect()
        
        # ä¸ºæ¯ä¸ªæ–‡ä»¶æ·»åŠ å†…å®¹é¢„è§ˆ
        for file_info in mapping.get('files', []):
            file_path = file_info.get('file_path', '')
            if os.path.exists(file_path):
                try:
                    # æ ¹æ®æ–‡ä»¶ç±»å‹å†³å®šé¢„è§ˆå†…å®¹
                    if file_info.get('file_name', '').endswith('.json'):
                        # JSONæ–‡ä»¶ï¼Œè¯»å–å¹¶æ ¼å¼åŒ–
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = json.load(f)
                            file_info['preview'] = json.dumps(content, indent=2, ensure_ascii=False)[:1000]
                            file_info['preview_type'] = 'json'
                    elif file_info.get('file_name', '').endswith('.html'):
                        # HTMLæ–‡ä»¶ï¼Œè¯»å–å‰500å­—ç¬¦
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                            file_info['preview'] = content[:500] + ('...' if len(content) > 500 else '')
                            file_info['preview_type'] = 'html'
                    else:
                        # å…¶ä»–æ–‡ä»¶ï¼Œç®€å•æ–‡æœ¬é¢„è§ˆ
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                            file_info['preview'] = content[:300] + ('...' if len(content) > 300 else '')
                            file_info['preview_type'] = 'text'
                except Exception as preview_error:
                    file_info['preview'] = f'é¢„è§ˆå¤±è´¥: {str(preview_error)}'
                    file_info['preview_type'] = 'error'
            else:
                file_info['preview'] = 'æ–‡ä»¶ä¸å­˜åœ¨'
                file_info['preview_type'] = 'error'
        
        return jsonify({
            'success': True,
            'data': mapping
        })
        
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡è¯¦æƒ…å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


if __name__ == '__main__':
    # ç¡®ä¿é™æ€æ–‡ä»¶ç›®å½•å­˜åœ¨
    os.makedirs('static', exist_ok=True)
    
    logger.info("ğŸš€ ä»»åŠ¡æ£€æŸ¥å·¥å…·çœ‹æ¿å¯åŠ¨ä¸­...")
    logger.info("ğŸ“Š ç»Ÿä¸€çš„ä»»åŠ¡ç®¡ç†å’Œç›‘æ§å¹³å°")
    logger.info("ğŸ“‹ è®¿é—®åœ°å€: http://localhost:5001")
    logger.info("=" * 50)
    
    # æ£€æŸ¥ç³»ç»ŸçŠ¶æ€
    logger.info("ğŸ” ç³»ç»ŸçŠ¶æ€æ£€æŸ¥:")
    logger.info(f"   ğŸ“ æ—¥å¿—ç›®å½•: {Path('logs').absolute()}")
    logger.info(f"   ğŸ—„ï¸  æœ¬åœ°æ•°æ®åº“: {'âœ… å¯ç”¨' if LOCAL_DB_AVAILABLE else 'âŒ ä¸å¯ç”¨'}")
    
    if LOCAL_DB_AVAILABLE:
        try:
            test_db = LocalDatabaseConnector()
            if test_db.connect():
                logger.info("   ğŸ”— æ•°æ®åº“è¿æ¥: âœ… æµ‹è¯•æˆåŠŸ")
                test_db.disconnect()
            else:
                logger.warning("   ğŸ”— æ•°æ®åº“è¿æ¥: âš ï¸ æµ‹è¯•å¤±è´¥")
        except Exception as e:
            logger.warning(f"   ğŸ”— æ•°æ®åº“è¿æ¥: âŒ æµ‹è¯•å¼‚å¸¸ - {str(e)}")
    
    logger.info("ğŸ¯ å¯åŠ¨å®Œæˆï¼Œå¼€å§‹ç›‘å¬è¯·æ±‚...")
    
    app.run(debug=True, host='0.0.0.0', port=5001) 