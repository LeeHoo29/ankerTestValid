#!/usr/bin/env python3
"""
Azure Storage èµ„æºè¯»å–å™¨çš„ --with-parse æ¨¡å¼ä¼˜åŒ–ç‰ˆæœ¬
ä¸“é—¨ç”¨äºåŒæ—¶è·å–åŸå§‹æ•°æ®å’Œè§£ææ•°æ®ï¼Œé›†æˆäº†analysis_responseä¼˜åŒ–åŠŸèƒ½
"""
import sys
import argparse
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))

from src.azure_resource_reader import (
    AzureResourceReader, 
    is_valid_task_id, 
    convert_job_id_to_task_id,
    get_default_files_for_task_type,
    update_task_mapping,
    print_task_mapping_info,
    _generate_save_filename,
    _save_content_to_file
)
import json


def handle_with_parse_mode_optimized(args) -> None:
    """
    å¤„ç†åŒæ—¶è·å–åŸå§‹æ•°æ®å’Œè§£ææ•°æ®çš„æ¨¡å¼ (é›†æˆä¼˜åŒ–ç‰ˆæœ¬)
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
    """
    print(f"ğŸ” Azure Storage èµ„æºè¯»å–å™¨ (åŸå§‹æ•°æ® + è§£ææ•°æ® - ä¼˜åŒ–ç‰ˆæœ¬)")
    print(f"ğŸ“‹ ä»»åŠ¡ç±»å‹: {args.task_type_or_job_id}")
    print(f"ğŸ“‹ è¾“å…¥å‚æ•°: {args.task_id_or_task_id}")
    
    # ä¿å­˜åŸå§‹è¾“å…¥å‚æ•°ç”¨äºæ˜ å°„
    original_input = args.task_id_or_task_id
    
    # ç¬¬ä¸€æ­¥ï¼šéªŒè¯å’Œè½¬æ¢ä»»åŠ¡IDï¼ŒåŒæ—¶è·å–analysis_response
    task_id = None
    analysis_response = None
    job_id = None
    
    if is_valid_task_id(args.task_id_or_task_id):
        # ç›´æ¥ä½¿ç”¨ä½œä¸ºä»»åŠ¡ID
        task_id = args.task_id_or_task_id
        print(f"âœ… æ£€æµ‹åˆ°æœ‰æ•ˆçš„ä»»åŠ¡ID: {task_id}")
    else:
        # éœ€è¦è½¬æ¢ä¸ºä»»åŠ¡IDå¹¶è·å–analysis_response
        job_id = args.task_id_or_task_id
        
        # å¦‚æœæ˜¯çº¯æ•°å­—ï¼Œæ·»åŠ SLå‰ç¼€
        if job_id.isdigit():
            job_id = f"SL{job_id}"
            print(f"ğŸ”„ æ·»åŠ SLå‰ç¼€: {job_id}")
        
        print(f"ğŸ” é€šè¿‡æ•°æ®åº“æŸ¥è¯¢è½¬æ¢ job_id: {job_id}")
        
        # ğŸ†• å°è¯•ä½¿ç”¨ä¼˜åŒ–å™¨è·å–task_idå’Œanalysis_response
        try:
            from src.azure_resource_reader_optimizer import convert_job_id_to_task_info
            task_info = convert_job_id_to_task_info(job_id)
            
            if task_info is None:
                print(f"âŒ æ— æ³•æ‰¾åˆ°å¯¹åº”çš„ä»»åŠ¡IDï¼Œè¯·æ£€æŸ¥ job_id: {job_id}")
                return
            
            task_id, analysis_response = task_info
            print(f"âœ… æŸ¥è¯¢æˆåŠŸï¼Œè·å¾—ä»»åŠ¡ID: {task_id}")
            
            if analysis_response:
                print(f"âœ… è·å¾— analysis_response æ•°æ®")
                
                # æ£€æŸ¥æ˜¯å¦å¯ç”¨äº†analysis_responseè§£æ
                try:
                    from config.analysis_response_config import is_analysis_response_enabled
                    
                    if is_analysis_response_enabled(args.task_type_or_job_id):
                        print(f"âœ… ä»»åŠ¡ç±»å‹ {args.task_type_or_job_id} å·²å¯ç”¨ analysis_response è§£æ")
                    else:
                        print(f"âš ï¸  ä»»åŠ¡ç±»å‹ {args.task_type_or_job_id} æœªå¯ç”¨ analysis_response è§£æï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•")
                        analysis_response = None
                except ImportError:
                    print(f"âš ï¸  æ— æ³•å¯¼å…¥ analysis_response é…ç½®ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•")
                    analysis_response = None
            else:
                print(f"â„¹ï¸  æœªæ‰¾åˆ° analysis_response æ•°æ®")
                
        except ImportError:
            # å›é€€åˆ°åŸå§‹æ–¹æ³•
            print(f"âš ï¸  ä¼˜åŒ–å™¨æ¨¡å—ä¸å¯ç”¨ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•")
            task_id = convert_job_id_to_task_id(job_id)
            
            if task_id is None:
                print(f"âŒ æ— æ³•æ‰¾åˆ°å¯¹åº”çš„ä»»åŠ¡IDï¼Œè¯·æ£€æŸ¥ job_id: {job_id}")
                return
            
            print(f"âœ… æŸ¥è¯¢æˆåŠŸï¼Œè·å¾—ä»»åŠ¡ID: {task_id}")
    
    task_type = args.task_type_or_job_id
    print(f"ğŸ“ åŸå§‹æ•°æ®è·¯å¾„: yiya0110/download/compress/{task_type}/{task_id}/")
    print(f"ğŸ“ è§£ææ•°æ®è·¯å¾„: collector0109/parse/{task_type}/{task_id}/")
    print("=" * 80)
    
    # ç¬¬äºŒæ­¥ï¼šç¡®å®šè¦å¤„ç†çš„æ–‡ä»¶åˆ—è¡¨
    if args.files is None:
        # æ ¹æ®ä»»åŠ¡ç±»å‹è‡ªåŠ¨é€‰æ‹©é»˜è®¤æ–‡ä»¶
        files_to_process = get_default_files_for_task_type(task_type)
        print(f"ğŸ“„ æ ¹æ®ä»»åŠ¡ç±»å‹è‡ªåŠ¨é€‰æ‹©åŸå§‹æ–‡ä»¶: {', '.join(files_to_process)}")
    else:
        files_to_process = args.files
        print(f"ğŸ“„ ç”¨æˆ·æŒ‡å®šåŸå§‹æ–‡ä»¶: {', '.join(files_to_process)}")
    
    # åˆ›å»ºä¸»è¯»å–å™¨ï¼ˆyiya0110ï¼‰
    reader = AzureResourceReader('yiya0110')
    
    # ç¡®å®šæ˜¯å¦éœ€è¦è§£å‹ç¼©
    decompress = args.output_type in ['html', 'txt', 'json']
    
    # ç”¨äºè®°å½•æ˜¯å¦æœ‰æ–‡ä»¶æˆåŠŸä¸‹è½½
    successfully_downloaded_files = []
    parse_file_downloaded = False
    parse_result = None
    
    # ğŸ†• é¦–å…ˆå°è¯•ä½¿ç”¨ä¼˜åŒ–æ–¹æ³•è·å–è§£ææ–‡ä»¶
    if not args.info_only:
        print(f"\nğŸš€ æ­¥éª¤1: ä½¿ç”¨ä¼˜åŒ–æ–¹æ³•è·å–è§£ææ–‡ä»¶")
        print("-" * 60)
        
        try:
            from src.azure_resource_reader_optimizer import fetch_and_save_parse_files_optimized
            
            # åˆ›å»ºcollector0109è¯»å–å™¨ç”¨äºè§£ææ–‡ä»¶
            parse_reader = AzureResourceReader('collector0109')
            
            # ä½¿ç”¨ä¼˜åŒ–æ–¹æ³•è·å–è§£ææ–‡ä»¶
            parse_result = fetch_and_save_parse_files_optimized(
                reader=parse_reader,
                task_type=task_type,
                task_id=task_id,
                save_dir=args.save_dir,
                decompress=decompress,
                job_id=job_id,
                analysis_response=analysis_response
            )
            
            if parse_result['success']:
                print(f"âœ… è§£ææ–‡ä»¶è·å–æˆåŠŸ!")
                if 'method_used' in parse_result:
                    method_name = "analysis_responseé“¾æ¥" if parse_result['method_used'] == 'analysis_response' else "Azureå­˜å‚¨"
                    print(f"ğŸ“¡ è·å–æ–¹å¼: {method_name}")
                print(f"ğŸ“¥ ä¸‹è½½æ–‡ä»¶æ•°: {parse_result['total_files_downloaded']}")
                
                # æ˜¾ç¤ºæ–‡ä»¶è¯¦æƒ…
                if parse_result['files_downloaded']:
                    for file_info in parse_result['files_downloaded']:
                        print(f"  âœ… {file_info['saved_name']}")
                        if 'size' in file_info:
                            print(f"     ğŸ“Š å¤§å°: {file_info['size']} å­—èŠ‚")
                        print(f"     ğŸ“ è·¯å¾„: {file_info['local_path']}")
                
                parse_file_downloaded = True
            else:
                print(f"âŒ è§£ææ–‡ä»¶è·å–å¤±è´¥: {parse_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                
        except ImportError:
            print(f"âš ï¸  ä¼˜åŒ–å™¨æ¨¡å—ä¸å¯ç”¨ï¼Œå°†åœ¨å¤„ç†åŸå§‹æ–‡ä»¶æ—¶ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•")
    
    # å¤„ç†æ¯ä¸ªåŸå§‹æ–‡ä»¶
    print(f"\nğŸ“„ æ­¥éª¤2: è·å–åŸå§‹æ–‡ä»¶")
    print("-" * 60)
    
    for filename in files_to_process:
        print(f"\nğŸ“„ å¤„ç†åŸå§‹æ–‡ä»¶: {filename}")
        print("-" * 40)
        
        if args.info_only:
            # æ˜¾ç¤ºåŸå§‹æ–‡ä»¶ä¿¡æ¯
            blob_path = f"compress/{task_type}/{task_id}/{filename}"
            blob_info = reader.get_blob_info('download', blob_path)
            
            if blob_info:
                print(f"âœ… åŸå§‹æ–‡ä»¶ä¿¡æ¯:")
                print(f"  ğŸ“Š å¤§å°: {blob_info['size_mb']} MB")
                print(f"  ğŸ“… ä¿®æ”¹æ—¶é—´: {blob_info['last_modified']}")
                print(f"  ğŸ”— URL: {blob_info['url']}")
            else:
                print("âŒ åŸå§‹æ–‡ä»¶ä¸å­˜åœ¨æˆ–è·å–ä¿¡æ¯å¤±è´¥")
            continue
        
        # è¯»å–åŸå§‹æ–‡ä»¶
        content = reader.read_task_file(task_type, task_id, filename, decompress)
        
        if content is not None:
            print("âœ… åŸå§‹æ–‡ä»¶è¯»å–æˆåŠŸ!")
            if isinstance(content, str):
                print(f"ğŸ“ åŸå§‹æ–‡ä»¶é•¿åº¦: {len(content)} å­—ç¬¦")
            else:
                print(f"ğŸ“Š åŸå§‹æ–‡ä»¶å¤§å°: {len(content)} å­—èŠ‚")
            
            # ä¿å­˜åŸå§‹æ–‡ä»¶
            save_filename = _generate_save_filename(filename, task_id, args.output_type)
            local_path = f"{args.save_dir}/{task_type}/{task_id}/{save_filename}"
            
            success = _save_content_to_file(content, local_path)
            if success:
                print(f"ğŸ’¾ åŸå§‹æ–‡ä»¶å·²ä¿å­˜åˆ°: {local_path}")
                successfully_downloaded_files.append(filename)
        else:
            print("âŒ åŸå§‹æ–‡ä»¶è¯»å–å¤±è´¥æˆ–æ–‡ä»¶ä¸å­˜åœ¨")
    
    # å¦‚æœä¼˜åŒ–æ–¹æ³•æ²¡æœ‰æˆåŠŸè·å–è§£ææ–‡ä»¶ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•
    if not parse_file_downloaded and not args.info_only:
        print(f"\nğŸ”„ æ­¥éª¤3: ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•è·å–è§£ææ–‡ä»¶")
        print("-" * 60)
        
        # ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•è·å–è§£ææ–‡ä»¶ï¼ˆä»…ç¬¬ä¸€ä¸ªæ–‡ä»¶æ—¶æ‰§è¡Œï¼‰
        if files_to_process:
            filename = files_to_process[0]  # è§£ææ–‡ä»¶ä¸å…·ä½“åŸå§‹æ–‡ä»¶æ— å…³
            
            # ä½¿ç”¨æ–°æ–¹æ³•åŒæ—¶è¯»å–åŸå§‹æ–‡ä»¶å’Œè§£ææ–‡ä»¶ä¸­çš„è§£æéƒ¨åˆ†
            combined_content = reader.read_task_file_with_parse(task_type, task_id, filename, decompress)
            parse_content = combined_content.get('parse')
            
            if parse_content is not None:
                print("âœ… è§£ææ–‡ä»¶è¯»å–æˆåŠŸ!")
                if isinstance(parse_content, str):
                    print(f"ğŸ“ è§£ææ–‡ä»¶é•¿åº¦: {len(parse_content)} å­—ç¬¦")
                    
                    # JSONæ ¼å¼éªŒè¯å’Œæ£€æµ‹
                    if parse_content.strip().startswith('{') or parse_content.strip().startswith('['):
                        try:
                            parsed_data = json.loads(parse_content)
                            print(f"ğŸ“‹ JSONè§£ææˆåŠŸï¼Œç±»å‹: {type(parsed_data)}")
                        except json.JSONDecodeError:
                            print("âš ï¸  è§£ææ–‡ä»¶ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼")
                    
                    # æ˜¾ç¤ºé¢„è§ˆ
                    print(f"ğŸ” è§£ææ–‡ä»¶é¢„è§ˆ (å‰200å­—ç¬¦):")
                    print(parse_content[:200] + "..." if len(parse_content) > 200 else parse_content)
                else:
                    print(f"ğŸ“Š è§£ææ–‡ä»¶å¤§å°: {len(parse_content)} å­—èŠ‚")
                
                # ä¿å­˜è§£ææ–‡ä»¶
                parse_filename = _generate_save_filename("parse_result", task_id, "json")
                parse_local_path = f"{args.save_dir}/{task_type}/{task_id}/{parse_filename}"
                
                parse_success = _save_content_to_file(parse_content, parse_local_path)
                if parse_success:
                    print(f"ğŸ’¾ è§£ææ–‡ä»¶å·²ä¿å­˜åˆ°: {parse_local_path}")
                    parse_file_downloaded = True
            else:
                print("âŒ è§£ææ–‡ä»¶è¯»å–å¤±è´¥æˆ–æ–‡ä»¶ä¸å­˜åœ¨")
    
    # ç¬¬ä¸‰æ­¥ï¼šæ›´æ–°ä»»åŠ¡æ˜ å°„æ–‡ä»¶
    if (successfully_downloaded_files or parse_file_downloaded) and not args.info_only and not args.no_mapping:
        print("\n" + "=" * 80)
        print("æ­¥éª¤4ï¼šæ›´æ–°ä»»åŠ¡æ˜ å°„æ–‡ä»¶")
        print("=" * 80)
        
        # æ˜¾ç¤ºæ˜ å°„ä¿¡æ¯
        print_task_mapping_info(original_input, task_type, task_id, args.save_dir)
        
        # æ›´æ–°åŸå§‹æ–‡ä»¶æ˜ å°„
        mapping_success = update_task_mapping(original_input, task_type, task_id, args.save_dir)
        
        # åœ¨ --with-parse æ¨¡å¼ä¸‹ï¼Œè§£ææ–‡ä»¶å’ŒåŸå§‹æ–‡ä»¶åœ¨åŒä¸€ç›®å½•ï¼Œä¸éœ€è¦å•ç‹¬çš„æ˜ å°„
        if mapping_success:
            if parse_file_downloaded:
                print(f"âœ… æˆåŠŸä¸‹è½½åŸå§‹æ–‡ä»¶å’Œè§£ææ–‡ä»¶å¹¶æ›´æ–°æ˜ å°„")
                print(f"ğŸ“„ åŸå§‹æ–‡ä»¶æ•°é‡: {len(successfully_downloaded_files)}")
                print(f"ğŸ“„ è§£ææ–‡ä»¶: 1ä¸ª (ä¿å­˜åœ¨åŒä¸€ç›®å½•)")
                
                # æ˜¾ç¤ºä½¿ç”¨çš„æ–¹æ³•
                if parse_result and 'method_used' in parse_result:
                    method_name = "analysis_responseä¼˜åŒ–é“¾æ¥" if parse_result['method_used'] == 'analysis_response' else "Azureå­˜å‚¨"
                    print(f"ğŸš€ è§£ææ–‡ä»¶è·å–æ–¹å¼: {method_name}")
            else:
                print(f"âœ… æˆåŠŸä¸‹è½½åŸå§‹æ–‡ä»¶å¹¶æ›´æ–°æ˜ å°„")
                print(f"ğŸ“„ å·²ä¸‹è½½æ–‡ä»¶: {', '.join(successfully_downloaded_files)}")
                if not parse_file_downloaded:
                    print("âš ï¸  è§£ææ–‡ä»¶ä¸‹è½½å¤±è´¥")
    
    elif args.info_only:
        print(f"\nğŸ“‹ ä¿¡æ¯æŸ¥çœ‹æ¨¡å¼ï¼Œæœªä¸‹è½½æ–‡ä»¶")
        # æ˜¾ç¤ºè§£ææ–‡ä»¶ä¿¡æ¯ï¼ˆå¦‚æœæœªåœ¨ä¼˜åŒ–æ­¥éª¤ä¸­è·å–ï¼‰
        if not parse_file_downloaded:
            parse_reader = AzureResourceReader('collector0109')
            parse_files = parse_reader.list_parse_files(task_type, task_id)
            if parse_files:
                print(f"\nâœ… è§£ææ–‡ä»¶ä¿¡æ¯ (å…±{len(parse_files)}ä¸ª):")
                for parse_file in parse_files:
                    print(f"  ğŸ“„ {parse_file['name']}")
                    print(f"     ğŸ“Š å¤§å°: {parse_file['size']} å­—èŠ‚")
            else:
                print("\nâŒ æœªæ‰¾åˆ°è§£ææ–‡ä»¶")
                
    elif args.no_mapping:
        print(f"\nğŸ“‹ å·²ç¦ç”¨æ˜ å°„æ–‡ä»¶ç”Ÿæˆ")
    elif not successfully_downloaded_files and not parse_file_downloaded:
        print(f"\nâš ï¸  æ²¡æœ‰æ–‡ä»¶æˆåŠŸä¸‹è½½ï¼Œæœªæ›´æ–°æ˜ å°„")


def main():
    """ä¸»å‡½æ•°ï¼šå¤„ç†å‘½ä»¤è¡Œå‚æ•°å¹¶æ‰§è¡Œä¼˜åŒ–ç‰ˆ --with-parse æ¨¡å¼"""
    parser = argparse.ArgumentParser(
        description='Azure Storage èµ„æºè¯»å–å™¨ --with-parse æ¨¡å¼ä¼˜åŒ–ç‰ˆæœ¬',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # ä½¿ç”¨ä»»åŠ¡IDç›´æ¥è·å–åŸå§‹æ–‡ä»¶å’Œè§£ææ–‡ä»¶
  python3 src/azure_resource_reader_with_optimization.py AmazonReviewStarJob 1887037115222994944 html
  
  # ä½¿ç”¨job_idè·å–ï¼ˆä¼˜å…ˆä»analysis_responseé“¾æ¥ä¸‹è½½è§£ææ–‡ä»¶ï¼‰
  python3 src/azure_resource_reader_with_optimization.py AmazonReviewStarJob 2796867471 html
  
  # æŒ‡å®šä¿å­˜ç›®å½•å’Œç‰¹å®šæ–‡ä»¶
  python3 src/azure_resource_reader_with_optimization.py AmazonReviewStarJob 2796867471 html --save-dir data/test_output --files page_1.gz

ä¼˜åŒ–åŠŸèƒ½è¯´æ˜:
  1. è‡ªåŠ¨æŸ¥è¯¢æ•°æ®åº“è·å– analysis_response å­—æ®µ
  2. å¦‚æœä»»åŠ¡ç±»å‹æ”¯æŒä¸”æœ‰æœ‰æ•ˆé“¾æ¥ï¼Œä¼˜å…ˆä» analysis_response ä¸­çš„é“¾æ¥ä¸‹è½½è§£ææ–‡ä»¶
  3. å¦‚æœé“¾æ¥å¤±æ•ˆæˆ–ä»»åŠ¡ç±»å‹ä¸æ”¯æŒï¼Œè‡ªåŠ¨å›é€€åˆ° Azure å­˜å‚¨æ–¹æ³•
  4. æ”¯æŒè‡ªåŠ¨JSONæ ¼å¼æ£€æµ‹å’Œæ–‡ä»¶æ‰©å±•åä¿®æ­£
  5. è§£ææ–‡ä»¶å’ŒåŸå§‹æ–‡ä»¶ä¿å­˜åœ¨åŒä¸€ç›®å½•ä¸‹
        """
    )
    
    parser.add_argument('task_type_or_job_id', 
                       help='ä»»åŠ¡ç±»å‹ï¼ˆå¦‚: AmazonReviewStarJobï¼‰')
    parser.add_argument('task_id_or_task_id', 
                       help='ä»»åŠ¡IDï¼ˆé•¿æ•°å­—ä¸²ï¼‰æˆ–job_idï¼ˆå¦‚: 2796867471ï¼‰')
    parser.add_argument('output_type',
                       choices=['html', 'txt', 'json', 'binary'],
                       help='è¾“å‡ºæ ¼å¼ï¼šhtml, txt, json, binary')
    parser.add_argument('--save-dir', '-s',
                       default='data/output',
                       help='ä¿å­˜ç›®å½•ï¼Œé»˜è®¤: data/output')
    parser.add_argument('--files', '-f',
                       nargs='+',
                       help='æŒ‡å®šè¦è¯»å–çš„æ–‡ä»¶åï¼ˆå¤šä¸ªæ–‡ä»¶ç”¨ç©ºæ ¼åˆ†éš”ï¼‰')
    parser.add_argument('--info-only',
                       action='store_true',
                       help='ä»…æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯ï¼Œä¸ä¸‹è½½')
    parser.add_argument('--no-mapping',
                       action='store_true',
                       help='ä¸ç”Ÿæˆæ˜ å°„æ–‡ä»¶')
    
    args = parser.parse_args()
    
    # æ‰§è¡Œä¼˜åŒ–ç‰ˆ --with-parse æ¨¡å¼
    handle_with_parse_mode_optimized(args)


if __name__ == '__main__':
    main() 