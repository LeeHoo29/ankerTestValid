#!/usr/bin/env python3
"""
Shulex-Ankeræ•°æ®éªŒè¯å·¥å…·ä¸»ç¨‹åºå…¥å£
"""
import os
import sys
import logging
import argparse
import json
import requests
from pathlib import Path
from typing import Dict, List, Optional, Any

import pandas as pd
from tabulate import tabulate

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°ç³»ç»Ÿè·¯å¾„
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from src.db.connector import DatabaseConnector
from src.file_processors.csv_processor import CSVProcessor
from src.file_processors.excel_processor import ExcelProcessor
from config.db_config import REPARSER_API_CONFIG, CRAWLER_API_CONFIG

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def test_db_connection() -> bool:
    """
    æµ‹è¯•æ•°æ®åº“è¿æ¥
    
    Returns:
        bool: è¿æ¥æˆåŠŸè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    logger.info("æµ‹è¯•æ•°æ®åº“è¿æ¥...")
    db = DatabaseConnector()
    result = db.test_connection()
    
    if result:
        logger.info("æ•°æ®åº“è¿æ¥æµ‹è¯•æˆåŠŸï¼")
    else:
        logger.error("æ•°æ®åº“è¿æ¥æµ‹è¯•å¤±è´¥ï¼")
        
    return result


def read_excel_chunked(file_path: str, nrows: int = 10, skiprows: int = 0, sheet_name: Optional[str] = None, output_path: Optional[str] = None) -> Optional[pd.DataFrame]:
    """
    åˆ†å—è¯»å–å¤§Excelæ–‡ä»¶çš„å‰Nè¡Œæ•°æ®
    
    Args:
        file_path: Excelæ–‡ä»¶è·¯å¾„
        nrows: è¯»å–çš„è¡Œæ•°ï¼Œé»˜è®¤ä¸º10
        skiprows: è·³è¿‡çš„è¡Œæ•°ï¼Œé»˜è®¤ä¸º0
        sheet_name: å·¥ä½œè¡¨åç§°ï¼Œé»˜è®¤ä¸ºç¬¬ä¸€ä¸ªå·¥ä½œè¡¨
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        
    Returns:
        pd.DataFrame: è¯»å–çš„æ•°æ®ï¼Œå¤±è´¥è¿”å›None
    """
    if not os.path.exists(file_path):
        logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return None
        
    processor = ExcelProcessor(file_path)
    
    try:
        # é¦–å…ˆé¢„è§ˆæ–‡ä»¶ä¿¡æ¯
        print("=== Excelæ–‡ä»¶é¢„è§ˆä¿¡æ¯ ===")
        file_info = processor.preview_file_info(sheet_name=sheet_name or 0)
        if file_info:
            print(f"æ–‡ä»¶è·¯å¾„: {file_info['file_path']}")
            print(f"æ–‡ä»¶å¤§å°: {file_info['file_size']}")
            print(f"å·¥ä½œè¡¨åˆ—è¡¨: {', '.join(file_info['sheet_names'])}")
            print(f"åˆ—æ•°: {file_info['column_count']}")
            print(f"åˆ—å: {', '.join(file_info['columns'])}")
        
        # ä½¿ç”¨åˆ†å—è¯»å–æ–¹æ³•
        print(f"\n=== å¼€å§‹è¯»å–å‰ {nrows} è¡Œæ•°æ® ===")
        success = processor.read_file_chunked(
            sheet_name=sheet_name or 0,
            nrows=nrows,
            skiprows=skiprows
        )
        
        if not success:
            return None
            
        # è·å–è¯»å–çš„æ•°æ®
        data = processor.get_data()
        if data is not None and len(data) > 0:
            print(f"\n=== æ•°æ®æ‘˜è¦ ===")
            print(f"å®é™…è¯»å–è¡Œæ•°: {len(data)}")
            print(f"åˆ—æ•°: {len(data.columns)}")
            
            print(f"\n=== å‰ {min(len(data), 10)} è¡Œæ•°æ®é¢„è§ˆ ===")
            print(tabulate(data.head(10), headers='keys', tablefmt='psql', showindex=True))
            
            # æ˜¾ç¤ºæ•°æ®ç±»å‹ä¿¡æ¯
            print(f"\n=== åˆ—æ•°æ®ç±»å‹ä¿¡æ¯ ===")
            dtype_info = pd.DataFrame({
                'åˆ—å': data.columns,
                'æ•°æ®ç±»å‹': data.dtypes.astype(str),
                'éç©ºå€¼æ•°é‡': data.count(),
                'ç©ºå€¼æ•°é‡': data.isnull().sum()
            })
            print(tabulate(dtype_info, headers='keys', tablefmt='psql', showindex=False))
            
            # ä¿å­˜æ•°æ®
            if output_path:
                if output_path.endswith('.csv'):
                    processor.save_sheet_to_csv(output_path, data=data)
                else:
                    processor.save_to_excel(output_path, data={processor.current_sheet: data})
                    
            return data
        else:
            print("æœªè¯»å–åˆ°æ•°æ®")
            return None
            
    except Exception as e:
        logger.error(f"å¤„ç†Excelæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        return None


def process_file(file_path: str, file_type: str = None, output_path: Optional[str] = None) -> Optional[pd.DataFrame]:
    """
    å¤„ç†æ–‡ä»¶ï¼ˆCSVæˆ–Excelï¼‰
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        file_type: æ–‡ä»¶ç±»å‹ï¼ˆ'csv'æˆ–'excel'ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™æ ¹æ®æ–‡ä»¶æ‰©å±•ååˆ¤æ–­
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæä¾›åˆ™å°†å¤„ç†ç»“æœä¿å­˜åˆ°è¯¥æ–‡ä»¶
        
    Returns:
        pd.DataFrame: å¤„ç†åçš„æ•°æ®ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›None
    """
    if not os.path.exists(file_path):
        logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return None
        
    # å¦‚æœæœªæŒ‡å®šæ–‡ä»¶ç±»å‹ï¼Œæ ¹æ®æ‰©å±•ååˆ¤æ–­
    if file_type is None:
        ext = os.path.splitext(file_path)[1].lower()
        if ext in ['.csv']:
            file_type = 'csv'
        elif ext in ['.xlsx', '.xls', '.xlsm']:
            file_type = 'excel'
        else:
            logger.error(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {ext}")
            return None
    
    # å¤„ç†CSVæ–‡ä»¶
    if file_type.lower() == 'csv':
        processor = CSVProcessor(file_path)
        if not processor.read_file():
            return None
            
        data = processor.get_data()
        logger.info(f"CSVæ–‡ä»¶å¤„ç†å®Œæˆï¼Œå…± {len(data)} è¡Œæ•°æ®")
        
        # å¦‚æœæŒ‡å®šäº†è¾“å‡ºè·¯å¾„ï¼Œä¿å­˜å¤„ç†ç»“æœ
        if output_path:
            processor.save_to_csv(output_path)
            
        return data
        
    # å¤„ç†Excelæ–‡ä»¶
    elif file_type.lower() == 'excel':
        processor = ExcelProcessor(file_path)
        if not processor.read_file():
            return None
            
        sheet_names = processor.get_sheet_names()
        logger.info(f"Excelæ–‡ä»¶å¤„ç†å®Œæˆï¼ŒåŒ…å«å·¥ä½œè¡¨: {', '.join(sheet_names)}")
        
        data = processor.get_data()  # è·å–ç¬¬ä¸€ä¸ªå·¥ä½œè¡¨æ•°æ®
        
        # å¦‚æœæŒ‡å®šäº†è¾“å‡ºè·¯å¾„ï¼Œä¿å­˜å¤„ç†ç»“æœ
        if output_path:
            if output_path.endswith('.csv'):
                processor.save_sheet_to_csv(output_path)
            else:
                processor.save_to_excel(output_path)
                
        return data
        
    else:
        logger.error(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_type}")
        return None


def validate_data(file_path: str, output_path: Optional[str] = None) -> bool:
    """
    éªŒè¯æ•°æ®æ–‡ä»¶ä¸æ•°æ®åº“ä¸­çš„æ•°æ®
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        
    Returns:
        bool: éªŒè¯æˆåŠŸè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    # å¤„ç†æ–‡ä»¶
    file_data = process_file(file_path)
    if file_data is None:
        return False
        
    # æ˜¾ç¤ºæ–‡ä»¶æ‘˜è¦ä¿¡æ¯
    print("\næ–‡ä»¶æ‘˜è¦ä¿¡æ¯:")
    print(f"- è¡Œæ•°: {len(file_data)}")
    print(f"- åˆ—æ•°: {len(file_data.columns)}")
    print(f"- åˆ—å: {', '.join(file_data.columns)}")
    
    # è¿æ¥æ•°æ®åº“
    db = DatabaseConnector()
    if not db.connect():
        return False
        
    try:
        # è·å–æ•°æ®åº“ä¿¡æ¯
        db_info = db.execute_query("SELECT database() as db_name")
        if db_info:
            print(f"\nå½“å‰æ•°æ®åº“: {db_info[0]['db_name']}")
        
        # è·å–æ•°æ®åº“è¡¨ä¿¡æ¯
        tables = db.execute_query("SHOW TABLES")
        if tables:
            print("\næ•°æ®åº“è¡¨åˆ—è¡¨:")
            for table in tables:
                table_name = list(table.values())[0]
                print(f"- {table_name}")
        
        # æç¤ºç”¨æˆ·è¾“å…¥SQLæŸ¥è¯¢
        print("\nè¯·è¾“å…¥è¦æ‰§è¡Œçš„SQLæŸ¥è¯¢æ¥éªŒè¯æ•°æ® (ç•™ç©ºè·³è¿‡):")
        sql_query = input("> ")
        
        if sql_query:
            result = db.execute_query(sql_query)
            if result:
                # å°†ç»“æœè½¬æ¢ä¸ºDataFrameå¹¶æ˜¾ç¤º
                result_df = pd.DataFrame(result)
                print("\næŸ¥è¯¢ç»“æœ (å‰10è¡Œ):")
                print(tabulate(result_df.head(10), headers='keys', tablefmt='psql'))
                
                # ä¿å­˜ç»“æœ
                if output_path:
                    result_df.to_csv(output_path, index=False)
                    print(f"\næŸ¥è¯¢ç»“æœå·²ä¿å­˜è‡³: {output_path}")
                    
                return True
            else:
                print("æŸ¥è¯¢æœªè¿”å›ç»“æœ")
                return False
                
        return True
        
    finally:
        # å…³é—­æ•°æ®åº“è¿æ¥
        db.disconnect()


def filter_null_column(file_path: str, column_name: str, nrows: int = 10, sheet_name: Optional[str] = None, output_path: Optional[str] = None, prepare_db_query: bool = False) -> Optional[pd.DataFrame]:
    """
    ç­›é€‰æŒ‡å®šåˆ—ä¸ºç©ºçš„æ•°æ®
    
    Args:
        file_path: Excelæ–‡ä»¶è·¯å¾„
        column_name: è¦ç­›é€‰çš„åˆ—å
        nrows: è¿”å›çš„è¡Œæ•°ï¼Œé»˜è®¤ä¸º10
        sheet_name: å·¥ä½œè¡¨åç§°ï¼Œé»˜è®¤ä¸ºç¬¬ä¸€ä¸ªå·¥ä½œè¡¨
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        prepare_db_query: æ˜¯å¦å‡†å¤‡æ•°æ®åº“æŸ¥è¯¢ä¿¡æ¯
        
    Returns:
        pd.DataFrame: ç­›é€‰åçš„æ•°æ®ï¼Œå¤±è´¥è¿”å›None
    """
    if not os.path.exists(file_path):
        logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return None
        
    processor = ExcelProcessor(file_path)
    
    try:
        # é¦–å…ˆè¯»å–è¶³å¤Ÿçš„æ•°æ®ä»¥è¿›è¡Œç­›é€‰ï¼ˆè¯»å–æ›´å¤šè¡Œä»¥ç¡®ä¿èƒ½æ‰¾åˆ°è¶³å¤Ÿçš„ç©ºå€¼è®°å½•ï¼‰
        print("=== æ­£åœ¨è¯»å–Excelæ–‡ä»¶å¹¶åˆ†æåˆ—ä¿¡æ¯ ===")
        
        # å…ˆè¯»å–ä¸€éƒ¨åˆ†æ•°æ®è¿›è¡Œåˆ†æ
        success = processor.read_file_chunked(
            sheet_name=sheet_name or 0,
            nrows=1000  # å…ˆè¯»å–1000è¡Œè¿›è¡Œåˆ†æ
        )
        
        if not success:
            return None
            
        # åˆ†ææŒ‡å®šåˆ—çš„æƒ…å†µ
        print(f"\n=== åˆ†æåˆ— '{column_name}' çš„æ•°æ®åˆ†å¸ƒ ===")
        analysis = processor.analyze_column_values(column_name)
        if analysis:
            print(f"æ€»è¡Œæ•°: {analysis['total_count']}")
            print(f"ç©ºå€¼æ•°é‡: {analysis['null_count']}")
            print(f"ç©ºå­—ç¬¦ä¸²æ•°é‡: {analysis['empty_string_count']}")
            print(f"éç©ºå€¼æ•°é‡: {analysis['non_null_count']}")
            print(f"å”¯ä¸€å€¼æ•°é‡: {analysis['unique_values']}")
        
        # å¦‚æœ1000è¡Œä¸­ç©ºå€¼æ•°é‡ä¸å¤Ÿï¼Œéœ€è¦è¯»å–æ›´å¤šæ•°æ®
        if analysis and analysis['null_count'] < nrows:
            print(f"\nå½“å‰è¯»å–çš„æ•°æ®ä¸­ç©ºå€¼æ•°é‡({analysis['null_count']})ä¸è¶³{nrows}æ¡ï¼Œå°è¯•è¯»å–æ›´å¤šæ•°æ®...")
            success = processor.read_file_chunked(
                sheet_name=sheet_name or 0,
                nrows=5000  # è¯»å–æ›´å¤šæ•°æ®
            )
            if not success:
                print("è¯»å–æ›´å¤šæ•°æ®å¤±è´¥ï¼Œä½¿ç”¨å½“å‰å·²è¯»å–çš„æ•°æ®")
        
        # ç­›é€‰ç©ºå€¼æ•°æ®
        print(f"\n=== ç­›é€‰ '{column_name}' åˆ—ä¸ºç©ºçš„å‰ {nrows} æ¡æ•°æ® ===")
        filtered_data = processor.filter_null_values(column_name, nrows)
        
        if filtered_data is not None and len(filtered_data) > 0:
            print(f"\n=== ç­›é€‰ç»“æœæ‘˜è¦ ===")
            print(f"æ‰¾åˆ° {len(filtered_data)} æ¡ '{column_name}' ä¸ºç©ºçš„è®°å½•")
            
            print(f"\n=== ç­›é€‰å‡ºçš„æ•°æ®é¢„è§ˆ ===")
            print(tabulate(filtered_data, headers='keys', tablefmt='psql', showindex=True))
            
            # ä¸ºæ•°æ®åº“æŸ¥è¯¢å‡†å¤‡å…³é”®å­—æ®µä¿¡æ¯
            if prepare_db_query:
                print(f"\n=== æ•°æ®åº“æŸ¥è¯¢å‡†å¤‡ä¿¡æ¯ ===")
                
                # æ˜¾ç¤ºå¯èƒ½ç”¨äºæ•°æ®åº“æŸ¥è¯¢çš„å…³é”®åˆ—
                key_columns = []
                for col in filtered_data.columns:
                    col_lower = col.lower()
                    if any(keyword in col_lower for keyword in ['id', 'number', 'code', 'ç¼–å·', 'è®¢å•', 'order', 'listing']):
                        key_columns.append(col)
                
                if key_columns:
                    print(f"å»ºè®®ç”¨äºæ•°æ®åº“æŸ¥è¯¢çš„å…³é”®åˆ—: {', '.join(key_columns)}")
                    
                    for key_col in key_columns:
                        unique_values = filtered_data[key_col].dropna().unique()
                        if len(unique_values) > 0:
                            print(f"\n{key_col} çš„å”¯ä¸€å€¼ (å‰10ä¸ª):")
                            for i, value in enumerate(unique_values[:10]):
                                print(f"  {i+1}. {value}")
                            
                            # ç”ŸæˆSQLæŸ¥è¯¢ç¤ºä¾‹
                            if len(unique_values) <= 5:
                                values_str = "', '".join(str(v) for v in unique_values)
                                print(f"\nç¤ºä¾‹SQLæŸ¥è¯¢ (åŸºäº {key_col}):")
                                print(f"SELECT * FROM your_table WHERE {key_col} IN ('{values_str}');")
                else:
                    print("æœªæ‰¾åˆ°æ˜æ˜¾çš„å…³é”®åˆ—ï¼Œæ˜¾ç¤ºæ‰€æœ‰åˆ—ä¿¡æ¯ä¾›å‚è€ƒ:")
                    for col in filtered_data.columns:
                        non_null_count = filtered_data[col].count()
                        if non_null_count > 0:
                            sample_values = filtered_data[col].dropna().head(3).tolist()
                            print(f"  {col}: {non_null_count}ä¸ªéç©ºå€¼, ç¤ºä¾‹: {sample_values}")
            
            # ä¿å­˜ç­›é€‰ç»“æœ
            if output_path:
                if output_path.endswith('.csv'):
                    processor.save_sheet_to_csv(output_path, data=filtered_data)
                else:
                    processor.save_to_excel(output_path, data={f"filtered_{column_name}": filtered_data})
                    
            return filtered_data
        else:
            print(f"æœªæ‰¾åˆ° '{column_name}' åˆ—ä¸ºç©ºçš„æ•°æ®")
            return None
            
    except Exception as e:
        logger.error(f"ç­›é€‰æ•°æ®æ—¶å‡ºé”™: {str(e)}")
        return None


def analyze_tasks_with_db(file_path: str, column_name: str, nrows: int = 10, sheet_name: Optional[str] = None, output_path: Optional[str] = None) -> Optional[pd.DataFrame]:
    """
    åˆ†æä»»åŠ¡æ•°æ®å¹¶æŸ¥è¯¢æ•°æ®åº“è¿›è¡Œé—®é¢˜è¯Šæ–­
    
    Args:
        file_path: Excelæ–‡ä»¶è·¯å¾„
        column_name: è¦ç­›é€‰çš„åˆ—åï¼ˆå¦‚"è§£å†³è¿›åº¦"ï¼‰
        nrows: åˆ†æçš„ä»»åŠ¡æ•°é‡ï¼Œé»˜è®¤ä¸º10
        sheet_name: å·¥ä½œè¡¨åç§°ï¼Œé»˜è®¤ä¸ºç¬¬ä¸€ä¸ªå·¥ä½œè¡¨
        output_path: è¾“å‡ºåˆ†æç»“æœçš„æ–‡ä»¶è·¯å¾„
        
    Returns:
        pd.DataFrame: åˆ†æç»“æœï¼Œå¤±è´¥è¿”å›None
    """
    import json
    
    if not os.path.exists(file_path):
        logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return None
        
    # ç¬¬ä¸€æ­¥ï¼šç­›é€‰ç©ºå€¼æ•°æ®
    print("=" * 60)
    print("ç¬¬ä¸€æ­¥ï¼šç­›é€‰å¾…å¤„ç†ä»»åŠ¡")
    print("=" * 60)
    
    processor = ExcelProcessor(file_path)
    
    try:
        # è¯»å–æ•°æ®å¹¶ç­›é€‰
        success = processor.read_file_chunked(
            sheet_name=sheet_name or 0,
            nrows=5000  # è¯»å–è¶³å¤Ÿçš„æ•°æ®ç¡®ä¿èƒ½æ‰¾åˆ°ç©ºå€¼è®°å½•
        )
        
        if not success:
            return None
            
        # ç­›é€‰ç©ºå€¼æ•°æ®
        filtered_data = processor.filter_null_values(column_name, nrows)
        
        if filtered_data is None or len(filtered_data) == 0:
            print(f"æœªæ‰¾åˆ° '{column_name}' åˆ—ä¸ºç©ºçš„æ•°æ®")
            return None
            
        print(f"æ‰¾åˆ° {len(filtered_data)} æ¡å¾…å¤„ç†ä»»åŠ¡")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰shulex_ssnåˆ—
        if 'shulex_ssn' not in filtered_data.columns:
            logger.error("æ•°æ®ä¸­ç¼ºå°‘ 'shulex_ssn' åˆ—")
            return None
            
        # åœ¨å¼€å§‹æ•°æ®åº“æŸ¥è¯¢å‰ï¼Œæ ‡è®°æ‰€æœ‰ç­›é€‰å‡ºçš„ä»»åŠ¡ä¸º"è§£å†³ä¸­"
        print(f"\næ ‡è®° {len(filtered_data)} ä¸ªä»»åŠ¡çŠ¶æ€ä¸º'è§£å†³ä¸­'...")
        
        # è·å–å®Œæ•´æ•°æ®è¿›è¡Œæ›´æ–°
        full_data = processor.get_data()
        if full_data is not None and column_name in full_data.columns:
            # æ›´æ–°ç­›é€‰å‡ºçš„ä»»åŠ¡çš„çŠ¶æ€
            for index in filtered_data.index:
                if index < len(full_data):
                    full_data.loc[index, column_name] = "è§£å†³ä¸­"
            
            # ä¿å­˜æ›´æ–°åçš„æ–‡ä»¶
            updated_file_path = file_path.replace('.xlsx', '_å·²æ›´æ–°.xlsx')
            processor.save_to_excel(updated_file_path, data={processor.current_sheet: full_data})
            print(f"å·²æ›´æ–°æ–‡ä»¶ä¿å­˜è‡³: {updated_file_path}")
        
        # ç¬¬äºŒæ­¥ï¼šè¿æ¥æ•°æ®åº“
        print("\n" + "=" * 60)
        print("ç¬¬äºŒæ­¥ï¼šè¿æ¥æ•°æ®åº“è¿›è¡ŒæŸ¥è¯¢åˆ†æ")
        print("=" * 60)
        
        # åˆ›å»ºæ•°æ®åº“è¿æ¥ï¼ŒæŒ‡å®šæ•°æ®åº“
        from config.db_config import DB_CONFIG
        db_config = DB_CONFIG.copy()
        db_config['database'] = 'shulex_collector_prod'
        
        db = DatabaseConnector(db_config)
        if not db.connect():
            logger.error("æ— æ³•è¿æ¥åˆ°æ•°æ®åº“ shulex_collector_prod")
            return None
            
        try:
            # å‡†å¤‡åˆ†æç»“æœ
            analysis_results = []
            
            # å®šä¹‰è¦æŸ¥è¯¢çš„è¡¨
            tables_to_check = ['log_a', 'log_b', 'log_c', 'log_d']
            
            print(f"å¼€å§‹åˆ†æ {len(filtered_data)} ä¸ªä»»åŠ¡...")
            
            for index, row in filtered_data.iterrows():
                shulex_ssn = row['shulex_ssn']
                task_id = row['id']
                
                print(f"\n--- åˆ†æä»»åŠ¡ {task_id} (SSN: {shulex_ssn}) ---")
                
                # åˆå§‹åŒ–ç»“æœè®°å½•
                result_record = {
                    'id': task_id,
                    'shulex_ssn': shulex_ssn,
                    'asin': row.get('asin', ''),
                    'market': row.get('market', ''),
                    'type': row.get('type', ''),
                    'status': row.get('status', ''),
                    'task_id': '',  # æ–°å¢åˆ—å­˜å‚¨ä»logä¸­æå–çš„ext_ssn
                    'é—®é¢˜åˆ†æç»“æœ': '',
                    'é—®é¢˜ç›´æ¥åŸå› ': '',
                    'error_details': '',
                    'error_msg': ''  # æ–°å¢åˆ—å­˜å‚¨å®Œæ•´é”™è¯¯ä¿¡æ¯
                }
                
                # æŸ¥è¯¢å„ä¸ªè¡¨
                table_results = {}
                total_records = 0
                
                for table_name in tables_to_check:
                    query = f"SELECT * FROM {table_name} WHERE req_ssn = %s"
                    try:
                        records = db.execute_query(query, (shulex_ssn,))
                        record_count = len(records) if records else 0
                        table_results[table_name] = {
                            'count': record_count,
                            'records': records
                        }
                        total_records += record_count
                        print(f"  {table_name}: {record_count} æ¡è®°å½•")
                        
                    except Exception as e:
                        logger.error(f"æŸ¥è¯¢è¡¨ {table_name} å¤±è´¥: {str(e)}")
                        table_results[table_name] = {
                            'count': 0,
                            'records': []
                        }
                
                # åˆ†ææŸ¥è¯¢ç»“æœ
                if total_records == 0:
                    result_record['é—®é¢˜åˆ†æç»“æœ'] = "æ‰€æœ‰æ—¥å¿—è¡¨ä¸­éƒ½æ²¡æœ‰æ‰¾åˆ°ç›¸å…³è®°å½•"
                    result_record['é—®é¢˜ç›´æ¥åŸå› '] = "æŸ¥è¯¢çš„ç»“æœä¸ºç©º"
                    print(f"  âŒ é—®é¢˜è¯Šæ–­: æŸ¥è¯¢çš„ç»“æœä¸ºç©º")
                    
                elif total_records == 1:
                    # æ‰¾åˆ°å”¯ä¸€è®°å½•ï¼Œè¿›è¡Œè¯¦ç»†åˆ†æ
                    found_table = None
                    found_record = None
                    for table_name, table_result in table_results.items():
                        if table_result['count'] == 1:
                            found_table = table_name
                            found_record = table_result['records'][0]
                            break
                    
                    print(f"  âœ… æ‰¾åˆ°å”¯ä¸€è®°å½•åœ¨è¡¨: {found_table}")
                    
                    if found_record:
                        # æå–task_id (ext_ssn)
                        ext_ssn = found_record.get('ext_ssn', '')
                        result_record['task_id'] = ext_ssn
                        
                        # è¯¦ç»†åˆ†æè®°å½•
                        print("  ğŸ“‹ åŸºæœ¬ä¿¡æ¯:")
                        print(f"    ID: {found_record.get('id')}")
                        print(f"    Task ID (ext_ssn): {ext_ssn}")
                        print(f"    State: {found_record.get('state')}")
                        print(f"    Created: {found_record.get('created_at')}")
                        print(f"    Completed: {found_record.get('completed_at')}")
                        
                        # æ£€æŸ¥stateæ˜¯å¦ä¸ºFAILURE
                        if found_record.get('state') == 'FAILURE':
                            print("  âŒ ä»»åŠ¡çŠ¶æ€: FAILURE")
                            
                            # æ£€æŸ¥analysis_responseæ˜¯å¦ä¸ä¸ºç©º
                            analysis_response = found_record.get('analysis_response')
                            if analysis_response:
                                print("  ğŸ“„ åˆ†æå“åº”å­˜åœ¨ï¼Œå¼€å§‹è§£æ...")
                                
                                try:
                                    # è§£æJSONå¹¶æ ¼å¼åŒ–æ‰“å°
                                    if isinstance(analysis_response, str):
                                        response_data = json.loads(analysis_response)
                                    else:
                                        response_data = analysis_response
                                    
                                    print("  ğŸ“Š Analysis Response (æ ¼å¼åŒ–):")
                                    print(json.dumps(response_data, indent=2, ensure_ascii=False))
                                    
                                    # åˆ¤æ–­code
                                    code = response_data.get('code')
                                    meta = response_data.get('meta', {})
                                    error_msg = meta.get('error_msg', '')
                                    
                                    print(f"  ğŸ” Response Code: {code}")
                                    
                                    if code != 500:
                                        print(f"  âš ï¸  Codeä¸ç­‰äº500ï¼ŒError Message:")
                                        print(f"     {error_msg}")
                                        result_record['é—®é¢˜åˆ†æç»“æœ'] = f"åœ¨ {found_table} ä¸­æ‰¾åˆ°è®°å½•ï¼ŒçŠ¶æ€FAILUREï¼ŒCode: {code}"
                                        result_record['é—®é¢˜ç›´æ¥åŸå› '] = f"Codeä¸ç­‰äº500: {error_msg[:100]}..."
                                        result_record['error_details'] = json.dumps(response_data, ensure_ascii=False)
                                        result_record['error_msg'] = error_msg
                                    else:
                                        print(f"  ğŸ”´ Codeç­‰äº500ï¼Œç³»ç»Ÿé”™è¯¯:")
                                        # æå–å…³é”®é”™è¯¯ä¿¡æ¯
                                        if error_msg:
                                            # æå–æœ€å…³é”®çš„é”™è¯¯è¡Œ
                                            error_lines = error_msg.split('\n')
                                            key_error = ""
                                            for line in error_lines:
                                                if any(keyword in line for keyword in ['Error:', 'Exception:', 'IndexError:', 'KeyError:']):
                                                    key_error = line.strip()
                                                    break
                                            
                                            print(f"     å…³é”®é”™è¯¯: {key_error}")
                                            result_record['é—®é¢˜åˆ†æç»“æœ'] = f"åœ¨ {found_table} ä¸­æ‰¾åˆ°è®°å½•ï¼ŒçŠ¶æ€FAILUREï¼ŒCode: 500"
                                            result_record['é—®é¢˜ç›´æ¥åŸå› '] = f"ç³»ç»Ÿé”™è¯¯: {key_error[:100]}..."
                                            result_record['error_details'] = json.dumps(response_data, ensure_ascii=False)
                                            result_record['error_msg'] = error_msg
                                        else:
                                            result_record['é—®é¢˜åˆ†æç»“æœ'] = f"åœ¨ {found_table} ä¸­æ‰¾åˆ°è®°å½•ï¼ŒçŠ¶æ€FAILUREï¼ŒCode: 500"
                                            result_record['é—®é¢˜ç›´æ¥åŸå› '] = "ç³»ç»Ÿé”™è¯¯ï¼Œä½†æ— è¯¦ç»†é”™è¯¯ä¿¡æ¯"
                                            result_record['error_details'] = json.dumps(response_data, ensure_ascii=False)
                                            result_record['error_msg'] = ""
                                    
                                except json.JSONDecodeError as e:
                                    print(f"  âŒ JSONè§£æå¤±è´¥: {e}")
                                    print(f"  åŸå§‹æ•°æ®: {analysis_response}")
                                    result_record['é—®é¢˜åˆ†æç»“æœ'] = f"åœ¨ {found_table} ä¸­æ‰¾åˆ°è®°å½•ï¼ŒçŠ¶æ€FAILUREï¼Œä½†JSONè§£æå¤±è´¥"
                                    result_record['é—®é¢˜ç›´æ¥åŸå› '] = "JSONè§£æé”™è¯¯"
                                    result_record['error_details'] = ""
                                    result_record['error_msg'] = ""
                                    
                            else:
                                print("  ğŸ“­ Analysis Response ä¸ºç©º")
                                result_record['é—®é¢˜åˆ†æç»“æœ'] = f"åœ¨ {found_table} ä¸­æ‰¾åˆ°è®°å½•ï¼ŒçŠ¶æ€FAILUREï¼Œä½†æ— åˆ†æå“åº”"
                                result_record['é—®é¢˜ç›´æ¥åŸå› '] = "FAILUREçŠ¶æ€ä½†æ— åˆ†æå“åº”"
                                result_record['error_details'] = ""
                                result_record['error_msg'] = ""
                                
                        else:
                            print(f"  âœ… ä»»åŠ¡çŠ¶æ€: {found_record.get('state', 'Unknown')}")
                            result_record['é—®é¢˜åˆ†æç»“æœ'] = f"åœ¨ {found_table} ä¸­æ‰¾åˆ°è®°å½•ï¼ŒçŠ¶æ€æ­£å¸¸"
                            result_record['é—®é¢˜ç›´æ¥åŸå› '] = "çŠ¶æ€æ­£å¸¸"
                            result_record['error_details'] = ""
                            result_record['error_msg'] = ""
                            
                else:
                    result_record['é—®é¢˜åˆ†æç»“æœ'] = f"æ‰¾åˆ° {total_records} æ¡è®°å½•ï¼Œåˆ†å¸ƒåœ¨å¤šä¸ªè¡¨ä¸­"
                    result_record['é—®é¢˜ç›´æ¥åŸå› '] = "ä»»åŠ¡æ•°é‡é”™è¯¯"
                    print(f"  âŒ é—®é¢˜è¯Šæ–­: ä»»åŠ¡æ•°é‡é”™è¯¯ (æ‰¾åˆ° {total_records} æ¡è®°å½•)")
                    
                    # å°è¯•ä»ç¬¬ä¸€æ¡è®°å½•ä¸­æå–task_id
                    first_record = None
                    for table_name, table_result in table_results.items():
                        if table_result['count'] > 0 and table_result['records']:
                            first_record = table_result['records'][0]
                            break
                    
                    if first_record:
                        ext_ssn = first_record.get('ext_ssn', '')
                        result_record['task_id'] = ext_ssn
                        print(f"    æå–åˆ°çš„Task ID (ext_ssn): {ext_ssn}")
                    
                    # æ˜¾ç¤ºå„è¡¨çš„è®°å½•åˆ†å¸ƒ
                    for table_name, table_result in table_results.items():
                        if table_result['count'] > 0:
                            print(f"    {table_name}: {table_result['count']} æ¡è®°å½•")
                
                analysis_results.append(result_record)
            
            # ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆåˆ†ææŠ¥å‘Š
            print("\n" + "=" * 60)
            print("ç¬¬ä¸‰æ­¥ï¼šåˆ†æç»“æœæ±‡æ€»")
            print("=" * 60)
            
            # è½¬æ¢ä¸ºDataFrame
            results_df = pd.DataFrame(analysis_results)
            
            # ç»Ÿè®¡é—®é¢˜ç±»å‹
            problem_summary = results_df['é—®é¢˜ç›´æ¥åŸå› '].value_counts()
            print("\né—®é¢˜ç±»å‹ç»Ÿè®¡:")
            for problem_type, count in problem_summary.items():
                print(f"  {problem_type}: {count} ä¸ªä»»åŠ¡")
            
            # æ˜¾ç¤ºè¯¦ç»†ç»“æœè¡¨
            print(f"\nè¯¦ç»†åˆ†æç»“æœ:")
            display_columns = ['id', 'shulex_ssn', 'task_id', 'asin', 'market', 'é—®é¢˜åˆ†æç»“æœ', 'é—®é¢˜ç›´æ¥åŸå› ']
            print(tabulate(results_df[display_columns], headers='keys', tablefmt='psql', showindex=False))
            
            # ä¿å­˜ç»“æœ
            if output_path:
                # å°†åŸå§‹æ•°æ®å’Œåˆ†æç»“æœåˆå¹¶
                final_results = filtered_data.copy()
                for i, result in enumerate(analysis_results):
                    final_results.loc[final_results.index[i], 'è§£å†³è¿›åº¦'] = "å·²åˆ†æ"
                    final_results.loc[final_results.index[i], 'task_id'] = result['task_id']
                    final_results.loc[final_results.index[i], 'é—®é¢˜åˆ†æç»“æœ'] = result['é—®é¢˜åˆ†æç»“æœ']
                    final_results.loc[final_results.index[i], 'é—®é¢˜ç›´æ¥åŸå› '] = result['é—®é¢˜ç›´æ¥åŸå› ']
                    final_results.loc[final_results.index[i], 'error_msg'] = result['error_msg']
                
                if output_path.endswith('.csv'):
                    final_results.to_csv(output_path, index=False, encoding='utf-8')
                else:
                    processor.save_to_excel(output_path, data={"åˆ†æç»“æœ": final_results})
                
                print(f"\nåˆ†æç»“æœå·²ä¿å­˜è‡³: {output_path}")
            
            return results_df
            
        finally:
            # å…³é—­æ•°æ®åº“è¿æ¥
            db.disconnect()
            
    except Exception as e:
        logger.error(f"åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        return None


def resubmit_parse_jobs(job_ids: List[str], output_path: Optional[str] = None) -> bool:
    """
    é‡æ–°æäº¤è§£æä»»åŠ¡
    
    Args:
        job_ids: è¦é‡æ–°æäº¤çš„ä»»åŠ¡IDåˆ—è¡¨ï¼ˆå¦‚: ["SL2813610252", "SL2789485480"]ï¼‰
        output_path: è¾“å‡ºç»“æœæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        bool: æäº¤æˆåŠŸè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    if not job_ids:
        logger.error("ä»»åŠ¡IDåˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        return False
    
    print("=" * 60)
    print("é‡æ–°æäº¤è§£æä»»åŠ¡")
    print("=" * 60)
    
    print(f"å‡†å¤‡é‡æ–°æäº¤ {len(job_ids)} ä¸ªä»»åŠ¡:")
    for i, job_id in enumerate(job_ids, 1):
        print(f"  {i}. {job_id}")
    
    # å‡†å¤‡è¯·æ±‚
    url = REPARSER_API_CONFIG['url']
    headers = REPARSER_API_CONFIG['headers'].copy()
    headers['X-Token'] = REPARSER_API_CONFIG['x_token']
    timeout = REPARSER_API_CONFIG['timeout']
    
    # å‡†å¤‡è¯·æ±‚ä½“ï¼ˆJSONæ•°ç»„ï¼‰
    request_data = job_ids
    
    print(f"\nå‘é€è¯·æ±‚åˆ°: {url}")
    print(f"è¯·æ±‚å¤´:")
    for key, value in headers.items():
        if key == 'X-Token':
            print(f"  {key}: {'*' * len(value[:4]) + value[:4]}...")  # éšè—å¤§éƒ¨åˆ†tokenå†…å®¹
        else:
            print(f"  {key}: {value}")
    
    print(f"è¯·æ±‚ä½“: {json.dumps(request_data, indent=2)}")
    
    try:
        # å‘é€POSTè¯·æ±‚
        response = requests.post(
            url=url,
            headers=headers,
            json=request_data,
            timeout=timeout
        )
        
        print(f"\nå“åº”çŠ¶æ€ç : {response.status_code}")
        print(f"å“åº”å¤´:")
        for key, value in response.headers.items():
            print(f"  {key}: {value}")
        
        # è§£æå“åº”
        if response.status_code == 200:
            try:
                response_data = response.json()
                print(f"\nâœ… è¯·æ±‚æˆåŠŸ!")
                print(f"å“åº”å†…å®¹:")
                print(json.dumps(response_data, indent=2, ensure_ascii=False))
                
                # åˆ›å»ºç»“æœè®°å½•
                results = []
                for job_id in job_ids:
                    results.append({
                        'job_id': job_id,
                        'status': 'submitted',
                        'timestamp': pd.Timestamp.now(),
                        'response': json.dumps(response_data, ensure_ascii=False)
                    })
                
                # ä¿å­˜ç»“æœ
                if output_path:
                    results_df = pd.DataFrame(results)
                    if output_path.endswith('.csv'):
                        results_df.to_csv(output_path, index=False, encoding='utf-8')
                    else:
                        # ä½¿ç”¨ExcelProcessorä¿å­˜
                        from src.file_processors.excel_processor import ExcelProcessor
                        processor = ExcelProcessor("")  # åˆ›å»ºä¸€ä¸ªä¸´æ—¶å¤„ç†å™¨
                        processor.save_to_excel(output_path, data={"é‡æ–°æäº¤ç»“æœ": results_df})
                    
                    print(f"\nç»“æœå·²ä¿å­˜è‡³: {output_path}")
                
                return True
                
            except json.JSONDecodeError:
                print(f"\nâœ… è¯·æ±‚æˆåŠŸï¼Œä½†å“åº”ä¸æ˜¯JSONæ ¼å¼:")
                print(f"å“åº”å†…å®¹: {response.text}")
                return True
                
        else:
            print(f"\nâŒ è¯·æ±‚å¤±è´¥!")
            print(f"å“åº”å†…å®¹: {response.text}")
            return False
            
    except requests.exceptions.RequestException as e:
        logger.error(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}")
        print(f"\nâŒ ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}")
        return False
    except Exception as e:
        logger.error(f"é‡æ–°æäº¤ä»»åŠ¡æ—¶å‡ºé”™: {str(e)}")
        print(f"\nâŒ é‡æ–°æäº¤ä»»åŠ¡æ—¶å‡ºé”™: {str(e)}")
        return False


def resubmit_from_analysis_results(file_path: str, column_name: str = 'shulex_ssn', output_path: Optional[str] = None) -> bool:
    """
    ä»åˆ†æç»“æœæ–‡ä»¶ä¸­æå–ä»»åŠ¡IDå¹¶é‡æ–°æäº¤è§£æä»»åŠ¡
    
    Args:
        file_path: åˆ†æç»“æœExcelæ–‡ä»¶è·¯å¾„
        column_name: åŒ…å«ä»»åŠ¡IDçš„åˆ—åï¼Œé»˜è®¤ä¸º'shulex_ssn'
        output_path: è¾“å‡ºç»“æœæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        bool: æäº¤æˆåŠŸè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    if not os.path.exists(file_path):
        logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return False
    
    print("=" * 60)
    print("ä»åˆ†æç»“æœä¸­æå–ä»»åŠ¡IDå¹¶é‡æ–°æäº¤")
    print("=" * 60)
    
    try:
        # è¯»å–Excelæ–‡ä»¶
        processor = ExcelProcessor(file_path)
        if not processor.read_file():
            return False
        
        data = processor.get_data()
        if data is None or len(data) == 0:
            logger.error("æ–‡ä»¶ä¸­æ²¡æœ‰æ•°æ®")
            return False
        
        # æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
        if column_name not in data.columns:
            logger.error(f"åˆ— '{column_name}' ä¸å­˜åœ¨äºæ–‡ä»¶ä¸­")
            print(f"å¯ç”¨åˆ—: {', '.join(data.columns)}")
            return False
        
        # æå–ä»»åŠ¡ID
        job_ids = data[column_name].dropna().unique().tolist()
        job_ids = [str(job_id) for job_id in job_ids if str(job_id).strip()]  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶å»é™¤ç©ºå€¼
        
        if not job_ids:
            logger.error(f"ä»åˆ— '{column_name}' ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„ä»»åŠ¡ID")
            return False
        
        print(f"ä»æ–‡ä»¶ {file_path} çš„åˆ— '{column_name}' ä¸­æå–åˆ° {len(job_ids)} ä¸ªä»»åŠ¡ID")
        
        # è°ƒç”¨é‡æ–°æäº¤æ–¹æ³•
        return resubmit_parse_jobs(job_ids, output_path)
        
    except Exception as e:
        logger.error(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        return False


def resubmit_crawler_jobs(job_ids: List[str], output_path: Optional[str] = None) -> bool:
    """
    é‡æ–°æäº¤çˆ¬è™«ä»»åŠ¡
    
    Args:
        job_ids: è¦é‡æ–°æäº¤çš„ä»»åŠ¡IDåˆ—è¡¨ï¼ˆå¦‚: ["SL2813610252", "SL2789485480"]ï¼‰
        output_path: è¾“å‡ºç»“æœæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        bool: æäº¤æˆåŠŸè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    if not job_ids:
        logger.error("ä»»åŠ¡IDåˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        return False
    
    print("=" * 60)
    print("é‡æ–°æäº¤çˆ¬è™«ä»»åŠ¡")
    print("=" * 60)
    
    print(f"å‡†å¤‡é‡æ–°æäº¤ {len(job_ids)} ä¸ªä»»åŠ¡:")
    for i, job_id in enumerate(job_ids, 1):
        print(f"  {i}. {job_id}")
    
    # å‡†å¤‡è¯·æ±‚
    url = CRAWLER_API_CONFIG['url']
    headers = CRAWLER_API_CONFIG['headers'].copy()
    headers['X-Token'] = CRAWLER_API_CONFIG['x_token']
    timeout = CRAWLER_API_CONFIG['timeout']
    
    print(f"\nå‘é€è¯·æ±‚åˆ°: {url}")
    print(f"è¯·æ±‚å¤´:")
    for key, value in headers.items():
        if key == 'X-Token':
            print(f"  {key}: {'*' * len(value[:4]) + value[:4]}...")  # éšè—å¤§éƒ¨åˆ†tokenå†…å®¹
        else:
            print(f"  {key}: {value}")
    
    # é€ä¸ªå¤„ç†ä»»åŠ¡IDï¼ˆå› ä¸ºAPIè¦æ±‚å•ä¸ªreq_ssnæ ¼å¼ï¼‰
    all_results = []
    success_count = 0
    fail_count = 0
    
    for i, job_id in enumerate(job_ids):
        print(f"\n--- æäº¤ç¬¬ {i+1}/{len(job_ids)} ä¸ªä»»åŠ¡: {job_id} ---")
        
        # å‡†å¤‡å•ä¸ªä»»åŠ¡çš„è¯·æ±‚ä½“
        request_data = {"req_ssn": job_id}
        
        print(f"è¯·æ±‚ä½“: {json.dumps(request_data, indent=2)}")
        
        try:
            # å‘é€POSTè¯·æ±‚
            response = requests.post(
                url=url,
                headers=headers,
                json=request_data,
                timeout=timeout
            )
            
            print(f"å“åº”çŠ¶æ€ç : {response.status_code}")
            
            # è§£æå“åº”
            if response.status_code == 200:
                try:
                    response_data = response.json()
                    print(f"âœ… è¯·æ±‚æˆåŠŸ!")
                    print(f"å“åº”å†…å®¹:")
                    print(json.dumps(response_data, indent=2, ensure_ascii=False))
                    
                    # è®°å½•æˆåŠŸç»“æœ
                    result_record = {
                        'job_id': job_id,
                        'status': 'submitted',
                        'timestamp': pd.Timestamp.now(),
                        'response': json.dumps(response_data, ensure_ascii=False),
                        'http_status': response.status_code
                    }
                    success_count += 1
                    
                except json.JSONDecodeError:
                    print(f"âœ… è¯·æ±‚æˆåŠŸï¼Œä½†å“åº”ä¸æ˜¯JSONæ ¼å¼:")
                    print(f"å“åº”å†…å®¹: {response.text}")
                    
                    # è®°å½•æˆåŠŸç»“æœï¼ˆéJSONå“åº”ï¼‰
                    result_record = {
                        'job_id': job_id,
                        'status': 'submitted',
                        'timestamp': pd.Timestamp.now(),
                        'response': response.text,
                        'http_status': response.status_code
                    }
                    success_count += 1
                    
            else:
                print(f"âŒ è¯·æ±‚å¤±è´¥!")
                print(f"å“åº”å†…å®¹: {response.text}")
                
                # è®°å½•å¤±è´¥ç»“æœ
                result_record = {
                    'job_id': job_id,
                    'status': 'failed',
                    'timestamp': pd.Timestamp.now(),
                    'response': response.text,
                    'http_status': response.status_code
                }
                fail_count += 1
                
        except requests.exceptions.RequestException as e:
            logger.error(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}")
            print(f"âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}")
            
            # è®°å½•å¤±è´¥ç»“æœ
            result_record = {
                'job_id': job_id,
                'status': 'network_error',
                'timestamp': pd.Timestamp.now(),
                'response': str(e),
                'http_status': -1
            }
            fail_count += 1
            
        except Exception as e:
            logger.error(f"å¤„ç†ä»»åŠ¡æ—¶å‡ºé”™: {str(e)}")
            print(f"âŒ å¤„ç†ä»»åŠ¡æ—¶å‡ºé”™: {str(e)}")
            
            # è®°å½•å¤±è´¥ç»“æœ
            result_record = {
                'job_id': job_id,
                'status': 'error',
                'timestamp': pd.Timestamp.now(),
                'response': str(e),
                'http_status': -1
            }
            fail_count += 1
        
        all_results.append(result_record)
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    print("\n" + "=" * 60)
    print("æäº¤ç»“æœæ±‡æ€»")
    print("=" * 60)
    print(f"æ€»ä»»åŠ¡æ•°: {len(job_ids)}")
    print(f"æˆåŠŸæäº¤: {success_count}")
    print(f"å¤±è´¥æäº¤: {fail_count}")
    print(f"æˆåŠŸç‡: {(success_count/len(job_ids)*100):.1f}%")
    
    # ä¿å­˜ç»“æœ
    if output_path:
        results_df = pd.DataFrame(all_results)
        if output_path.endswith('.csv'):
            results_df.to_csv(output_path, index=False, encoding='utf-8')
        else:
            # ä½¿ç”¨ExcelProcessorä¿å­˜
            from src.file_processors.excel_processor import ExcelProcessor
            processor = ExcelProcessor("")  # åˆ›å»ºä¸€ä¸ªä¸´æ—¶å¤„ç†å™¨
            processor.save_to_excel(output_path, data={"é‡æ–°æäº¤çˆ¬è™«ç»“æœ": results_df})
        
        print(f"\nç»“æœå·²ä¿å­˜è‡³: {output_path}")
    
    return fail_count == 0  # æ‰€æœ‰ä»»åŠ¡éƒ½æˆåŠŸæ‰è¿”å›True


def resubmit_crawler_from_txt_file(file_path: str, nrows: int = 1, output_path: Optional[str] = None) -> bool:
    """
    ä»txtæ–‡ä»¶ä¸­è¯»å–ä»»åŠ¡IDå¹¶é‡æ–°æäº¤çˆ¬è™«ä»»åŠ¡
    
    Args:
        file_path: txtæ–‡ä»¶è·¯å¾„ï¼Œæ¯è¡Œä¸€ä¸ªä»»åŠ¡ID
        nrows: è¯»å–çš„è¡Œæ•°ï¼Œé»˜è®¤ä¸º1
        output_path: è¾“å‡ºç»“æœæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        bool: æäº¤æˆåŠŸè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    if not os.path.exists(file_path):
        logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return False
    
    print("=" * 60)
    print("ä»txtæ–‡ä»¶ä¸­è¯»å–ä»»åŠ¡IDå¹¶é‡æ–°æäº¤çˆ¬è™«ä»»åŠ¡")
    print("=" * 60)
    
    try:
        # è¯»å–txtæ–‡ä»¶
        job_ids = []
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            
        print(f"æ–‡ä»¶ {file_path} å…±æœ‰ {len(lines)} è¡Œæ•°æ®")
        print(f"å°†è¯»å–å‰ {nrows} è¡Œæ•°æ®")
        
        # å¤„ç†æŒ‡å®šè¡Œæ•°çš„æ•°æ®
        for i, line in enumerate(lines[:nrows]):
            line = line.strip()
            if line:  # è·³è¿‡ç©ºè¡Œ
                # æ£€æŸ¥æ˜¯å¦æœ‰SLå‰ç¼€ï¼Œå¦‚æœæ²¡æœ‰åˆ™æ·»åŠ 
                if not line.startswith('SL'):
                    job_id = f"SL{line}"
                else:
                    job_id = line
                job_ids.append(job_id)
                print(f"  {i+1}. {line} -> {job_id}")
        
        if not job_ids:
            logger.error("æ²¡æœ‰è¯»å–åˆ°æœ‰æ•ˆçš„ä»»åŠ¡ID")
            return False
        
        print(f"\næˆåŠŸå¤„ç† {len(job_ids)} ä¸ªä»»åŠ¡ID")
        
        # è°ƒç”¨é‡æ–°æäº¤æ–¹æ³•
        return resubmit_crawler_jobs(job_ids, output_path)
        
    except Exception as e:
        logger.error(f"å¤„ç†txtæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        return False


def resubmit_from_txt_file(file_path: str, nrows: int = 1, output_path: Optional[str] = None) -> bool:
    """
    ä»txtæ–‡ä»¶ä¸­è¯»å–ä»»åŠ¡IDå¹¶é‡æ–°æäº¤çˆ¬è™«ä»»åŠ¡ï¼ˆä¸ºäº†ä¿æŒå‘åå…¼å®¹æ€§ï¼‰
    è¿™ä¸ªå‡½æ•°å®é™…ä¸Šè°ƒç”¨resubmit_crawler_from_txt_file
    
    Args:
        file_path: txtæ–‡ä»¶è·¯å¾„ï¼Œæ¯è¡Œä¸€ä¸ªä»»åŠ¡ID
        nrows: è¯»å–çš„è¡Œæ•°ï¼Œé»˜è®¤ä¸º1
        output_path: è¾“å‡ºç»“æœæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        bool: æäº¤æˆåŠŸè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    # ç›´æ¥è°ƒç”¨çˆ¬è™«ä»»åŠ¡æäº¤æ–¹æ³•
    return resubmit_crawler_from_txt_file(file_path, nrows, output_path)


def resubmit_parse_from_txt_file(file_path: str, nrows: int = 1, output_path: Optional[str] = None) -> bool:
    """
    ä»txtæ–‡ä»¶ä¸­è¯»å–ä»»åŠ¡IDå¹¶é‡æ–°æäº¤è§£æä»»åŠ¡
    
    Args:
        file_path: txtæ–‡ä»¶è·¯å¾„ï¼Œæ¯è¡Œä¸€ä¸ªä»»åŠ¡ID
        nrows: è¯»å–çš„è¡Œæ•°ï¼Œé»˜è®¤ä¸º1
        output_path: è¾“å‡ºç»“æœæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        bool: æäº¤æˆåŠŸè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    if not os.path.exists(file_path):
        logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return False
    
    print("=" * 60)
    print("ä»txtæ–‡ä»¶ä¸­è¯»å–ä»»åŠ¡IDå¹¶é‡æ–°æäº¤è§£æä»»åŠ¡")
    print("=" * 60)
    
    try:
        # è¯»å–txtæ–‡ä»¶
        job_ids = []
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            
        print(f"æ–‡ä»¶ {file_path} å…±æœ‰ {len(lines)} è¡Œæ•°æ®")
        print(f"å°†è¯»å–å‰ {nrows} è¡Œæ•°æ®")
        
        # å¤„ç†æŒ‡å®šè¡Œæ•°çš„æ•°æ®
        for i, line in enumerate(lines[:nrows]):
            line = line.strip()
            if line:  # è·³è¿‡ç©ºè¡Œ
                # æ£€æŸ¥æ˜¯å¦æœ‰SLå‰ç¼€ï¼Œå¦‚æœæ²¡æœ‰åˆ™æ·»åŠ 
                if not line.startswith('SL'):
                    job_id = f"SL{line}"
                else:
                    job_id = line
                job_ids.append(job_id)
                print(f"  {i+1}. {line} -> {job_id}")
        
        if not job_ids:
            logger.error("æ²¡æœ‰è¯»å–åˆ°æœ‰æ•ˆçš„ä»»åŠ¡ID")
            return False
        
        print(f"\næˆåŠŸå¤„ç† {len(job_ids)} ä¸ªä»»åŠ¡ID")
        
        # è°ƒç”¨é‡æ–°æäº¤è§£æä»»åŠ¡æ–¹æ³•
        return resubmit_parse_jobs(job_ids, output_path)
        
    except Exception as e:
        logger.error(f"å¤„ç†txtæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        return False


def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨
    parser = argparse.ArgumentParser(description='Shulex-Ankeræ•°æ®éªŒè¯å·¥å…·')
    subparsers = parser.add_subparsers(dest='command', help='å­å‘½ä»¤')
    
    # æµ‹è¯•æ•°æ®åº“è¿æ¥å‘½ä»¤
    test_parser = subparsers.add_parser('test_connection', help='æµ‹è¯•æ•°æ®åº“è¿æ¥')
    
    # å¤„ç†æ–‡ä»¶å‘½ä»¤
    process_parser = subparsers.add_parser('process_file', help='å¤„ç†CSVæˆ–Excelæ–‡ä»¶')
    process_parser.add_argument('--file', '-f', required=True, help='è¾“å…¥æ–‡ä»¶è·¯å¾„')
    process_parser.add_argument('--type', '-t', choices=['csv', 'excel'], help='æ–‡ä»¶ç±»å‹ï¼Œé»˜è®¤æ ¹æ®æ‰©å±•ååˆ¤æ–­')
    process_parser.add_argument('--output', '-o', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    
    # åˆ†å—è¯»å–Excelæ–‡ä»¶å‘½ä»¤
    excel_parser = subparsers.add_parser('read_excel_chunked', help='åˆ†å—è¯»å–å¤§Excelæ–‡ä»¶')
    excel_parser.add_argument('--file', '-f', required=True, help='Excelæ–‡ä»¶è·¯å¾„')
    excel_parser.add_argument('--nrows', '-n', type=int, default=10, help='è¯»å–çš„è¡Œæ•°ï¼Œé»˜è®¤ä¸º10')
    excel_parser.add_argument('--skiprows', '-s', type=int, default=0, help='è·³è¿‡çš„è¡Œæ•°ï¼Œé»˜è®¤ä¸º0')
    excel_parser.add_argument('--sheet', help='å·¥ä½œè¡¨åç§°ï¼Œé»˜è®¤ä¸ºç¬¬ä¸€ä¸ªå·¥ä½œè¡¨')
    excel_parser.add_argument('--output', '-o', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    
    # ç­›é€‰ç©ºå€¼åˆ—å‘½ä»¤
    filter_parser = subparsers.add_parser('filter_null_column', help='ç­›é€‰æŒ‡å®šåˆ—ä¸ºç©ºçš„æ•°æ®')
    filter_parser.add_argument('--file', '-f', required=True, help='Excelæ–‡ä»¶è·¯å¾„')
    filter_parser.add_argument('--column', '-c', required=True, help='è¦ç­›é€‰çš„åˆ—å')
    filter_parser.add_argument('--nrows', '-n', type=int, default=10, help='è¿”å›çš„è¡Œæ•°ï¼Œé»˜è®¤ä¸º10')
    filter_parser.add_argument('--sheet', help='å·¥ä½œè¡¨åç§°ï¼Œé»˜è®¤ä¸ºç¬¬ä¸€ä¸ªå·¥ä½œè¡¨')
    filter_parser.add_argument('--output', '-o', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    filter_parser.add_argument('--prepare-db', action='store_true', help='å‡†å¤‡æ•°æ®åº“æŸ¥è¯¢ä¿¡æ¯')
    
    # ä»»åŠ¡æ•°æ®åº“åˆ†æå‘½ä»¤
    analyze_parser = subparsers.add_parser('analyze_tasks_with_db', help='åˆ†æä»»åŠ¡æ•°æ®å¹¶æŸ¥è¯¢æ•°æ®åº“è¿›è¡Œé—®é¢˜è¯Šæ–­')
    analyze_parser.add_argument('--file', '-f', required=True, help='Excelæ–‡ä»¶è·¯å¾„')
    analyze_parser.add_argument('--column', '-c', default='è§£å†³è¿›åº¦', help='è¦ç­›é€‰çš„åˆ—åï¼Œé»˜è®¤ä¸º"è§£å†³è¿›åº¦"')
    analyze_parser.add_argument('--nrows', '-n', type=int, default=10, help='åˆ†æçš„ä»»åŠ¡æ•°é‡ï¼Œé»˜è®¤ä¸º10')
    analyze_parser.add_argument('--sheet', help='å·¥ä½œè¡¨åç§°ï¼Œé»˜è®¤ä¸ºç¬¬ä¸€ä¸ªå·¥ä½œè¡¨')
    analyze_parser.add_argument('--output', '-o', help='è¾“å‡ºåˆ†æç»“æœçš„æ–‡ä»¶è·¯å¾„')
    
    # é‡æ–°æäº¤è§£æä»»åŠ¡å‘½ä»¤
    resubmit_parser = subparsers.add_parser('resubmit_parse_jobs', help='é‡æ–°æäº¤è§£æä»»åŠ¡')
    resubmit_parser.add_argument('--job-ids', nargs='+', required=True, help='è¦é‡æ–°æäº¤çš„ä»»åŠ¡IDåˆ—è¡¨ï¼Œå¦‚: SL2813610252 SL2789485480')
    resubmit_parser.add_argument('--output', '-o', help='è¾“å‡ºç»“æœæ–‡ä»¶è·¯å¾„')
    
    # ä»åˆ†æç»“æœæ–‡ä»¶é‡æ–°æäº¤ä»»åŠ¡å‘½ä»¤
    resubmit_from_file_parser = subparsers.add_parser('resubmit_from_analysis_results', help='ä»åˆ†æç»“æœæ–‡ä»¶ä¸­æå–ä»»åŠ¡IDå¹¶é‡æ–°æäº¤è§£æä»»åŠ¡')
    resubmit_from_file_parser.add_argument('--file', '-f', required=True, help='åˆ†æç»“æœExcelæ–‡ä»¶è·¯å¾„')
    resubmit_from_file_parser.add_argument('--column', '-c', default='shulex_ssn', help='åŒ…å«ä»»åŠ¡IDçš„åˆ—åï¼Œé»˜è®¤ä¸ºshulex_ssn')
    resubmit_from_file_parser.add_argument('--output', '-o', help='è¾“å‡ºç»“æœæ–‡ä»¶è·¯å¾„')
    
    # ä»txtæ–‡ä»¶é‡æ–°æäº¤ä»»åŠ¡å‘½ä»¤
    resubmit_from_txt_parser = subparsers.add_parser('resubmit_from_txt_file', help='ä»txtæ–‡ä»¶ä¸­è¯»å–ä»»åŠ¡IDå¹¶é‡æ–°æäº¤çˆ¬è™«ä»»åŠ¡')
    resubmit_from_txt_parser.add_argument('--file', '-f', required=True, help='txtæ–‡ä»¶è·¯å¾„ï¼Œæ¯è¡Œä¸€ä¸ªä»»åŠ¡ID')
    resubmit_from_txt_parser.add_argument('--nrows', '-n', type=int, default=1, help='è¯»å–çš„è¡Œæ•°ï¼Œé»˜è®¤ä¸º1')
    resubmit_from_txt_parser.add_argument('--output', '-o', help='è¾“å‡ºç»“æœæ–‡ä»¶è·¯å¾„')
    
    # é‡æ–°æäº¤çˆ¬è™«ä»»åŠ¡å‘½ä»¤
    resubmit_crawler_parser = subparsers.add_parser('resubmit_crawler_jobs', help='é‡æ–°æäº¤çˆ¬è™«ä»»åŠ¡')
    resubmit_crawler_parser.add_argument('--job-ids', nargs='+', required=True, help='è¦é‡æ–°æäº¤çš„ä»»åŠ¡IDåˆ—è¡¨ï¼Œå¦‚: SL2813610252 SL2789485480')
    resubmit_crawler_parser.add_argument('--output', '-o', help='è¾“å‡ºç»“æœæ–‡ä»¶è·¯å¾„')
    
    # ä»txtæ–‡ä»¶é‡æ–°æäº¤çˆ¬è™«ä»»åŠ¡å‘½ä»¤
    resubmit_crawler_from_txt_parser = subparsers.add_parser('resubmit_crawler_from_txt_file', help='ä»txtæ–‡ä»¶ä¸­è¯»å–ä»»åŠ¡IDå¹¶é‡æ–°æäº¤çˆ¬è™«ä»»åŠ¡')
    resubmit_crawler_from_txt_parser.add_argument('--file', '-f', required=True, help='txtæ–‡ä»¶è·¯å¾„ï¼Œæ¯è¡Œä¸€ä¸ªä»»åŠ¡ID')
    resubmit_crawler_from_txt_parser.add_argument('--nrows', '-n', type=int, default=1, help='è¯»å–çš„è¡Œæ•°ï¼Œé»˜è®¤ä¸º1')
    resubmit_crawler_from_txt_parser.add_argument('--output', '-o', help='è¾“å‡ºç»“æœæ–‡ä»¶è·¯å¾„')
    
    # ä»txtæ–‡ä»¶é‡æ–°æäº¤è§£æä»»åŠ¡å‘½ä»¤
    resubmit_parse_from_txt_parser = subparsers.add_parser('resubmit_parse_from_txt_file', help='ä»txtæ–‡ä»¶ä¸­è¯»å–ä»»åŠ¡IDå¹¶é‡æ–°æäº¤è§£æä»»åŠ¡')
    resubmit_parse_from_txt_parser.add_argument('--file', '-f', required=True, help='txtæ–‡ä»¶è·¯å¾„ï¼Œæ¯è¡Œä¸€ä¸ªä»»åŠ¡ID')
    resubmit_parse_from_txt_parser.add_argument('--nrows', '-n', type=int, default=1, help='è¯»å–çš„è¡Œæ•°ï¼Œé»˜è®¤ä¸º1')
    resubmit_parse_from_txt_parser.add_argument('--output', '-o', help='è¾“å‡ºç»“æœæ–‡ä»¶è·¯å¾„')
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parser.parse_args()
    
    # æ ¹æ®å‘½ä»¤æ‰§è¡Œç›¸åº”çš„åŠŸèƒ½
    if args.command == 'test_connection':
        test_db_connection()
    elif args.command == 'process_file':
        process_file(args.file, args.type, args.output)
    elif args.command == 'read_excel_chunked':
        read_excel_chunked(args.file, args.nrows, args.skiprows, args.sheet, args.output)
    elif args.command == 'filter_null_column':
        filter_null_column(args.file, args.column, args.nrows, args.sheet, args.output, args.prepare_db)
    elif args.command == 'validate':
        validate_data(args.file, args.output)
    elif args.command == 'analyze_tasks_with_db':
        analyze_tasks_with_db(args.file, args.column, args.nrows, args.sheet, args.output)
    elif args.command == 'resubmit_parse_jobs':
        resubmit_parse_jobs(args.job_ids, args.output)
    elif args.command == 'resubmit_from_analysis_results':
        resubmit_from_analysis_results(args.file, args.column, args.output)
    elif args.command == 'resubmit_from_txt_file':
        resubmit_from_txt_file(args.file, args.nrows, args.output)
    elif args.command == 'resubmit_crawler_jobs':
        resubmit_crawler_jobs(args.job_ids, args.output)
    elif args.command == 'resubmit_crawler_from_txt_file':
        resubmit_crawler_from_txt_file(args.file, args.nrows, args.output)
    elif args.command == 'resubmit_parse_from_txt_file':
        resubmit_parse_from_txt_file(args.file, args.nrows, args.output)
    else:
        parser.print_help()


if __name__ == '__main__':
    main() 