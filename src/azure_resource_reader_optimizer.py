#!/usr/bin/env python3
"""
Azure Resource Reader ä¼˜åŒ–å™¨æ¨¡å—
æä¾›ä¼˜åŒ–çš„è§£ææ–‡ä»¶è·å–æ–¹æ³•ï¼Œä¼˜å…ˆä½¿ç”¨analysis_responseé“¾æ¥ä¸‹è½½
"""

import os
import sys
import json
import logging
import requests
from pathlib import Path
from typing import Dict, List, Optional, Union, Tuple, Any
from datetime import datetime
import gzip

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))

# å¯¼å…¥æ•°æ®åº“è¿æ¥å™¨
from src.db.connector import DatabaseConnector
from config.db_config import DB_CONFIG

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


def convert_job_id_to_task_info(job_id: str) -> Optional[Tuple[str, str]]:
    """
    é€šè¿‡æ•°æ®åº“æŸ¥è¯¢å°† job_id è½¬æ¢ä¸º task_id å’Œ analysis_response
    
    Args:
        job_id: è¯·æ±‚åºåˆ—å·ï¼Œå¦‚ 'SL2796867471'
        
    Returns:
        Optional[Tuple[str, str]]: æ‰¾åˆ°çš„ (task_id, analysis_response)ï¼Œæœªæ‰¾åˆ°è¿”å›None
        - task_id: ext_ssnå­—æ®µçš„å€¼
        - analysis_response: analysis_responseå­—æ®µçš„å€¼ï¼ˆå¯èƒ½ä¸ºNoneï¼‰
    """
    # åˆ›å»ºæ•°æ®åº“è¿æ¥
    db_config = DB_CONFIG.copy()
    db_config['database'] = 'shulex_collector_prod'
    
    db = DatabaseConnector(db_config)
    if not db.connect():
        logger.error("æ— æ³•è¿æ¥åˆ°æ•°æ®åº“ shulex_collector_prod")
        return None
    
    try:
        # å®šä¹‰è¦æŸ¥è¯¢çš„è¡¨
        tables_to_check = ['log_a', 'log_b', 'log_c', 'log_d']
        
        logger.info(f"æ­£åœ¨æŸ¥è¯¢ job_id: {job_id}")
        
        all_results = []
        
        # æŸ¥è¯¢å„ä¸ªè¡¨ï¼Œå¢åŠ analysis_responseå­—æ®µ
        for table_name in tables_to_check:
            query = f"SELECT ext_ssn, analysis_response FROM {table_name} WHERE req_ssn = %s"
            try:
                records = db.execute_query(query, (job_id,))
                if records:
                    all_results.extend(records)
                    logger.info(f"åœ¨è¡¨ {table_name} ä¸­æ‰¾åˆ° {len(records)} æ¡è®°å½•")
                    
            except Exception as e:
                logger.error(f"æŸ¥è¯¢è¡¨ {table_name} å¤±è´¥: {str(e)}")
        
        # åˆ†ææŸ¥è¯¢ç»“æœ
        if len(all_results) == 0:
            logger.warning(f"åœ¨æ‰€æœ‰è¡¨ä¸­éƒ½æ²¡æœ‰æ‰¾åˆ° job_id: {job_id}")
            return None
            
        elif len(all_results) == 1:
            # æ‰¾åˆ°å”¯ä¸€è®°å½•
            record = all_results[0]
            ext_ssn = record.get('ext_ssn', '')
            analysis_response = record.get('analysis_response', None)
            
            if ext_ssn:
                logger.info(f"æ‰¾åˆ°å”¯ä¸€è®°å½•ï¼Œtask_id (ext_ssn): {ext_ssn}")
                if analysis_response:
                    logger.info(f"æ‰¾åˆ° analysis_response å­—æ®µï¼Œé•¿åº¦: {len(str(analysis_response))} å­—ç¬¦")
                else:
                    logger.info("analysis_response å­—æ®µä¸ºç©º")
                return (ext_ssn, analysis_response)
            else:
                logger.warning(f"æ‰¾åˆ°è®°å½•ä½† ext_ssn ä¸ºç©º")
                return None
                
        else:
            # æ‰¾åˆ°å¤šæ¡è®°å½•ï¼Œéœ€è¦å»é‡
            unique_records = {}
            for record in all_results:
                ext_ssn = record.get('ext_ssn', '')
                analysis_response = record.get('analysis_response', None)
                if ext_ssn:
                    if ext_ssn not in unique_records:
                        unique_records[ext_ssn] = analysis_response
                    # å¦‚æœå­˜åœ¨å¤šä¸ªç›¸åŒext_ssnä½†ä¸åŒanalysis_responseï¼Œä¼˜å…ˆé€‰æ‹©éç©ºçš„
                    elif analysis_response and not unique_records[ext_ssn]:
                        unique_records[ext_ssn] = analysis_response
            
            if len(unique_records) == 1:
                task_id = list(unique_records.keys())[0]
                analysis_response = unique_records[task_id]
                logger.info(f"æ‰¾åˆ° {len(all_results)} æ¡è®°å½•ï¼Œå»é‡åå¾—åˆ°å”¯ä¸€ task_id: {task_id}")
                if analysis_response:
                    logger.info(f"æ‰¾åˆ° analysis_response å­—æ®µï¼Œé•¿åº¦: {len(str(analysis_response))} å­—ç¬¦")
                return (task_id, analysis_response)
            else:
                logger.warning(f"æ‰¾åˆ° {len(all_results)} æ¡è®°å½•ï¼Œä½†åŒ…å« {len(unique_records)} ä¸ªä¸åŒçš„ ext_ssn")
                return None
    
    finally:
        db.disconnect()


def try_download_from_analysis_response(task_type: str, task_id: str, 
                                       analysis_response: str, save_dir: str, 
                                       decompress: bool = True) -> Dict[str, any]:
    """
    å°è¯•ä»analysis_responseä¸­çš„é“¾æ¥ä¸‹è½½æ–‡ä»¶
    
    Args:
        task_type: ä»»åŠ¡ç±»å‹
        task_id: ä»»åŠ¡ID
        analysis_response: analysis_response JSONå­—ç¬¦ä¸²
        save_dir: ä¿å­˜ç›®å½•
        decompress: æ˜¯å¦è§£å‹ç¼©
        
    Returns:
        Dict: ä¸‹è½½ç»“æœï¼ŒåŒ…å«æå–çš„æ–‡ä»¶è·¯å¾„ä¿¡æ¯
    """
    # å¯¼å…¥é…ç½®è§£æå‡½æ•°
    try:
        from config.analysis_response_config import parse_analysis_response
    except ImportError as e:
        return {
            'success': False,
            'error': f'å¯¼å…¥é…ç½®å¤±è´¥: {str(e)}',
            'files_downloaded': [],
            'extracted_blob_path': None
        }
    
    try:
        # è§£æanalysis_response
        download_links = parse_analysis_response(analysis_response)
        
        if not download_links:
            return {
                'success': False,
                'error': 'analysis_responseä¸­æ²¡æœ‰æ‰¾åˆ°ä¸‹è½½é“¾æ¥',
                'files_downloaded': [],
                'extracted_blob_path': None
            }
        
        logger.info(f"ğŸ“¡ ä»analysis_responseè§£æåˆ° {len(download_links)} ä¸ªä¸‹è½½é“¾æ¥")
        
        # ğŸ†• åˆ›å»ºç»Ÿä¸€ç›®å½•ç»“æ„: data/output/{task_type}/{task_id}/
        save_path = Path(save_dir) / task_type / task_id
        save_path.mkdir(parents=True, exist_ok=True)
        
        downloaded_files = []
        extracted_blob_path = None
        
        for i, url in enumerate(download_links):
            try:
                logger.info(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½ç¬¬ {i+1} ä¸ªæ–‡ä»¶: {url}")
                
                # ğŸ†• æå–Blobè·¯å¾„ï¼ˆç”¨äºåç»­å›é€€ï¼‰
                if extracted_blob_path is None:
                    extracted_blob_path = extract_blob_path_from_url(url)
                
                # ä½¿ç”¨requestsä¸‹è½½
                response = requests.get(url, timeout=30)
                response.raise_for_status()
                
                content = response.content
                
                # å¦‚æœéœ€è¦è§£å‹ç¼©ä¸”æ˜¯gzipå†…å®¹
                if decompress and (url.endswith('.gz') or 'gzip' in response.headers.get('content-encoding', '')):
                    try:
                        content = gzip.decompress(content)
                        logger.info("âœ… æ–‡ä»¶å·²è§£å‹ç¼©")
                    except gzip.BadGzipFile:
                        logger.warning("æ–‡ä»¶ä¸æ˜¯æœ‰æ•ˆçš„gzipæ ¼å¼ï¼Œä¿æŒåŸå†…å®¹")
                
                # ğŸ†• ç»Ÿä¸€ä½¿ç”¨ parse_result.json ä½œä¸ºæ–‡ä»¶å
                saved_filename = 'parse_result.json'
                file_path = save_path / saved_filename
                
                # ä¿å­˜æ–‡ä»¶ 
                if isinstance(content, str):
                    with open(file_path, 'w', encoding='utf-8') as f:
                        f.write(content)
                else:
                    # å°è¯•è§£ç ä¸ºUTF-8ï¼ˆJSONæ–‡ä»¶é€šå¸¸æ˜¯æ–‡æœ¬ï¼‰
                    try:
                        content_str = content.decode('utf-8')
                        with open(file_path, 'w', encoding='utf-8') as f:
                            f.write(content_str)
                    except UnicodeDecodeError:
                        # å¦‚æœæ— æ³•è§£ç ï¼Œä¿å­˜ä¸ºäºŒè¿›åˆ¶
                        with open(file_path, 'wb') as f:
                            f.write(content)
                
                logger.info(f"âœ… æ–‡ä»¶å·²ä¿å­˜: {file_path}")
                
                downloaded_files.append({
                    'original_name': extract_filename_from_url(url),
                    'saved_name': saved_filename,  # ğŸ†• ç»Ÿä¸€çš„æ–‡ä»¶å
                    'local_path': str(file_path),
                    'size': len(content),
                    'url': url
                })
                
            except Exception as e:
                logger.error(f"âŒ ä¸‹è½½æ–‡ä»¶å¤±è´¥ ({url}): {str(e)}")
                continue
        
        if downloaded_files:
            logger.info(f"âœ… ä»analysis_responseé“¾æ¥ä¸‹è½½æˆåŠŸï¼Œå…± {len(downloaded_files)} ä¸ªæ–‡ä»¶")
            return {
                'success': True,
                'files_downloaded': downloaded_files,
                'save_path': str(save_path),
                'extracted_blob_path': extracted_blob_path
            }
        else:
            logger.warning("âš ï¸  æ‰€æœ‰ä¸‹è½½é“¾æ¥éƒ½å¤±è´¥äº†")
            return {
                'success': False,
                'error': 'æ‰€æœ‰ä¸‹è½½é“¾æ¥éƒ½å¤±è´¥äº†',
                'files_downloaded': [],
                'extracted_blob_path': extracted_blob_path
            }
            
    except Exception as e:
        logger.error(f"âŒ å¤„ç†analysis_responseå¤±è´¥: {str(e)}")
        return {
            'success': False,
            'error': f'å¤„ç†analysis_responseå¤±è´¥: {str(e)}',
            'files_downloaded': [],
            'extracted_blob_path': None
        }


def extract_filename_from_url(url: str) -> str:
    """
    ä»URLä¸­æå–æ–‡ä»¶å
    
    Args:
        url: ä¸‹è½½URL
        
    Returns:
        str: æ–‡ä»¶å
    """
    try:
        from urllib.parse import urlparse, unquote
        parsed_url = urlparse(url)
        path = unquote(parsed_url.path)
        filename = path.split('/')[-1]
        
        # å¦‚æœæå–åˆ°æœ‰æ•ˆæ–‡ä»¶ååˆ™è¿”å›ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤åç§°
        if filename and '.' in filename:
            return filename
        else:
            return "parse_result.json"
            
    except Exception:
        return "parse_result.json"


def extract_blob_path_from_url(url: str) -> Optional[str]:
    """
    ğŸ†• ä»URLä¸­æå–Azure Blobè·¯å¾„
    ä¾‹å¦‚: https://collector0109.blob.core.windows.net/parse/parse/AmazonReviewStarJob/1910599147004108800/7c5f4199-0512-48e6-993e-7301ccd4d356.json
    æå–: parse/AmazonReviewStarJob/1910599147004108800/7c5f4199-0512-48e6-993e-7301ccd4d356.json
    
    Args:
        url: å®Œæ•´çš„ä¸‹è½½URL
        
    Returns:
        Optional[str]: æå–çš„Blobè·¯å¾„ï¼Œå¦‚æœå¤±è´¥è¿”å›None
    """
    try:
        from urllib.parse import urlparse, unquote
        parsed_url = urlparse(url)
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯collector0109.blob.core.windows.netåŸŸå
        if 'collector0109.blob.core.windows.net' not in parsed_url.netloc:
            return None
            
        # è·å–è·¯å¾„éƒ¨åˆ†ï¼Œå»æ‰å‰å¯¼çš„/
        path = unquote(parsed_url.path)
        if path.startswith('/'):
            path = path[1:]
        
        # æ£€æŸ¥è·¯å¾„æ˜¯å¦ä»¥.jsonç»“å°¾ (ç¡®ä¿æ˜¯JSONæ–‡ä»¶)
        if not path.endswith('.json'):
            return None
            
        # ç§»é™¤å®¹å™¨å (parse/) ä¿ç•™å®é™…çš„Blobè·¯å¾„
        path_parts = path.split('/', 1)
        if len(path_parts) >= 2 and path_parts[0] == 'parse':
            blob_path = path_parts[1]  # å»æ‰å®¹å™¨åï¼Œä¿ç•™å®é™…è·¯å¾„
            logger.info(f"ğŸ” æˆåŠŸæå–Blobè·¯å¾„: {blob_path}")
            return blob_path
        else:
            logger.warning(f"âš ï¸  æ— æ³•ä»è·¯å¾„ä¸­æå–æœ‰æ•ˆçš„Blobè·¯å¾„: {path}")
            return None
            
    except Exception as e:
        logger.error(f"âŒ æå–Blobè·¯å¾„å¤±è´¥: {str(e)}")
        return None


def try_azure_storage_with_specific_path(reader, blob_path: str, task_type: str, task_id: str, 
                                        save_dir: str, decompress: bool = True) -> Dict[str, any]:
    """
    ğŸ†• ä½¿ç”¨æŒ‡å®šçš„Blobè·¯å¾„ä»Azureå­˜å‚¨ä¸‹è½½æ–‡ä»¶
    
    Args:
        reader: Azureè¯»å–å™¨å®ä¾‹ 
        blob_path: å…·ä½“çš„Blobè·¯å¾„ (å¦‚: parse/AmazonReviewStarJob/1910599147004108800/xxx.json)
        task_type: ä»»åŠ¡ç±»å‹
        task_id: ä»»åŠ¡ID
        save_dir: ä¿å­˜ç›®å½•
        decompress: æ˜¯å¦è§£å‹ç¼©
        
    Returns:
        Dict: ä¸‹è½½ç»“æœ
    """
    logger.info(f"ğŸ¯ å°è¯•ä½¿ç”¨å…·ä½“è·¯å¾„ä»Azureå­˜å‚¨ä¸‹è½½: {blob_path}")
    
    try:
        # ç›´æ¥è¯»å–æŒ‡å®šè·¯å¾„çš„æ–‡ä»¶
        container_name = reader.storage_config['container_name']  # åº”è¯¥æ˜¯ 'parse'
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        content = reader.read_blob_content(container_name, blob_path, decompress=decompress)
        
        if content is None:
            logger.warning(f"âš ï¸  æŒ‡å®šè·¯å¾„æ–‡ä»¶ä¸å­˜åœ¨æˆ–è¯»å–å¤±è´¥: {blob_path}")
            return {
                'success': False,
                'error': f'æŒ‡å®šè·¯å¾„æ–‡ä»¶ä¸å­˜åœ¨: {blob_path}',
                'files_downloaded': []
            }
        
        # ğŸ†• ä¿å­˜åˆ°ç»Ÿä¸€ç›®å½•ç»“æ„: data/output/{task_type}/{task_id}/
        save_path = Path(save_dir) / task_type / task_id
        save_path.mkdir(parents=True, exist_ok=True)
        
        # ğŸ†• ç»Ÿä¸€ä½¿ç”¨ parse_result.json ä½œä¸ºæ–‡ä»¶åï¼ˆè€Œä¸æ˜¯åŸå§‹çš„é•¿æ–‡ä»¶åï¼‰
        saved_filename = 'parse_result.json'
        file_path = save_path / saved_filename
        
        if isinstance(content, str):
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)
        else:
            with open(file_path, 'wb') as f:
                f.write(content)
        
        logger.info(f"âœ… ä»Azureå­˜å‚¨ç²¾ç¡®ä¸‹è½½æˆåŠŸ: {file_path}")
        
        # æå–åŸå§‹æ–‡ä»¶åç”¨äºè®°å½•
        original_filename = blob_path.split('/')[-1]
        
        return {
            'success': True,
            'files_downloaded': [{
                'original_name': original_filename,
                'saved_name': saved_filename,  # ğŸ†• ç»Ÿä¸€çš„æ–‡ä»¶å
                'local_path': str(file_path),
                'size': len(content),
                'content_length': len(content),
                'blob_path': blob_path
            }],
            'save_path': str(save_path),
            'method_used': 'azure_storage_specific_path'
        }
        
    except Exception as e:
        logger.error(f"âŒ Azureå­˜å‚¨ç²¾ç¡®ä¸‹è½½å¤±è´¥: {str(e)}")
        return {
            'success': False,
            'error': f'Azureå­˜å‚¨ç²¾ç¡®ä¸‹è½½å¤±è´¥: {str(e)}',
            'files_downloaded': []
        }


def fetch_and_save_parse_files_optimized(reader, task_type: str, task_id: str, 
                                         save_dir: str = 'data/output', 
                                         decompress: bool = True,
                                         job_id: str = None,
                                         analysis_response: str = None) -> Dict[str, any]:
    """
    ğŸ†• ä¼˜åŒ–ç‰ˆè§£ææ–‡ä»¶è·å–æ–¹æ³•
    ä¼˜å…ˆå°è¯•ä»analysis_responseä¸­çš„é“¾æ¥ä¸‹è½½ï¼Œå¤±è´¥æ—¶æ™ºèƒ½å›é€€åˆ°Azureå­˜å‚¨
    
    Args:
        reader: AzureResourceReaderå®ä¾‹
        task_type: ä»»åŠ¡ç±»å‹ï¼ˆå¦‚: AmazonReviewStarJobï¼‰
        task_id: ä»»åŠ¡IDï¼ˆå¦‚: 1887037115222994944ï¼‰
        save_dir: ä¿å­˜ç›®å½•ï¼Œé»˜è®¤: data/output
        decompress: æ˜¯å¦è‡ªåŠ¨è§£å‹ç¼©ï¼Œé»˜è®¤ä¸ºTrue
        job_id: è¯·æ±‚åºåˆ—å·ï¼ˆç”¨äºæ•°æ®åº“æŸ¥è¯¢ï¼‰
        analysis_response: analysis_response JSONå­—ç¬¦ä¸²
        
    Returns:
        Dict: åŒ…å«æ“ä½œç»“æœçš„å­—å…¸
    """
    logger.info(f"ğŸ” å¼€å§‹ä¼˜åŒ–ç‰ˆè§£ææ–‡ä»¶è·å–æµç¨‹")
    logger.info(f"ğŸ“‹ ä»»åŠ¡ç±»å‹: {task_type}")
    logger.info(f"ğŸ“‹ ä»»åŠ¡ID: {task_id}")
    
    # ç¬¬ä¸€æ­¥ï¼šå°è¯•ä»analysis_responseè·å–ä¸‹è½½é“¾æ¥
    download_success = False
    downloaded_files = []
    extracted_blob_path = None
    method_used = None
    
    if analysis_response:
        logger.info(f"ğŸ”— å°è¯•ä» analysis_response è·å–ä¸‹è½½é“¾æ¥")
        download_result = try_download_from_analysis_response(
            task_type, task_id, analysis_response, save_dir, decompress
        )
        
        # ä¿å­˜æå–çš„è·¯å¾„ä¿¡æ¯ï¼Œæ— è®ºæ˜¯å¦ä¸‹è½½æˆåŠŸ
        extracted_blob_path = download_result.get('extracted_blob_path')
        
        if download_result['success']:
            logger.info(f"âœ… ä» analysis_response é“¾æ¥ä¸‹è½½æˆåŠŸ")
            download_success = True
            downloaded_files = download_result['files_downloaded']
            method_used = 'analysis_response'
        else:
            logger.warning(f"âš ï¸  ä» analysis_response é“¾æ¥ä¸‹è½½å¤±è´¥: {download_result.get('error')}")
    else:
        logger.info(f"ğŸ“ æœªæä¾› analysis_response æˆ–ä»»åŠ¡ç±»å‹æœªå¯ç”¨")
    
    # ç¬¬äºŒæ­¥ï¼šå¦‚æœç¬¬ä¸€æ­¥å¤±è´¥ï¼Œå°è¯•æ™ºèƒ½å›é€€
    if not download_success:
        logger.info(f"ğŸ”„ å›é€€åˆ° Azure å­˜å‚¨è·å–æ–¹æ³•")
        
        # ğŸ†• å¦‚æœæœ‰æå–çš„Blobè·¯å¾„ï¼Œä¼˜å…ˆä½¿ç”¨ç²¾ç¡®è·¯å¾„ä¸‹è½½
        if extracted_blob_path:
            logger.info(f"ğŸ¯ ä½¿ç”¨ä»é“¾æ¥ä¸­æå–çš„ç²¾ç¡®è·¯å¾„è¿›è¡Œä¸‹è½½")
            
            # ç¡®ä¿ä½¿ç”¨collector0109è¯»å–å™¨
            if reader.account_name != 'collector0109':
                from src.azure_resource_reader import AzureResourceReader
                azure_reader = AzureResourceReader('collector0109')
            else:
                azure_reader = reader
            
            # å°è¯•ç²¾ç¡®è·¯å¾„ä¸‹è½½
            precise_result = try_azure_storage_with_specific_path(
                azure_reader, extracted_blob_path, task_type, task_id, save_dir, decompress
            )
            
            if precise_result['success']:
                logger.info(f"âœ… ä½¿ç”¨ç²¾ç¡®è·¯å¾„ä»Azureå­˜å‚¨ä¸‹è½½æˆåŠŸ")
                download_success = True
                downloaded_files = precise_result['files_downloaded']
                method_used = 'azure_storage_specific_path'
            else:
                logger.warning(f"âš ï¸  ç²¾ç¡®è·¯å¾„ä¸‹è½½ä¹Ÿå¤±è´¥: {precise_result.get('error')}")
        
        # å¦‚æœç²¾ç¡®è·¯å¾„ä¸‹è½½å¤±è´¥æˆ–æ²¡æœ‰æå–åˆ°è·¯å¾„ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•
        if not download_success:
            logger.info(f"ğŸ”„ ä½¿ç”¨ä¼ ç»ŸAzureå­˜å‚¨æœç´¢æ–¹æ³•")
            
            # ç¡®ä¿ä½¿ç”¨collector0109è¯»å–å™¨
            if reader.account_name != 'collector0109':
                from src.azure_resource_reader import AzureResourceReader
                azure_reader = AzureResourceReader('collector0109')
            else:
                azure_reader = reader
            
            # ğŸ†• ä½¿ç”¨ä¿®æ”¹åçš„æ–¹æ³•ï¼Œç¡®ä¿ä¿å­˜åˆ°æ­£ç¡®ç›®å½•
            azure_result = fetch_parse_files_to_unified_directory(
                azure_reader, task_type, task_id, save_dir, decompress
            )
            
            if azure_result['success']:
                logger.info(f"âœ… ä¼ ç»ŸAzureå­˜å‚¨æ–¹æ³•ä¸‹è½½æˆåŠŸ")
                download_success = True
                downloaded_files = azure_result['files_downloaded']
                method_used = 'azure_storage_search'
            else:
                logger.error(f"âŒ ä¼ ç»ŸAzureå­˜å‚¨æ–¹æ³•ä¹Ÿå¤±è´¥: {azure_result.get('error')}")
    
    # è¿”å›ç»“æœ
    if download_success:
        save_path = f"{save_dir}/{task_type}/{task_id}" if downloaded_files else None
        return {
            'success': True,
            'files_downloaded': downloaded_files,
            'save_path': save_path,
            'total_files_downloaded': len(downloaded_files),
            'method_used': method_used
        }
    else:
        return {
            'success': False,
            'error': 'æ‰€æœ‰è·å–æ–¹æ³•éƒ½å¤±è´¥',
            'files_downloaded': [],
            'save_path': None
        }


def fetch_parse_files_to_unified_directory(reader, task_type: str, task_id: str, 
                                          save_dir: str = 'data/output', 
                                          decompress: bool = True) -> Dict[str, any]:
    """
    ğŸ†• ä»Azureå­˜å‚¨è·å–è§£ææ–‡ä»¶å¹¶ä¿å­˜åˆ°ç»Ÿä¸€ç›®å½•ç»“æ„
    è¿™æ˜¯å¯¹åŸå§‹ fetch_and_save_parse_files çš„åŒ…è£…ï¼Œç¡®ä¿ä¿å­˜åˆ°æ­£ç¡®ä½ç½®
    
    Args:
        reader: AzureResourceReaderå®ä¾‹ (collector0109)
        task_type: ä»»åŠ¡ç±»å‹
        task_id: ä»»åŠ¡ID
        save_dir: ä¿å­˜ç›®å½•
        decompress: æ˜¯å¦è§£å‹ç¼©
        
    Returns:
        Dict: æ“ä½œç»“æœ
    """
    logger.info(f"ğŸ“ ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•æœç´¢è§£ææ–‡ä»¶")
    
    try:
        # ä½¿ç”¨åŸå§‹æ–¹æ³•è·å–æ–‡ä»¶ï¼ˆä½†ä¼šä¿å­˜åˆ°parseå­ç›®å½•ï¼‰
        original_result = reader.fetch_and_save_parse_files(
            task_type, task_id, save_dir, decompress
        )
        
        if not original_result['success']:
            return original_result
        
        # ğŸ†• å°†æ–‡ä»¶ä»parseå­ç›®å½•ç§»åŠ¨åˆ°ç»Ÿä¸€ç›®å½•ï¼Œå¹¶ç»Ÿä¸€å‘½åä¸ºparse_result.json
        source_dir = Path(save_dir) / 'parse' / task_type / task_id
        target_dir = Path(save_dir) / task_type / task_id
        
        if source_dir.exists():
            target_dir.mkdir(parents=True, exist_ok=True)
            
            moved_files = []
            for i, file_info in enumerate(original_result['files_downloaded']):
                source_file = Path(file_info['local_path'])
                
                # ğŸ†• ç»Ÿä¸€ä½¿ç”¨ parse_result.json ä½œä¸ºæ–‡ä»¶å
                saved_filename = 'parse_result.json'
                target_file = target_dir / saved_filename
                
                # ç§»åŠ¨å¹¶é‡å‘½åæ–‡ä»¶
                import shutil
                shutil.move(str(source_file), str(target_file))
                
                # æ›´æ–°æ–‡ä»¶ä¿¡æ¯
                updated_file_info = file_info.copy()
                updated_file_info['saved_name'] = saved_filename  # ğŸ†• ç»Ÿä¸€çš„æ–‡ä»¶å
                updated_file_info['local_path'] = str(target_file)
                moved_files.append(updated_file_info)
                
                logger.info(f"ğŸ“¦ æ–‡ä»¶å·²ç§»åŠ¨åˆ°ç»Ÿä¸€ç›®å½•å¹¶é‡å‘½å: {target_file}")
            
            # æ¸…ç†ç©ºçš„parseç›®å½•
            try:
                source_dir.rmdir()
                source_dir.parent.rmdir()  # task_typeç›®å½•
                (source_dir.parent.parent).rmdir()  # parseç›®å½•
            except OSError:
                pass  # ç›®å½•ä¸ä¸ºç©ºï¼Œå¿½ç•¥
            
            return {
                'success': True,
                'files_downloaded': moved_files,
                'save_path': str(target_dir),
                'total_files_found': original_result.get('total_files_found', 0),
                'total_files_downloaded': len(moved_files)
            }
        else:
            logger.warning(f"âš ï¸  åŸå§‹æ–¹æ³•æœªåˆ›å»ºé¢„æœŸçš„ç›®å½•ç»“æ„")
            return original_result
        
    except Exception as e:
        logger.error(f"âŒ ç»Ÿä¸€ç›®å½•å¤„ç†å¤±è´¥: {str(e)}")
        return {
            'success': False,
            'error': f'ç»Ÿä¸€ç›®å½•å¤„ç†å¤±è´¥: {str(e)}',
            'files_downloaded': []
        } 