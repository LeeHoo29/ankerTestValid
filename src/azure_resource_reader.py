#!/usr/bin/env python3
"""
Azure Storage èµ„æºè¯»å–å™¨
ä¸“é—¨ç”¨äºè¯»å–çº¿ä¸ŠAzure Storageä¸­çš„èµ„æºæ–‡ä»¶
"""
import os
import sys
import gzip
import logging
import json
import argparse
import re
from pathlib import Path
from typing import Dict, List, Optional, Union, BinaryIO
from datetime import datetime
import io

# Azure Storage SDK imports
from azure.identity import ClientSecretCredential, DefaultAzureCredential
from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient
from azure.core.exceptions import AzureError, ResourceNotFoundError

# å¯¼å…¥é¡¹ç›®é…ç½®
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))
from config.azure_storage_config import (
    AZURE_STORAGE_CONFIG,
    YIYA0110_STORAGE_CONFIG,
    COLLECTOR0109_STORAGE_CONFIG,
    set_azure_environment_variables,
    get_storage_account_url
)

# å¯¼å…¥æ•°æ®åº“è¿æ¥å™¨
from src.db.connector import DatabaseConnector
from config.db_config import DB_CONFIG

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class AzureResourceReader:
    """Azure Storage èµ„æºè¯»å–å™¨"""
    
    def __init__(self, account_name: str = 'yiya0110', use_default_credential: bool = False):
        """
        åˆå§‹åŒ–Azureèµ„æºè¯»å–å™¨
        
        Args:
            account_name: Azureå­˜å‚¨è´¦æˆ·åï¼Œæ”¯æŒ 'yiya0110' (åŸå§‹æ•°æ®) æˆ– 'collector0109' (è§£ææ•°æ®)
            use_default_credential: æ˜¯å¦ä½¿ç”¨é»˜è®¤å‡­æ®ï¼ŒFalseåˆ™ä½¿ç”¨æœåŠ¡ä¸»ä½“è®¤è¯
        """
        self.account_name = account_name
        self.account_url = get_storage_account_url(account_name, 'blob')
        
        # æ ¹æ®è´¦æˆ·åè®¾ç½®å­˜å‚¨é…ç½®
        if account_name == 'yiya0110':
            self.storage_config = YIYA0110_STORAGE_CONFIG
        elif account_name == 'collector0109':
            self.storage_config = COLLECTOR0109_STORAGE_CONFIG
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å­˜å‚¨è´¦æˆ·: {account_name}ã€‚æ”¯æŒçš„è´¦æˆ·: 'yiya0110', 'collector0109'")
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        set_azure_environment_variables()
        
        # åˆ›å»ºè®¤è¯å‡­æ®
        if use_default_credential:
            self.credential = DefaultAzureCredential()
        else:
            self.credential = ClientSecretCredential(
                tenant_id=AZURE_STORAGE_CONFIG['tenant_id'],
                client_id=AZURE_STORAGE_CONFIG['client_id'],
                client_secret=AZURE_STORAGE_CONFIG['client_secret']
            )
        
        # åˆå§‹åŒ–BlobæœåŠ¡å®¢æˆ·ç«¯
        self.blob_service_client = BlobServiceClient(
            account_url=self.account_url,
            credential=self.credential
        )
        
        logger.info(f"Azureèµ„æºè¯»å–å™¨åˆå§‹åŒ–æˆåŠŸ: {account_name} ({self.storage_config['container_name']})")
    
    def read_amazon_listing_job_file(self, job_id: str, filename: str = 'login.gz', 
                                    decompress: bool = True) -> Union[str, bytes, None]:
        """
        è¯»å–Amazon Listing Jobæ–‡ä»¶
        
        Args:
            job_id: ä»»åŠ¡IDï¼Œå¦‚ '1925464883027513344'
            filename: æ–‡ä»¶åï¼Œé»˜è®¤ä¸º 'login.gz'ï¼Œä¹Ÿæ”¯æŒ 'normal.gz'
            decompress: æ˜¯å¦è‡ªåŠ¨è§£å‹ç¼©ï¼Œé»˜è®¤ä¸ºTrue
            
        Returns:
            Union[str, bytes, None]: æ–‡ä»¶å†…å®¹ï¼Œå¦‚æœè§£å‹ç¼©åˆ™è¿”å›strï¼Œå¦åˆ™è¿”å›bytes
        """
        return self.read_task_file('AmazonListingJob', job_id, filename, decompress)
    
    def read_task_file(self, task_type: str, job_id: str, filename: str, 
                      decompress: bool = True) -> Union[str, bytes, None]:
        """
        è¯»å–ä»»åŠ¡æ–‡ä»¶ï¼ˆé€šç”¨æ–¹æ³•ï¼‰
        
        Args:
            task_type: ä»»åŠ¡ç±»å‹ï¼Œå¦‚ 'AmazonListingJob'
            job_id: ä»»åŠ¡IDï¼Œå¦‚ '1925464883027513344'
            filename: æ–‡ä»¶åï¼Œå¦‚ 'login.gz' æˆ– 'normal.gz'
            decompress: æ˜¯å¦è‡ªåŠ¨è§£å‹ç¼©ï¼Œé»˜è®¤ä¸ºTrue
            
        Returns:
            Union[str, bytes, None]: æ–‡ä»¶å†…å®¹
        """
        container_name = self.storage_config['container_name']
        blob_path = f"{self.storage_config['blob_base_path']}/{task_type}/{job_id}/{filename}"
        
        return self.read_blob_content(container_name, blob_path, decompress=decompress)
    
    def read_amazon_listing_job_both_files(self, job_id: str, 
                                         decompress: bool = True) -> Dict[str, Union[str, bytes, None]]:
        """
        è¯»å–Amazon Listing Jobçš„ä¸¤ä¸ªé»˜è®¤æ–‡ä»¶ï¼šlogin.gz å’Œ normal.gz
        
        Args:
            job_id: ä»»åŠ¡IDï¼Œå¦‚ '1925464883027513344'
            decompress: æ˜¯å¦è‡ªåŠ¨è§£å‹ç¼©ï¼Œé»˜è®¤ä¸ºTrue
            
        Returns:
            Dict[str, Union[str, bytes, None]]: åŒ…å«ä¸¤ä¸ªæ–‡ä»¶å†…å®¹çš„å­—å…¸
        """
        result = {}
        
        # è¯»å–login.gz
        logger.info(f"æ­£åœ¨è¯»å– login.gz æ–‡ä»¶...")
        result['login'] = self.read_amazon_listing_job_file(job_id, 'login.gz', decompress)
        
        # è¯»å–normal.gz
        logger.info(f"æ­£åœ¨è¯»å– normal.gz æ–‡ä»¶...")
        result['normal'] = self.read_amazon_listing_job_file(job_id, 'normal.gz', decompress)
        
        return result
    
    def read_blob_content(self, container_name: str, blob_path: str, 
                         decompress: bool = True) -> Union[str, bytes, None]:
        """
        è¯»å–Blobå†…å®¹
        
        Args:
            container_name: å®¹å™¨åç§°
            blob_path: Blobè·¯å¾„
            decompress: æ˜¯å¦è‡ªåŠ¨è§£å‹ç¼©.gzæ–‡ä»¶ï¼Œé»˜è®¤ä¸ºTrue
            
        Returns:
            Union[str, bytes, None]: æ–‡ä»¶å†…å®¹
        """
        try:
            blob_client = self.blob_service_client.get_blob_client(
                container=container_name,
                blob=blob_path
            )
            
            logger.info(f"æ­£åœ¨è¯»å– Blob: {container_name}/{blob_path}")
            
            # ä¸‹è½½Blobæ•°æ®
            blob_data = blob_client.download_blob().readall()
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦è§£å‹ç¼©
            if decompress and blob_path.endswith('.gz'):
                logger.info("æ£€æµ‹åˆ°.gzæ–‡ä»¶ï¼Œæ­£åœ¨è§£å‹ç¼©...")
                try:
                    # ä½¿ç”¨gzipè§£å‹ç¼©
                    decompressed_data = gzip.decompress(blob_data)
                    # å°è¯•è§£ç ä¸ºUTF-8å­—ç¬¦ä¸²
                    try:
                        content = decompressed_data.decode('utf-8')
                        logger.info(f"âœ… æˆåŠŸè¯»å–å¹¶è§£å‹ç¼©æ–‡ä»¶ï¼Œå†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
                        return content
                    except UnicodeDecodeError:
                        logger.warning("æ— æ³•è§£ç ä¸ºUTF-8ï¼Œè¿”å›åŸå§‹å­—èŠ‚æ•°æ®")
                        logger.info(f"âœ… æˆåŠŸè¯»å–å¹¶è§£å‹ç¼©æ–‡ä»¶ï¼Œæ•°æ®é•¿åº¦: {len(decompressed_data)} å­—èŠ‚")
                        return decompressed_data
                        
                except gzip.BadGzipFile:
                    logger.warning("æ–‡ä»¶ä¸æ˜¯æœ‰æ•ˆçš„gzipæ ¼å¼ï¼Œè¿”å›åŸå§‹æ•°æ®")
                    logger.info(f"âœ… æˆåŠŸè¯»å–æ–‡ä»¶ï¼Œæ•°æ®é•¿åº¦: {len(blob_data)} å­—èŠ‚")
                    return blob_data
            else:
                logger.info(f"âœ… æˆåŠŸè¯»å–æ–‡ä»¶ï¼Œæ•°æ®é•¿åº¦: {len(blob_data)} å­—èŠ‚")
                return blob_data
                
        except ResourceNotFoundError:
            logger.error(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {container_name}/{blob_path}")
            return None
        except Exception as e:
            logger.error(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}")
            return None
    
    def save_blob_to_file(self, container_name: str, blob_path: str, 
                         local_file_path: str, decompress: bool = True) -> bool:
        """
        ä¸‹è½½Blobå¹¶ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶
        
        Args:
            container_name: å®¹å™¨åç§°
            blob_path: Blobè·¯å¾„
            local_file_path: æœ¬åœ°æ–‡ä»¶ä¿å­˜è·¯å¾„
            decompress: æ˜¯å¦è‡ªåŠ¨è§£å‹ç¼©.gzæ–‡ä»¶ï¼Œé»˜è®¤ä¸ºTrue
            
        Returns:
            bool: ä¸‹è½½æˆåŠŸè¿”å›True
        """
        content = self.read_blob_content(container_name, blob_path, decompress=decompress)
        
        if content is None:
            return False
        
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            local_path = Path(local_file_path)
            local_path.parent.mkdir(parents=True, exist_ok=True)
            
            # å†™å…¥æ–‡ä»¶
            if isinstance(content, str):
                with open(local_file_path, 'w', encoding='utf-8') as f:
                    f.write(content)
            else:
                with open(local_file_path, 'wb') as f:
                    f.write(content)
            
            logger.info(f"âœ… æ–‡ä»¶ä¿å­˜æˆåŠŸ: {local_file_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {str(e)}")
            return False
    
    def list_amazon_listing_jobs(self, limit: int = 100) -> List[Dict]:
        """
        åˆ—å‡ºAmazon Listing Jobç›®å½•
        
        Args:
            limit: é™åˆ¶è¿”å›çš„æ•°é‡ï¼Œé»˜è®¤100
            
        Returns:
            List[Dict]: ä»»åŠ¡ä¿¡æ¯åˆ—è¡¨
        """
        container_name = self.storage_config['container_name']
        prefix = f"{self.storage_config['blob_base_path']}/AmazonListingJob/"
        
        return self.list_blobs_with_prefix(container_name, prefix, limit)
    
    def list_task_jobs(self, task_type: str, limit: int = 100) -> List[Dict]:
        """
        åˆ—å‡ºæŒ‡å®šä»»åŠ¡ç±»å‹çš„æ‰€æœ‰ä»»åŠ¡
        
        Args:
            task_type: ä»»åŠ¡ç±»å‹ï¼Œå¦‚ 'AmazonListingJob'
            limit: é™åˆ¶è¿”å›çš„æ•°é‡ï¼Œé»˜è®¤100
            
        Returns:
            List[Dict]: ä»»åŠ¡ä¿¡æ¯åˆ—è¡¨
        """
        container_name = self.storage_config['container_name']
        prefix = f"{self.storage_config['blob_base_path']}/{task_type}/"
        
        return self.list_blobs_with_prefix(container_name, prefix, limit)
    
    def list_blobs_with_prefix(self, container_name: str, prefix: str, 
                              limit: int = 100) -> List[Dict]:
        """
        åˆ—å‡ºæŒ‡å®šå‰ç¼€çš„Blob
        
        Args:
            container_name: å®¹å™¨åç§°
            prefix: Blobè·¯å¾„å‰ç¼€
            limit: é™åˆ¶è¿”å›çš„æ•°é‡
            
        Returns:
            List[Dict]: Blobä¿¡æ¯åˆ—è¡¨
        """
        try:
            container_client = self.blob_service_client.get_container_client(container_name)
            blobs = []
            
            count = 0
            for blob in container_client.list_blobs(name_starts_with=prefix):
                if count >= limit:
                    break
                    
                blobs.append({
                    'name': blob.name,
                    'size': blob.size,
                    'last_modified': blob.last_modified,
                    'content_type': blob.content_settings.content_type if blob.content_settings else None,
                    'path_parts': blob.name.split('/'),
                    'job_id': self._extract_job_id_from_path(blob.name)
                })
                count += 1
            
            logger.info(f"åˆ—å‡º {len(blobs)} ä¸ªBlob (å‰ç¼€: {prefix})")
            return blobs
            
        except Exception as e:
            logger.error(f"åˆ—å‡ºBlobå¤±è´¥: {str(e)}")
            return []
    
    def _extract_job_id_from_path(self, blob_path: str) -> Optional[str]:
        """
        ä»Blobè·¯å¾„ä¸­æå–Job ID
        
        Args:
            blob_path: Blobè·¯å¾„ï¼Œå¦‚ "compress/AmazonListingJob/1925464883027513344/login.gz"
            
        Returns:
            Optional[str]: Job ID
        """
        try:
            parts = blob_path.split('/')
            # æ–°è·¯å¾„ç»“æ„: compress/TaskType/JobID/filename
            if len(parts) >= 4 and parts[0] == 'compress':
                return parts[2]  # JobIDåœ¨ç¬¬3ä¸ªä½ç½®ï¼ˆç´¢å¼•2ï¼‰
        except:
            pass
        return None
    
    def get_blob_info(self, container_name: str, blob_path: str) -> Optional[Dict]:
        """
        è·å–Blobè¯¦ç»†ä¿¡æ¯
        
        Args:
            container_name: å®¹å™¨åç§°
            blob_path: Blobè·¯å¾„
            
        Returns:
            Optional[Dict]: Blobä¿¡æ¯å­—å…¸
        """
        try:
            blob_client = self.blob_service_client.get_blob_client(
                container=container_name,
                blob=blob_path
            )
            
            properties = blob_client.get_blob_properties()
            
            return {
                'name': blob_path,
                'size': properties.size,
                'size_mb': round(properties.size / (1024 * 1024), 2),
                'content_type': properties.content_settings.content_type,
                'last_modified': properties.last_modified,
                'etag': properties.etag,
                'metadata': properties.metadata or {},
                'creation_time': properties.creation_time,
                'blob_type': properties.blob_type,
                'url': f"{self.account_url}/{container_name}/{blob_path}"
            }
            
        except Exception as e:
            logger.error(f"è·å–Blobä¿¡æ¯å¤±è´¥: {str(e)}")
            return None
    
    def read_parse_file(self, task_type: str, task_id: str, filename: str = None, 
                       decompress: bool = True) -> Union[str, bytes, None]:
        """
        è¯»å–è§£ææ–‡ä»¶ï¼ˆcollector0109è´¦æˆ·ï¼‰
        
        Args:
            task_type: ä»»åŠ¡ç±»å‹ï¼ˆå¦‚: AmazonListingJobï¼‰
            task_id: ä»»åŠ¡IDï¼ˆå¦‚: 1910599147004108800ï¼‰
            filename: æ–‡ä»¶åï¼Œå¦‚æœä¸æŒ‡å®šåˆ™å°è¯•æŸ¥æ‰¾JSONæ–‡ä»¶
            decompress: æ˜¯å¦è‡ªåŠ¨è§£å‹ç¼©ï¼Œé»˜è®¤ä¸ºTrue
            
        Returns:
            Union[str, bytes, None]: æ–‡ä»¶å†…å®¹
        """
        if self.account_name != 'collector0109':
            logger.error("è¯»å–è§£ææ–‡ä»¶éœ€è¦ä½¿ç”¨collector0109å­˜å‚¨è´¦æˆ·")
            return None
        
        container_name = self.storage_config['container_name']
        
        # è·¯å¾„ç»“æ„ï¼šparse/{task_type}/{task_id}/*
        blob_prefix = f"{self.storage_config['blob_base_path']}/{task_type}/{task_id}/"
        
        if filename:
            # æŒ‡å®šæ–‡ä»¶å
            blob_path = f"{blob_prefix}{filename}"
            return self.read_blob_content(container_name, blob_path, decompress=decompress)
        else:
            # è‡ªåŠ¨æŸ¥æ‰¾JSONæ–‡ä»¶
            return self._auto_find_parse_file(container_name, blob_prefix, decompress)
    
    def _auto_find_parse_file(self, container_name: str, blob_prefix: str, 
                             decompress: bool = True) -> Union[str, bytes, None]:
        """
        è‡ªåŠ¨æŸ¥æ‰¾è§£ææ–‡ä»¶ï¼ˆä¼˜å…ˆJSONæ–‡ä»¶ï¼‰
        
        Args:
            container_name: å®¹å™¨å
            blob_prefix: Blobè·¯å¾„å‰ç¼€
            decompress: æ˜¯å¦è§£å‹ç¼©
            
        Returns:
            Union[str, bytes, None]: æ–‡ä»¶å†…å®¹
        """
        try:
            container_client = self.blob_service_client.get_container_client(container_name)
            
            # æŸ¥æ‰¾æ‰€æœ‰æ–‡ä»¶
            blobs = list(container_client.list_blobs(name_starts_with=blob_prefix))
            
            if not blobs:
                logger.warning(f"æœªæ‰¾åˆ°ä»»ä½•æ–‡ä»¶ï¼Œè·¯å¾„å‰ç¼€: {blob_prefix}")
                return None
            
            # ä¼˜å…ˆçº§ï¼š.json > .json.gz > å…¶ä»–
            json_files = [b for b in blobs if b.name.endswith('.json')]
            json_gz_files = [b for b in blobs if b.name.endswith('.json.gz')]
            other_files = [b for b in blobs if not (b.name.endswith('.json') or b.name.endswith('.json.gz'))]
            
            target_blob = None
            if json_files:
                target_blob = json_files[0]
                logger.info(f"æ‰¾åˆ°JSONæ–‡ä»¶: {target_blob.name}")
            elif json_gz_files:
                target_blob = json_gz_files[0]
                logger.info(f"æ‰¾åˆ°å‹ç¼©JSONæ–‡ä»¶: {target_blob.name}")
            elif other_files:
                target_blob = other_files[0]
                logger.info(f"æ‰¾åˆ°å…¶ä»–æ–‡ä»¶: {target_blob.name}")
            
            if target_blob:
                return self.read_blob_content(container_name, target_blob.name, decompress=decompress)
            else:
                logger.warning("æœªæ‰¾åˆ°åˆé€‚çš„è§£ææ–‡ä»¶")
                return None
                
        except Exception as e:
            logger.error(f"è‡ªåŠ¨æŸ¥æ‰¾è§£ææ–‡ä»¶å¤±è´¥: {str(e)}")
            return None
    
    def list_parse_files(self, task_type: str, task_id: str) -> List[Dict]:
        """
        åˆ—å‡ºè§£ææ–‡ä»¶ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
        
        Args:
            task_type: ä»»åŠ¡ç±»å‹ï¼ˆå¦‚: AmazonListingJobï¼‰
            task_id: ä»»åŠ¡ID
            
        Returns:
            List[Dict]: æ–‡ä»¶ä¿¡æ¯åˆ—è¡¨
        """
        if self.account_name != 'collector0109':
            logger.error("åˆ—å‡ºè§£ææ–‡ä»¶éœ€è¦ä½¿ç”¨collector0109å­˜å‚¨è´¦æˆ·")
            return []
        
        container_name = self.storage_config['container_name']
        prefix = f"{self.storage_config['blob_base_path']}/{task_type}/{task_id}/"
        
        return self.list_blobs_with_prefix(container_name, prefix, limit=100)

    def read_task_file_with_parse(self, task_type: str, task_id: str, filename: str, 
                                 decompress: bool = True) -> Dict[str, Union[str, bytes, None]]:
        """
        åŒæ—¶è¯»å–åŸå§‹ä»»åŠ¡æ–‡ä»¶å’Œè§£ææ–‡ä»¶
        
        Args:
            task_type: ä»»åŠ¡ç±»å‹ï¼Œå¦‚ 'AmazonListingJob'
            task_id: ä»»åŠ¡IDï¼Œå¦‚ '1925464883027513344'
            filename: åŸå§‹æ–‡ä»¶åï¼Œå¦‚ 'login.gz' æˆ– 'normal.gz'
            decompress: æ˜¯å¦è‡ªåŠ¨è§£å‹ç¼©ï¼Œé»˜è®¤ä¸ºTrue
            
        Returns:
            Dict[str, Union[str, bytes, None]]: åŒ…å«åŸå§‹æ–‡ä»¶å’Œè§£ææ–‡ä»¶å†…å®¹çš„å­—å…¸
        """
        result = {}
        
        # è¯»å–åŸå§‹æ–‡ä»¶ï¼ˆyiya0110ï¼‰
        if self.account_name == 'yiya0110':
            logger.info(f"æ­£åœ¨è¯»å–åŸå§‹æ–‡ä»¶: {filename}")
            result['original'] = self.read_task_file(task_type, task_id, filename, decompress)
            
            # åˆ›å»ºcollector0109è¯»å–å™¨æ¥è¯»å–è§£ææ–‡ä»¶
            parse_reader = AzureResourceReader('collector0109')
            logger.info(f"æ­£åœ¨è¯»å–è§£ææ–‡ä»¶...")
            result['parse'] = parse_reader.read_parse_file(task_type, task_id, None, decompress)
            
        else:
            logger.warning("æ­¤æ–¹æ³•éœ€è¦ä½¿ç”¨yiya0110è´¦æˆ·ä½œä¸ºä¸»è¯»å–å™¨")
            result['original'] = None
            result['parse'] = None
            
        return result

    def fetch_and_save_parse_files(self, task_type: str, task_id: str, 
                                  save_dir: str = 'data/output', 
                                  decompress: bool = True) -> Dict[str, any]:
        """
        ä»collector0109è·å–è§£ææ–‡ä»¶å¹¶ä¿å­˜åˆ°æŒ‡å®šæ–‡ä»¶å¤¹
        æ ¹æ®æµ‹è¯•ç»“æœä¼˜åŒ–ï¼Œä»…ä½¿ç”¨æ­£ç¡®çš„è·¯å¾„ï¼šparse/{task_type}/{task_id}/
        
        Args:
            task_type: ä»»åŠ¡ç±»å‹ï¼ˆå¦‚: AmazonReviewStarJobï¼‰
            task_id: ä»»åŠ¡IDï¼ˆå¦‚: 1887037115222994944ï¼‰
            save_dir: ä¿å­˜ç›®å½•ï¼Œé»˜è®¤: data/output
            decompress: æ˜¯å¦è‡ªåŠ¨è§£å‹ç¼©ï¼Œé»˜è®¤ä¸ºTrue
            
        Returns:
            Dict: åŒ…å«æ“ä½œç»“æœçš„å­—å…¸
        """
        if self.account_name != 'collector0109':
            logger.error("è·å–è§£ææ–‡ä»¶éœ€è¦ä½¿ç”¨collector0109å­˜å‚¨è´¦æˆ·")
            return {
                'success': False,
                'error': 'éœ€è¦ä½¿ç”¨collector0109å­˜å‚¨è´¦æˆ·',
                'files_downloaded': [],
                'save_path': None
            }
        
        logger.info(f"ğŸ” æ­£åœ¨ä»collector0109è·å–è§£ææ–‡ä»¶")
        logger.info(f"ğŸ“‹ ä»»åŠ¡ç±»å‹: {task_type}")
        logger.info(f"ğŸ“‹ ä»»åŠ¡ID: {task_id}")
        
        container_name = self.storage_config['container_name']
        # æ ¹æ®æµ‹è¯•ç»“æœï¼Œä½¿ç”¨æ­£ç¡®çš„è·¯å¾„ç»“æ„ï¼šparse/{task_type}/{task_id}/
        blob_prefix = f"{self.storage_config['blob_base_path']}/{task_type}/{task_id}/"
        
        logger.info(f"ğŸ“ æœç´¢è·¯å¾„: {blob_prefix}")
        
        try:
            container_client = self.blob_service_client.get_container_client(container_name)
            
            # æŸ¥æ‰¾æ‰€æœ‰æ–‡ä»¶
            blobs = list(container_client.list_blobs(name_starts_with=blob_prefix))
            
            if not blobs:
                logger.warning(f"âŒ æœªæ‰¾åˆ°ä»»ä½•è§£ææ–‡ä»¶ï¼Œè·¯å¾„: {blob_prefix}")
                return {
                    'success': False,
                    'error': f'æœªæ‰¾åˆ°è§£ææ–‡ä»¶ï¼Œè·¯å¾„: {blob_prefix}',
                    'files_downloaded': [],
                    'save_path': None
                }
            
            logger.info(f"âœ… æ‰¾åˆ° {len(blobs)} ä¸ªè§£ææ–‡ä»¶")
            
            # åˆ›å»ºä¿å­˜ç›®å½•
            save_path = Path(save_dir) / 'parse' / task_type / task_id
            save_path.mkdir(parents=True, exist_ok=True)
            logger.info(f"ğŸ“‚ ä¿å­˜ç›®å½•: {save_path}")
            
            downloaded_files = []
            
            # ä¼˜å…ˆçº§å¤„ç†ï¼šJSON > JSON.GZ > å…¶ä»–
            json_files = [b for b in blobs if b.name.endswith('.json')]
            json_gz_files = [b for b in blobs if b.name.endswith('.json.gz')]
            other_files = [b for b in blobs if not (b.name.endswith('.json') or b.name.endswith('.json.gz'))]
            
            # æŒ‰ä¼˜å…ˆçº§å¤„ç†æ–‡ä»¶
            files_to_process = []
            if json_files:
                files_to_process.extend(json_files)
                logger.info(f"ğŸ“„ æ‰¾åˆ° {len(json_files)} ä¸ªJSONæ–‡ä»¶")
            if json_gz_files:
                files_to_process.extend(json_gz_files)
                logger.info(f"ğŸ“„ æ‰¾åˆ° {len(json_gz_files)} ä¸ªå‹ç¼©JSONæ–‡ä»¶")
            if other_files:
                files_to_process.extend(other_files)
                logger.info(f"ğŸ“„ æ‰¾åˆ° {len(other_files)} ä¸ªå…¶ä»–æ–‡ä»¶")
            
            # ä¸‹è½½æ‰€æœ‰æ–‡ä»¶
            for blob in files_to_process:
                try:
                    logger.info(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½: {blob.name}")
                    
                    # è¯»å–æ–‡ä»¶å†…å®¹
                    content = self.read_blob_content(container_name, blob.name, decompress=decompress)
                    
                    if content is not None:
                        # ç”Ÿæˆä¿å­˜æ–‡ä»¶åï¼Œç¡®ä¿JSONæ–‡ä»¶ä¿æŒ.jsonæ‰©å±•å
                        file_name = Path(blob.name).name
                        if decompress and blob.name.endswith('.gz'):
                            # å¦‚æœè§£å‹ç¼©äº†ï¼Œç§»é™¤.gzæ‰©å±•å
                            file_name = file_name.replace('.gz', '')
                        
                        # ç¡®ä¿JSONæ–‡ä»¶å§‹ç»ˆä¿æŒ.jsonæ‰©å±•å
                        if file_name.endswith('.json') or blob.name.endswith('.json') or blob.name.endswith('.json.gz'):
                            if not file_name.endswith('.json'):
                                file_name = file_name.rsplit('.', 1)[0] + '.json'
                        
                        local_file_path = save_path / file_name
                        
                        # ä¿å­˜æ–‡ä»¶
                        success = self._save_content_to_file(content, str(local_file_path))
                        
                        if success:
                            downloaded_files.append({
                                'original_name': blob.name,
                                'saved_name': file_name,
                                'local_path': str(local_file_path),
                                'size': blob.size,
                                'content_length': len(content) if isinstance(content, str) else len(content)
                            })
                            logger.info(f"âœ… å·²ä¿å­˜: {local_file_path}")
                            
                            # å¦‚æœæ˜¯JSONæ–‡ä»¶ï¼Œæ˜¾ç¤ºé¢„è§ˆ
                            if file_name.endswith('.json') and isinstance(content, str):
                                try:
                                    import json as json_module
                                    parsed_data = json_module.loads(content)
                                    logger.info(f"ğŸ“‹ JSONè§£ææˆåŠŸï¼Œç±»å‹: {type(parsed_data)}")
                                    if isinstance(parsed_data, dict):
                                        logger.info(f"ğŸ”‘ JSONé”®: {list(parsed_data.keys())[:5]}...")
                                    elif isinstance(parsed_data, list):
                                        logger.info(f"ğŸ“Š JSONæ•°ç»„é•¿åº¦: {len(parsed_data)}")
                                except:
                                    logger.info(f"ğŸ“„ JSONæ–‡ä»¶å·²ä¿å­˜ï¼Œä½†æ— æ³•è§£æå†…å®¹")
                        else:
                            logger.error(f"âŒ ä¿å­˜å¤±è´¥: {blob.name}")
                    else:
                        logger.error(f"âŒ è¯»å–å¤±è´¥: {blob.name}")
                        
                except Exception as e:
                    logger.error(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ {blob.name}: {str(e)}")
            
            # è¿”å›ç»“æœ
            result = {
                'success': len(downloaded_files) > 0,
                'files_downloaded': downloaded_files,
                'save_path': str(save_path),
                'total_files_found': len(blobs),
                'total_files_downloaded': len(downloaded_files)
            }
            
            if downloaded_files:
                logger.info(f"âœ… æˆåŠŸä¸‹è½½ {len(downloaded_files)} ä¸ªè§£ææ–‡ä»¶åˆ°: {save_path}")
            else:
                logger.warning(f"âš ï¸  æœªèƒ½ä¸‹è½½ä»»ä½•æ–‡ä»¶")
                result['error'] = 'æœªèƒ½ä¸‹è½½ä»»ä½•æ–‡ä»¶'
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ è·å–è§£ææ–‡ä»¶å¤±è´¥: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'files_downloaded': [],
                'save_path': None
            }
    
    def _save_content_to_file(self, content: Union[str, bytes], file_path: str) -> bool:
        """
        ä¿å­˜å†…å®¹åˆ°æ–‡ä»¶ï¼ˆå†…éƒ¨æ–¹æ³•ï¼‰
        
        Args:
            content: æ–‡ä»¶å†…å®¹
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: ä¿å­˜æˆåŠŸè¿”å›True
        """
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            local_path = Path(file_path)
            local_path.parent.mkdir(parents=True, exist_ok=True)
            
            # å†™å…¥æ–‡ä»¶
            if isinstance(content, str):
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(content)
            else:
                with open(file_path, 'wb') as f:
                    f.write(content)
            
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {str(e)}")
            return False


def main():
    """ä¸»å‡½æ•°ï¼šå¤„ç†å‘½ä»¤è¡Œå‚æ•°å¹¶æ‰§è¡Œç›¸åº”æ“ä½œ"""
    parser = argparse.ArgumentParser(
        description='Azure Storage èµ„æºè¯»å–å™¨',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # è¯»å–åŸå§‹æ•°æ®æ–‡ä»¶ï¼ˆyiya0110è´¦æˆ·ï¼‰
  python3 src/azure_resource_reader.py AmazonListingJob 1925464883027513344 html
  python3 src/azure_resource_reader.py AmazonReviewStarJob 2796867471 html
  
  # ğŸ†• ç›´æ¥è·å–è§£ææ–‡ä»¶ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼Œæ¨èï¼‰
  python3 src/azure_resource_reader.py AmazonReviewStarJob 1887037115222994944 --fetch-parse
  python3 src/azure_resource_reader.py AmazonListingJob 1925464883027513344 --fetch-parse
  
  # ğŸ†• åŒæ—¶è¯»å–åŸå§‹æ•°æ®å’Œè§£ææ•°æ®
  python3 src/azure_resource_reader.py AmazonReviewStarJob 2796867471 html --with-parse
  python3 src/azure_resource_reader.py AmazonListingJob 1925464883027513344 json --with-parse
  
  # è¯»å–è§£ææ–‡ä»¶ï¼ˆä¼ ç»Ÿæ–¹å¼ï¼‰
  python3 src/azure_resource_reader.py --account collector0109 --parse-mode SL2796867471 1910599147004108800 json
  python3 src/azure_resource_reader.py --account collector0109 --parse-mode SL2796867471 1910599147004108800 json --files result.json
  
  # æŸ¥çœ‹å½“å‰çš„ä»»åŠ¡æ˜ å°„
  python3 src/azure_resource_reader.py --show-mapping
  
  # ç¦ç”¨æ˜ å°„æ–‡ä»¶ç”Ÿæˆ
  python3 src/azure_resource_reader.py AmazonListingJob 2796867471 html --no-mapping
  
æ˜ å°„åŠŸèƒ½è¯´æ˜:
  - æ¯æ¬¡æˆåŠŸä¸‹è½½æ–‡ä»¶åï¼Œä¼šåœ¨ data/output/task_mapping.json ä¸­è®°å½•æ˜ å°„å…³ç³»
  - æ˜ å°„æ ¼å¼: è¾“å…¥å‚æ•° -> å®é™…ä¸‹è½½è·¯å¾„
  - ä¾‹å¦‚: 2796867471 -> ./AmazonReviewStarJob/1910599147004108800/*
  - ğŸ†• --with-parseæ¨¡å¼: åŸå§‹æ–‡ä»¶å’Œè§£ææ–‡ä»¶ä¿å­˜åœ¨åŒä¸€ç›®å½•ï¼Œä½¿ç”¨ç»Ÿä¸€æ˜ å°„
  - ğŸ†• --fetch-parseæ¨¡å¼: ä»…è§£ææ–‡ä»¶ï¼Œå•ç‹¬æ˜ å°„åˆ° ./parse/{ä»»åŠ¡ç±»å‹}/{ä»»åŠ¡ID}/
  - è§£ææ¨¡å¼: SL2796867471 -> ./parse/SL2796867471/1910599147004108800/*
        """
    )
    
    parser.add_argument('task_type_or_job_id', 
                       nargs='?',
                       help='ä»»åŠ¡ç±»å‹ï¼ˆå¦‚: AmazonListingJobï¼‰æˆ–è§£ææ¨¡å¼ä¸‹çš„job_idï¼ˆå¦‚: SL2796867471ï¼‰')
    parser.add_argument('task_id_or_task_id', 
                       nargs='?',
                       help='ä»»åŠ¡IDï¼ˆé•¿æ•°å­—ä¸²ï¼‰æˆ–è§£ææ¨¡å¼ä¸‹çš„task_id')
    parser.add_argument('output_type', 
                       nargs='?',
                       choices=['html', 'txt', 'json', 'raw'],
                       help='è¾“å‡ºæ–‡ä»¶ç±»å‹: html(è‡ªåŠ¨è§£å‹), txt(è‡ªåŠ¨è§£å‹), json(è‡ªåŠ¨è§£å‹), raw(ä¸è§£å‹)')
    
    parser.add_argument('--account', '-a',
                       choices=['yiya0110', 'collector0109'],
                       default='yiya0110',
                       help='Azureå­˜å‚¨è´¦æˆ·: yiya0110(åŸå§‹æ•°æ®), collector0109(è§£ææ•°æ®)')
    parser.add_argument('--parse-mode', '-p',
                       action='store_true',
                       help='è§£ææ–‡ä»¶æ¨¡å¼ï¼ˆéœ€è¦é…åˆ--account collector0109ä½¿ç”¨ï¼‰')
    parser.add_argument('--with-parse', 
                       action='store_true',
                       help='åŒæ—¶è·å–åŸå§‹æ•°æ®å’Œè§£ææ•°æ®ï¼ˆè‡ªåŠ¨ä½¿ç”¨yiya0110è¯»å–åŸå§‹æ•°æ®ï¼Œcollector0109è¯»å–è§£ææ•°æ®ï¼‰')
    parser.add_argument('--fetch-parse', 
                       action='store_true',
                       help='ğŸ†• ç›´æ¥ä»collector0109è·å–è§£ææ–‡ä»¶ï¼ˆä¼˜åŒ–è·¯å¾„ï¼ŒåŸºäºæµ‹è¯•ç»“æœï¼‰')
    parser.add_argument('--files', '-f',
                       nargs='+',
                       default=None,
                       help='è¦è¯»å–çš„æ–‡ä»¶åˆ—è¡¨ï¼Œè§£ææ¨¡å¼ä¸‹é»˜è®¤è‡ªåŠ¨æŸ¥æ‰¾JSONæ–‡ä»¶')
    parser.add_argument('--save-dir', '-s',
                       default='data/output',
                       help='ä¿å­˜ç›®å½•ï¼Œé»˜è®¤: data/output')
    parser.add_argument('--info-only', '-i',
                       action='store_true',
                       help='ä»…æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯ï¼Œä¸ä¸‹è½½å†…å®¹')
    parser.add_argument('--list-jobs', '-l',
                       action='store_true',
                       help='åˆ—å‡ºæŒ‡å®šä»»åŠ¡ç±»å‹çš„æ‰€æœ‰ä»»åŠ¡')
    parser.add_argument('--no-mapping', 
                       action='store_true',
                       help='ä¸ç”Ÿæˆä»»åŠ¡æ˜ å°„æ–‡ä»¶')
    parser.add_argument('--show-mapping', 
                       action='store_true',
                       help='æ˜¾ç¤ºå½“å‰çš„ä»»åŠ¡æ˜ å°„æ–‡ä»¶å†…å®¹')
    
    args = parser.parse_args()
    
    # å¦‚æœåªæ˜¯æ˜¾ç¤ºæ˜ å°„æ–‡ä»¶å†…å®¹
    if args.show_mapping:
        show_task_mapping(args.save_dir)
        return
    
    # ğŸ†• å¤„ç† --fetch-parse æ¨¡å¼
    if args.fetch_parse:
        if not args.task_type_or_job_id or not args.task_id_or_task_id:
            parser.error("--fetch-parse æ¨¡å¼éœ€è¦æä¾› task_type å’Œ task_id å‚æ•°")
        
        task_type = args.task_type_or_job_id
        task_id = args.task_id_or_task_id
        
        print(f"ğŸ” Azure Storage è§£ææ–‡ä»¶è·å–å™¨ (ä¼˜åŒ–ç‰ˆæœ¬)")
        print(f"ğŸ“‹ ä»»åŠ¡ç±»å‹: {task_type}")
        print(f"ğŸ“‹ ä»»åŠ¡ID: {task_id}")
        print(f"ğŸ“ æœç´¢è·¯å¾„: collector0109/parse/{task_type}/{task_id}/")
        print("=" * 80)
        
        # åˆ›å»ºcollector0109è¯»å–å™¨
        reader = AzureResourceReader('collector0109')
        
        # ä½¿ç”¨ä¼˜åŒ–çš„æ–¹æ³•è·å–è§£ææ–‡ä»¶
        result = reader.fetch_and_save_parse_files(
            task_type=task_type,
            task_id=task_id,
            save_dir=args.save_dir,
            decompress=True
        )
        
        # æ˜¾ç¤ºç»“æœ
        if result['success']:
            print(f"\nâœ… è§£ææ–‡ä»¶è·å–æˆåŠŸ!")
            print(f"ğŸ“‚ ä¿å­˜è·¯å¾„: {result['save_path']}")
            print(f"ğŸ“Š æ‰¾åˆ°æ–‡ä»¶æ•°: {result['total_files_found']}")
            print(f"ğŸ“¥ ä¸‹è½½æ–‡ä»¶æ•°: {result['total_files_downloaded']}")
            
            if result['files_downloaded']:
                print(f"\nğŸ“„ å·²ä¸‹è½½çš„æ–‡ä»¶:")
                for file_info in result['files_downloaded']:
                    print(f"  âœ… {file_info['saved_name']}")
                    print(f"     ğŸ“Š å¤§å°: {file_info['size']} å­—èŠ‚")
                    print(f"     ğŸ“ è·¯å¾„: {file_info['local_path']}")
                    print()
        else:
            print(f"\nâŒ è§£ææ–‡ä»¶è·å–å¤±è´¥!")
            print(f"ğŸ” é”™è¯¯ä¿¡æ¯: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        
        return
    
    # æ£€æŸ¥å¿…éœ€å‚æ•°
    if not args.task_type_or_job_id or not args.task_id_or_task_id or not args.output_type:
        parser.error("å½“ä¸ä½¿ç”¨ --show-mapping æ—¶ï¼Œéœ€è¦æä¾› task_type_or_job_id, task_id_or_task_id å’Œ output_type å‚æ•°")
    
    # ä¿å­˜åŸå§‹è¾“å…¥å‚æ•°ç”¨äºæ˜ å°„
    original_input = args.task_id_or_task_id
    
    # æ£€æŸ¥è§£ææ¨¡å¼çš„å‚æ•°ä¸€è‡´æ€§
    if args.parse_mode and args.account != 'collector0109':
        parser.error("è§£ææ¨¡å¼å¿…é¡»ä½¿ç”¨ --account collector0109")
    
    if args.account == 'collector0109' and not args.parse_mode:
        print("âš ï¸  ä½¿ç”¨collector0109è´¦æˆ·ä½†æœªå¯ç”¨è§£ææ¨¡å¼ï¼Œå°†è‡ªåŠ¨å¯ç”¨è§£ææ¨¡å¼")
        args.parse_mode = True
    
    # æ£€æŸ¥--with-parseå‚æ•°
    if args.with_parse:
        if args.account != 'yiya0110':
            print("âš ï¸  --with-parse æ¨¡å¼å°†è‡ªåŠ¨ä½¿ç”¨yiya0110ä½œä¸ºä¸»è´¦æˆ·")
            args.account = 'yiya0110'
        
        # å¤„ç†åŒæ—¶è·å–åŸå§‹æ•°æ®å’Œè§£ææ•°æ®çš„æ¨¡å¼
        handle_with_parse_mode(args)
        return
    
    # è§£ææ¨¡å¼ç‰¹æ®Šå¤„ç†
    if args.parse_mode:
        print(f"ğŸ” Azure Storage èµ„æºè¯»å–å™¨ (è§£ææ¨¡å¼)")
        print(f"ğŸ“‹ Job ID: {args.task_type_or_job_id}")
        print(f"ğŸ“‹ Task ID: {args.task_id_or_task_id}")
        
        job_id = args.task_type_or_job_id
        task_id = args.task_id_or_task_id
        original_input = f"{job_id}:{task_id}"
        
        print(f"ğŸ“ è§£ææ•°æ®è·¯å¾„: collector0109/parse/{job_id}/{task_id}/")
        print("=" * 80)
        
        # åˆ›å»ºèµ„æºè¯»å–å™¨
        reader = AzureResourceReader(args.account)
        
        # å¤„ç†è§£ææ–‡ä»¶
        handle_parse_mode(reader, job_id, task_id, args, original_input)
        return
    
    # åŸå§‹æ¨¡å¼å¤„ç†
    # ç¬¬ä¸€æ­¥ï¼šéªŒè¯å’Œè½¬æ¢ä»»åŠ¡ID
    print(f"ğŸ” Azure Storage èµ„æºè¯»å–å™¨")
    print(f"ğŸ“‹ ä»»åŠ¡ç±»å‹: {args.task_type_or_job_id}")
    print(f"ğŸ“‹ è¾“å…¥å‚æ•°: {args.task_id_or_task_id}")
    
    # æ£€æŸ¥è¾“å…¥æ˜¯å¦ä¸ºæœ‰æ•ˆçš„ä»»åŠ¡ID
    if is_valid_task_id(args.task_id_or_task_id):
        # ç›´æ¥ä½¿ç”¨ä½œä¸ºä»»åŠ¡ID
        task_id = args.task_id_or_task_id
        print(f"âœ… æ£€æµ‹åˆ°æœ‰æ•ˆçš„ä»»åŠ¡ID: {task_id}")
    else:
        # éœ€è¦è½¬æ¢ä¸ºä»»åŠ¡ID
        job_id = args.task_id_or_task_id
        
        # å¦‚æœæ˜¯çº¯æ•°å­—ï¼Œæ·»åŠ SLå‰ç¼€
        if job_id.isdigit():
            job_id = f"SL{job_id}"
            print(f"ğŸ”„ æ·»åŠ SLå‰ç¼€: {job_id}")
        
        print(f"ğŸ” é€šè¿‡æ•°æ®åº“æŸ¥è¯¢è½¬æ¢ job_id: {job_id}")
        task_id = convert_job_id_to_task_id(job_id)
        
        if task_id is None:
            print(f"âŒ æ— æ³•æ‰¾åˆ°å¯¹åº”çš„ä»»åŠ¡IDï¼Œè¯·æ£€æŸ¥ job_id: {job_id}")
            return
        
        print(f"âœ… æŸ¥è¯¢æˆåŠŸï¼Œè·å¾—ä»»åŠ¡ID: {task_id}")
    
    print(f"ğŸ“ è·¯å¾„ç»“æ„: {args.account if hasattr(args, 'account') else 'yiya0110'}/{args.task_type_or_job_id}/{task_id}/")
    print("=" * 80)
    
    # ç¬¬äºŒæ­¥ï¼šç¡®å®šè¦å¤„ç†çš„æ–‡ä»¶åˆ—è¡¨
    if args.files is None:
        # æ ¹æ®ä»»åŠ¡ç±»å‹è‡ªåŠ¨é€‰æ‹©é»˜è®¤æ–‡ä»¶
        files_to_process = get_default_files_for_task_type(args.task_type_or_job_id)
        print(f"ğŸ“„ æ ¹æ®ä»»åŠ¡ç±»å‹è‡ªåŠ¨é€‰æ‹©æ–‡ä»¶: {', '.join(files_to_process)}")
    else:
        files_to_process = args.files
        print(f"ğŸ“„ ç”¨æˆ·æŒ‡å®šæ–‡ä»¶: {', '.join(files_to_process)}")
    
    # åˆ›å»ºèµ„æºè¯»å–å™¨
    reader = AzureResourceReader(args.account)
    
    # å¦‚æœåªæ˜¯åˆ—å‡ºä»»åŠ¡
    if args.list_jobs:
        print(f"ğŸ“‹ åˆ—å‡º {args.task_type_or_job_id} ä»»åŠ¡ (å‰10ä¸ª):")
        jobs = reader.list_task_jobs(args.task_type_or_job_id, limit=10)
        
        if jobs:
            print(f"âœ… æ‰¾åˆ° {len(jobs)} ä¸ªä»»åŠ¡:")
            for job in jobs:
                job_id_extracted = job.get('job_id', 'Unknown')
                print(f"  ğŸ“‹ ä»»åŠ¡ID: {job_id_extracted}")
                print(f"     ğŸ“„ æ–‡ä»¶: {job['name']}")
                print(f"     ğŸ“Š å¤§å°: {job['size']} å­—èŠ‚")
                print(f"     ğŸ“… ä¿®æ”¹: {job['last_modified']}")
                print()
        else:
            print("âŒ æœªæ‰¾åˆ°ä»»åŠ¡")
        return
    
    # ç¡®å®šæ˜¯å¦éœ€è¦è§£å‹ç¼©
    decompress = args.output_type in ['html', 'txt', 'json']
    
    # ç”¨äºè®°å½•æ˜¯å¦æœ‰æ–‡ä»¶æˆåŠŸä¸‹è½½
    successfully_downloaded_files = []
    
    # å¤„ç†æ¯ä¸ªæ–‡ä»¶
    for filename in files_to_process:
        print(f"\nğŸ“„ å¤„ç†æ–‡ä»¶: {filename}")
        print("-" * 60)
        
        # ä»…æ˜¾ç¤ºä¿¡æ¯
        if args.info_only:
            blob_path = f"compress/{args.task_type_or_job_id}/{task_id}/{filename}"
            blob_info = reader.get_blob_info('download', blob_path)
            
            if blob_info:
                print(f"âœ… æ–‡ä»¶ä¿¡æ¯:")
                print(f"  ğŸ“Š å¤§å°: {blob_info['size_mb']} MB")
                print(f"  ğŸ“… ä¿®æ”¹æ—¶é—´: {blob_info['last_modified']}")
                print(f"  ğŸ”— URL: {blob_info['url']}")
            else:
                print("âŒ æ–‡ä»¶ä¸å­˜åœ¨æˆ–è·å–ä¿¡æ¯å¤±è´¥")
            continue
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        content = reader.read_task_file(args.task_type_or_job_id, task_id, filename, decompress)
        
        if content is None:
            print("âŒ è¯»å–å¤±è´¥æˆ–æ–‡ä»¶ä¸å­˜åœ¨")
            continue
        
        print("âœ… è¯»å–æˆåŠŸ!")
        
        # æ˜¾ç¤ºå†…å®¹ä¿¡æ¯
        if isinstance(content, str):
            print(f"ğŸ“ å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
            
            # å¦‚æœæ˜¯JSONç±»å‹ï¼Œå°è¯•è§£æ
            if args.output_type == 'json':
                try:
                    if content.strip().startswith('{') or content.strip().startswith('['):
                        parsed_data = json.loads(content)
                        print(f"ğŸ“‹ JSONè§£ææˆåŠŸï¼Œç±»å‹: {type(parsed_data)}")
                        if isinstance(parsed_data, dict):
                            print(f"ğŸ”‘ JSONé”®: {list(parsed_data.keys())}")
                        elif isinstance(parsed_data, list):
                            print(f"ğŸ“Š JSONæ•°ç»„é•¿åº¦: {len(parsed_data)}")
                except json.JSONDecodeError:
                    print("âš ï¸  å†…å®¹ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼")
            
            # æ˜¾ç¤ºé¢„è§ˆ
            print(f"ğŸ” å†…å®¹é¢„è§ˆ (å‰200å­—ç¬¦):")
            print(content[:200] + "..." if len(content) > 200 else content)
            
        else:
            print(f"ğŸ“Š æ•°æ®é•¿åº¦: {len(content)} å­—èŠ‚")
        
        # ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶
        actual_filename = filename if filename else "auto_found"
        
        # æ£€æµ‹å†…å®¹æ˜¯å¦ä¸ºJSONæ ¼å¼ï¼Œå¦‚æœæ˜¯åˆ™å¼ºåˆ¶ä½¿ç”¨jsonæ‰©å±•å
        output_type_to_use = args.output_type
        if isinstance(content, str) and (content.strip().startswith('{') or content.strip().startswith('[')):
            try:
                json.loads(content)  # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆJSON
                output_type_to_use = "json"  # å¼ºåˆ¶ä½¿ç”¨jsonæ ¼å¼
                print("ğŸ” æ£€æµ‹åˆ°JSONå†…å®¹ï¼Œå°†ä¿å­˜ä¸º.jsonæ–‡ä»¶")
            except json.JSONDecodeError:
                pass  # ä¸æ˜¯æœ‰æ•ˆJSONï¼Œä¿æŒåŸè¾“å‡ºç±»å‹
        
        save_filename = _generate_save_filename(actual_filename, task_id, output_type_to_use)
        local_path = f"{args.save_dir}/parse/{job_id}/{task_id}/{save_filename}"
        
        success = _save_content_to_file(content, local_path)
        if success:
            print(f"ğŸ’¾ æ–‡ä»¶å·²ä¿å­˜åˆ°: {local_path}")
            successfully_downloaded_files.append(filename)
        else:
            print("âŒ ä¿å­˜å¤±è´¥")
    
    # ç¬¬ä¸‰æ­¥ï¼šæ›´æ–°ä»»åŠ¡æ˜ å°„æ–‡ä»¶ï¼ˆå¦‚æœæœ‰æ–‡ä»¶æˆåŠŸä¸‹è½½ä¸”æœªç¦ç”¨æ˜ å°„ï¼‰
    if successfully_downloaded_files and not args.info_only and not args.no_mapping:
        print("\n" + "=" * 80)
        print("ç¬¬ä¸‰æ­¥ï¼šæ›´æ–°ä»»åŠ¡æ˜ å°„æ–‡ä»¶")
        print("=" * 80)
        
        # æ˜¾ç¤ºæ˜ å°„ä¿¡æ¯
        print_task_mapping_info(original_input, args.task_type_or_job_id, task_id, args.save_dir)
        
        # æ›´æ–°æ˜ å°„
        mapping_success = update_task_mapping(original_input, args.task_type_or_job_id, task_id, args.save_dir)
        
        if mapping_success:
            print(f"âœ… æˆåŠŸä¸‹è½½ {len(successfully_downloaded_files)} ä¸ªæ–‡ä»¶å¹¶æ›´æ–°æ˜ å°„")
            print(f"ğŸ“„ å·²ä¸‹è½½æ–‡ä»¶: {', '.join(successfully_downloaded_files)}")
        else:
            print("âš ï¸  æ–‡ä»¶ä¸‹è½½æˆåŠŸä½†æ˜ å°„æ›´æ–°å¤±è´¥")
    
    elif args.info_only:
        print(f"\nğŸ“‹ ä¿¡æ¯æŸ¥çœ‹æ¨¡å¼ï¼Œæœªä¸‹è½½æ–‡ä»¶")
    elif args.no_mapping:
        print(f"\nğŸ“‹ å·²ç¦ç”¨æ˜ å°„æ–‡ä»¶ç”Ÿæˆ")
    elif not successfully_downloaded_files:
        print(f"\nâš ï¸  æ²¡æœ‰æ–‡ä»¶æˆåŠŸä¸‹è½½ï¼Œæœªæ›´æ–°æ˜ å°„")


def _generate_save_filename(original_filename: str, task_id: str, output_type: str) -> str:
    """
    ç”Ÿæˆä¿å­˜æ–‡ä»¶åï¼ˆä¸åŒ…å«è·¯å¾„ï¼Œåªæ˜¯æ–‡ä»¶åï¼‰
    
    Args:
        original_filename: åŸå§‹æ–‡ä»¶åï¼Œå¦‚ 'login.gz'
        task_id: ä»»åŠ¡ID
        output_type: è¾“å‡ºç±»å‹
        
    Returns:
        str: ç”Ÿæˆçš„æ–‡ä»¶å
    """
    base_name = original_filename.replace('.gz', '').replace('.', '_')
    
    if output_type == 'raw':
        return original_filename
    else:
        extension = output_type if output_type in ['html', 'txt', 'json'] else 'txt'
        return f"{base_name}.{extension}"


def _save_content_to_file(content: Union[str, bytes], file_path: str) -> bool:
    """
    ä¿å­˜å†…å®¹åˆ°æ–‡ä»¶
    
    Args:
        content: æ–‡ä»¶å†…å®¹
        file_path: æ–‡ä»¶è·¯å¾„
        
    Returns:
        bool: ä¿å­˜æˆåŠŸè¿”å›True
    """
    try:
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        local_path = Path(file_path)
        local_path.parent.mkdir(parents=True, exist_ok=True)
        
        # å†™å…¥æ–‡ä»¶
        if isinstance(content, str):
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)
        else:
            with open(file_path, 'wb') as f:
                f.write(content)
        
        return True
        
    except Exception as e:
        logger.error(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {str(e)}")
        return False


def demo_read_amazon_listing_job():
    """æ¼”ç¤ºè¯»å–Amazon Listing Jobæ–‡ä»¶ï¼ˆä¿ç•™ç”¨äºæµ‹è¯•ï¼‰"""
    # åˆ›å»ºèµ„æºè¯»å–å™¨
    reader = AzureResourceReader('yiya0110')
    
    # æŒ‡å®šè¦è¯»å–çš„ä»»åŠ¡ID
    job_id = '1925464883027513344'
    
    print(f"ğŸ” æ­£åœ¨è¯»å–Amazon Listing Jobæ–‡ä»¶...")
    print(f"ğŸ“‹ ä»»åŠ¡ç±»å‹: AmazonListingJob")
    print(f"ğŸ“‹ ä»»åŠ¡ID: {job_id}")
    print(f"ğŸ“ è·¯å¾„ç»“æ„: yiya0110/download/compress/AmazonListingJob/{job_id}/")
    print("=" * 80)
    
    # æ–¹æ³•1ï¼šè¯»å–é»˜è®¤çš„ä¸¤ä¸ªæ–‡ä»¶
    print("æ–¹æ³•1ï¼šè¯»å–ä¸¤ä¸ªé»˜è®¤æ–‡ä»¶ (login.gz + normal.gz)")
    both_files = reader.read_amazon_listing_job_both_files(job_id)
    
    for file_type, content in both_files.items():
        print(f"\nğŸ“„ {file_type}.gz æ–‡ä»¶:")
        if content:
            print(f"âœ… è¯»å–æˆåŠŸ!")
            if isinstance(content, str):
                print(f"ğŸ“ å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
                print(f"ğŸ” å†…å®¹é¢„è§ˆ (å‰200å­—ç¬¦):")
                print(content[:200] + "..." if len(content) > 200 else content)
            else:
                print(f"ğŸ“Š æ•°æ®é•¿åº¦: {len(content)} å­—èŠ‚")
        else:
            print("âŒ è¯»å–å¤±è´¥æˆ–æ–‡ä»¶ä¸å­˜åœ¨")
    
    print("\n" + "=" * 80)
    
    # æ–¹æ³•2ï¼šä½¿ç”¨é€šç”¨æ–¹æ³•è¯»å–æŒ‡å®šæ–‡ä»¶
    print("æ–¹æ³•2ï¼šä½¿ç”¨é€šç”¨æ–¹æ³•è¯»å–login.gz")
    login_content = reader.read_task_file('AmazonListingJob', job_id, 'login.gz')
    
    if login_content:
        print("âœ… é€šç”¨æ–¹æ³•è¯»å–æˆåŠŸ!")
        if isinstance(login_content, str):
            print(f"ğŸ“ å†…å®¹é•¿åº¦: {len(login_content)} å­—ç¬¦")
        else:
            print(f"ğŸ“Š æ•°æ®é•¿åº¦: {len(login_content)} å­—èŠ‚")
    else:
        print("âŒ é€šç”¨æ–¹æ³•è¯»å–å¤±è´¥")
    
    print("\n" + "=" * 80)
    
    # æ–¹æ³•3ï¼šè·å–æ–‡ä»¶ä¿¡æ¯
    print("æ–¹æ³•3ï¼šè·å–æ–‡ä»¶è¯¦ç»†ä¿¡æ¯")
    for filename in ['login.gz', 'normal.gz']:
        blob_path = f"compress/AmazonListingJob/{job_id}/{filename}"
        blob_info = reader.get_blob_info('download', blob_path)
        
        print(f"\nğŸ“„ {filename}:")
        if blob_info:
            print(f"  ğŸ“Š å¤§å°: {blob_info['size_mb']} MB")
            print(f"  ğŸ“… ä¿®æ”¹æ—¶é—´: {blob_info['last_modified']}")
            print(f"  ğŸ”— URL: {blob_info['url']}")
        else:
            print("  âŒ æ–‡ä»¶ä¸å­˜åœ¨æˆ–è·å–ä¿¡æ¯å¤±è´¥")
    
    print("\n" + "=" * 80)
    
    # æ–¹æ³•4ï¼šä¿å­˜æ–‡ä»¶åˆ°æœ¬åœ°
    print("æ–¹æ³•4ï¼šä¸‹è½½å¹¶ä¿å­˜åˆ°æœ¬åœ°")
    for filename in ['login.gz', 'normal.gz']:
        blob_path = f"compress/AmazonListingJob/{job_id}/{filename}"
        local_filename = filename.replace('.gz', '.txt')
        local_path = f"data/output/amazon_listing_{job_id}_{local_filename}"
        
        success = reader.save_blob_to_file('download', blob_path, local_path)
        if success:
            print(f"âœ… {filename} å·²ä¿å­˜åˆ°: {local_path}")
        else:
            print(f"âŒ {filename} ä¿å­˜å¤±è´¥")
    
    print("\n" + "=" * 80)
    
    # æ–¹æ³•5ï¼šåˆ—å‡ºAmazon Listing Jobs
    print("æ–¹æ³•5ï¼šåˆ—å‡ºæœ€è¿‘çš„Amazon Listing Jobs (å‰5ä¸ª)")
    jobs = reader.list_amazon_listing_jobs(limit=5)
    
    if jobs:
        print(f"âœ… æ‰¾åˆ° {len(jobs)} ä¸ªä»»åŠ¡:")
        for job in jobs:
            job_id_extracted = job.get('job_id', 'Unknown')
            print(f"  ğŸ“‹ ä»»åŠ¡ID: {job_id_extracted}")
            print(f"     ğŸ“„ æ–‡ä»¶: {job['name']}")
            print(f"     ğŸ“Š å¤§å°: {job['size']} å­—èŠ‚")
            print(f"     ğŸ“… ä¿®æ”¹: {job['last_modified']}")
            print()
    else:
        print("âŒ æœªæ‰¾åˆ°ä»»åŠ¡æˆ–åˆ—å–å¤±è´¥")


def is_valid_task_id(task_id: str) -> bool:
    """
    æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆçš„ä»»åŠ¡IDæ ¼å¼ï¼ˆé•¿æ•°å­—ä¸²ï¼‰
    
    Args:
        task_id: è¦æ£€æŸ¥çš„ä»»åŠ¡ID
        
    Returns:
        bool: å¦‚æœæ˜¯æœ‰æ•ˆçš„ä»»åŠ¡IDè¿”å›True
    """
    # æ£€æŸ¥æ˜¯å¦ä¸º18-20ä½çš„çº¯æ•°å­—
    return re.match(r'^\d{18,20}$', task_id) is not None


def convert_job_id_to_task_id(job_id: str) -> Optional[str]:
    """
    é€šè¿‡æ•°æ®åº“æŸ¥è¯¢å°† job_id è½¬æ¢ä¸º task_id
    
    Args:
        job_id: è¯·æ±‚åºåˆ—å·ï¼Œå¦‚ 'SL2796867471'
        
    Returns:
        Optional[str]: æ‰¾åˆ°çš„ task_id (ext_ssn)ï¼Œæœªæ‰¾åˆ°è¿”å›None
    """
    # åˆ›å»ºæ•°æ®åº“è¿æ¥
    db_config = DB_CONFIG.copy()
    db_config['database'] = 'shulex_collector_prod'
    
    db = DatabaseConnector(db_config)
    if not db.connect():
        logger.error("æ— æ³•è¿æ¥åˆ°æ•°æ®åº“ shulex_collector_prod")
        return None
    
    try:
        # å®šä¹‰è¦æŸ¥è¯¢çš„è¡¨
        tables_to_check = ['log_a', 'log_b', 'log_c', 'log_d']
        
        logger.info(f"æ­£åœ¨æŸ¥è¯¢ job_id: {job_id}")
        
        all_results = []
        
        # æŸ¥è¯¢å„ä¸ªè¡¨
        for table_name in tables_to_check:
            query = f"SELECT * FROM {table_name} WHERE req_ssn = %s"
            try:
                records = db.execute_query(query, (job_id,))
                if records:
                    all_results.extend(records)
                    logger.info(f"åœ¨è¡¨ {table_name} ä¸­æ‰¾åˆ° {len(records)} æ¡è®°å½•")
                    
            except Exception as e:
                logger.error(f"æŸ¥è¯¢è¡¨ {table_name} å¤±è´¥: {str(e)}")
        
        # åˆ†ææŸ¥è¯¢ç»“æœ
        if len(all_results) == 0:
            logger.warning(f"åœ¨æ‰€æœ‰è¡¨ä¸­éƒ½æ²¡æœ‰æ‰¾åˆ° job_id: {job_id}")
            return None
            
        elif len(all_results) == 1:
            # æ‰¾åˆ°å”¯ä¸€è®°å½•
            record = all_results[0]
            ext_ssn = record.get('ext_ssn', '')
            if ext_ssn:
                logger.info(f"æ‰¾åˆ°å”¯ä¸€è®°å½•ï¼Œtask_id (ext_ssn): {ext_ssn}")
                return ext_ssn
            else:
                logger.warning(f"æ‰¾åˆ°è®°å½•ä½† ext_ssn ä¸ºç©º")
                return None
                
        else:
            # æ‰¾åˆ°å¤šæ¡è®°å½•ï¼Œéœ€è¦å»é‡
            unique_ext_ssns = set()
            for record in all_results:
                ext_ssn = record.get('ext_ssn', '')
                if ext_ssn:
                    unique_ext_ssns.add(ext_ssn)
            
            if len(unique_ext_ssns) == 1:
                task_id = list(unique_ext_ssns)[0]
                logger.info(f"æ‰¾åˆ° {len(all_results)} æ¡è®°å½•ï¼Œå»é‡åå¾—åˆ°å”¯ä¸€ task_id: {task_id}")
                return task_id
            else:
                logger.warning(f"æ‰¾åˆ° {len(all_results)} æ¡è®°å½•ï¼Œä½†åŒ…å« {len(unique_ext_ssns)} ä¸ªä¸åŒçš„ ext_ssn")
                return None
    
    finally:
        db.disconnect()


def get_default_files_for_task_type(task_type: str) -> List[str]:
    """
    æ ¹æ®ä»»åŠ¡ç±»å‹è·å–é»˜è®¤çš„æ–‡ä»¶åˆ—è¡¨
    
    Args:
        task_type: ä»»åŠ¡ç±»å‹ï¼Œå¦‚ 'AmazonListingJob' æˆ– 'AmazonReviewStarJob'
        
    Returns:
        List[str]: é»˜è®¤æ–‡ä»¶åˆ—è¡¨
    """
    if task_type == 'AmazonListingJob':
        return ['login.gz', 'normal.gz']
    elif task_type == 'AmazonReviewStarJob':
        return ['page_1.gz', 'page_2.gz', 'page_3.gz', 'page_4.gz', 'page_5.gz']
    else:
        # å¯¹äºæœªçŸ¥ä»»åŠ¡ç±»å‹ï¼Œå°è¯•é»˜è®¤æ–‡ä»¶
        return ['login.gz', 'normal.gz']


def update_task_mapping(input_param: str, task_type: str, actual_task_id: str, 
                       save_dir: str = 'data/output') -> bool:
    """
    æ›´æ–°ä»»åŠ¡æ˜ å°„æ–‡ä»¶ï¼Œè®°å½•è¾“å…¥å‚æ•°åˆ°å®é™…ä¸‹è½½è·¯å¾„çš„æ˜ å°„
    
    Args:
        input_param: ç”¨æˆ·è¾“å…¥çš„å‚æ•°ï¼ˆtask_idæˆ–job_idï¼‰
        task_type: ä»»åŠ¡ç±»å‹
        actual_task_id: å®é™…çš„ä»»åŠ¡ID
        save_dir: ä¿å­˜ç›®å½•
        
    Returns:
        bool: æ›´æ–°æˆåŠŸè¿”å›True
    """
    try:
        # æ˜ å°„æ–‡ä»¶è·¯å¾„
        map_file_path = f"{save_dir}/task_mapping.json"
        
        # è¯»å–ç°æœ‰æ˜ å°„
        mapping = {}
        if os.path.exists(map_file_path):
            try:
                with open(map_file_path, 'r', encoding='utf-8') as f:
                    mapping = json.load(f)
            except (json.JSONDecodeError, FileNotFoundError):
                logger.warning(f"æ˜ å°„æ–‡ä»¶æ ¼å¼é”™è¯¯æˆ–ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°çš„æ˜ å°„æ–‡ä»¶")
                mapping = {}
        
        # ç”Ÿæˆç›¸å¯¹è·¯å¾„
        relative_path = f"./{task_type}/{actual_task_id}/"
        
        # æ›´æ–°æ˜ å°„
        mapping[input_param] = {
            'relative_path': relative_path,
            'task_type': task_type,
            'actual_task_id': actual_task_id,
            'last_updated': datetime.now().isoformat()
        }
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(save_dir, exist_ok=True)
        
        # ä¿å­˜æ›´æ–°åçš„æ˜ å°„
        with open(map_file_path, 'w', encoding='utf-8') as f:
            json.dump(mapping, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ… ä»»åŠ¡æ˜ å°„å·²æ›´æ–°: {input_param} -> {relative_path}")
        return True
        
    except Exception as e:
        logger.error(f"âŒ æ›´æ–°ä»»åŠ¡æ˜ å°„å¤±è´¥: {str(e)}")
        return False


def print_task_mapping_info(input_param: str, task_type: str, actual_task_id: str, 
                           save_dir: str = 'data/output') -> None:
    """
    æ‰“å°ä»»åŠ¡æ˜ å°„ä¿¡æ¯
    
    Args:
        input_param: ç”¨æˆ·è¾“å…¥çš„å‚æ•°
        task_type: ä»»åŠ¡ç±»å‹
        actual_task_id: å®é™…çš„ä»»åŠ¡ID
        save_dir: ä¿å­˜ç›®å½•
    """
    relative_path = f"./{task_type}/{actual_task_id}/"
    full_path = f"{save_dir}/{task_type}/{actual_task_id}/"
    
    print(f"\nğŸ“‹ ä»»åŠ¡æ˜ å°„ä¿¡æ¯:")
    print(f"  ğŸ” è¾“å…¥å‚æ•°: {input_param}")
    print(f"  ğŸ“ ç›¸å¯¹è·¯å¾„: {relative_path}")
    print(f"  ğŸ“„ æ˜ å°„æ–‡ä»¶: {save_dir}/task_mapping.json")


def show_task_mapping(save_dir: str = 'data/output') -> None:
    """
    æ˜¾ç¤ºå½“å‰çš„ä»»åŠ¡æ˜ å°„æ–‡ä»¶å†…å®¹
    
    Args:
        save_dir: ä¿å­˜ç›®å½•
    """
    map_file_path = f"{save_dir}/task_mapping.json"
    
    print("ğŸ” Azure Storage ä»»åŠ¡æ˜ å°„æŸ¥çœ‹å™¨")
    print("=" * 80)
    
    if not os.path.exists(map_file_path):
        print(f"âŒ æ˜ å°„æ–‡ä»¶ä¸å­˜åœ¨: {map_file_path}")
        return
    
    try:
        with open(map_file_path, 'r', encoding='utf-8') as f:
            mapping = json.load(f)
        
        if not mapping:
            print("ğŸ“‹ æ˜ å°„æ–‡ä»¶ä¸ºç©º")
            return
        
        print(f"ğŸ“„ æ˜ å°„æ–‡ä»¶è·¯å¾„: {map_file_path}")
        print(f"ğŸ“Š æ€»è®¡æ˜ å°„æ•°é‡: {len(mapping)}")
        print("\nğŸ“‹ ä»»åŠ¡æ˜ å°„åˆ—è¡¨:")
        print("-" * 80)
        
        # æŒ‰æœ€åæ›´æ–°æ—¶é—´æ’åº
        sorted_mappings = sorted(
            mapping.items(),
            key=lambda x: x[1].get('last_updated', ''),
            reverse=True
        )
        
        for input_param, info in sorted_mappings:
            print(f"\nğŸ” è¾“å…¥å‚æ•°: {input_param}")
            task_type = info.get('task_type', 'Unknown')
            
            if task_type == 'parse':
                # è§£ææ–‡ä»¶æ˜ å°„æ˜¾ç¤º
                print(f"  ğŸ“‚ ç±»å‹: è§£ææ–‡ä»¶")
                print(f"  ğŸ“‹ Job ID: {info.get('job_id', 'Unknown')}")
                print(f"  ğŸ“‹ Task ID: {info.get('task_id', 'Unknown')}")
            else:
                # åŸå§‹æ–‡ä»¶æ˜ å°„æ˜¾ç¤º
                print(f"  ğŸ“‚ ä»»åŠ¡ç±»å‹: {task_type}")
                print(f"  ğŸ“‹ å®é™…ä»»åŠ¡ID: {info.get('actual_task_id', 'Unknown')}")
            
            print(f"  ğŸ“ ç›¸å¯¹è·¯å¾„: {info.get('relative_path', 'Unknown')}")
            print(f"  ğŸ“… æœ€åæ›´æ–°: {info.get('last_updated', 'Unknown')}")
            
            # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨ï¼Œé€šè¿‡relative_pathæ„å»ºå®Œæ•´è·¯å¾„
            relative_path = info.get('relative_path', '')
            if relative_path:
                # ç§»é™¤ç›¸å¯¹è·¯å¾„å‰ç¼€ './' å¹¶æ„å»ºå®Œæ•´è·¯å¾„
                clean_path = relative_path.lstrip('./')
                full_path = os.path.join(save_dir, clean_path)
                
                if os.path.exists(full_path):
                    file_count = len([f for f in os.listdir(full_path) if os.path.isfile(os.path.join(full_path, f))])
                    print(f"  ğŸ“„ æ–‡ä»¶æ•°é‡: {file_count}")
                else:
                    print(f"  âš ï¸  ç›®å½•ä¸å­˜åœ¨")
            else:
                print(f"  âš ï¸  ç›¸å¯¹è·¯å¾„ä¿¡æ¯ç¼ºå¤±")
        
        print("\n" + "=" * 80)
        print("ğŸ’¡ ä½¿ç”¨æ–¹å¼:")
        print(f"   æ ¹æ®æ˜ å°„ï¼Œè¾“å…¥å‚æ•°å¯ä»¥ç›´æ¥å®šä½åˆ°å¯¹åº”çš„ä¸‹è½½æ–‡ä»¶å¤¹")
        print(f"   ä¾‹å¦‚ï¼šè¾“å…¥ '{list(mapping.keys())[0]}' å¯¹åº”æ–‡ä»¶å¤¹ '{list(mapping.values())[0].get('relative_path', '')}*'")
        
    except json.JSONDecodeError:
        print(f"âŒ æ˜ å°„æ–‡ä»¶æ ¼å¼é”™è¯¯: {map_file_path}")
    except Exception as e:
        print(f"âŒ è¯»å–æ˜ å°„æ–‡ä»¶å¤±è´¥: {str(e)}")


def handle_with_parse_mode(args) -> None:
    """
    å¤„ç†åŒæ—¶è·å–åŸå§‹æ•°æ®å’Œè§£ææ•°æ®çš„æ¨¡å¼ï¼ˆé›†æˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
    """
    print(f"ğŸ” Azure Storage èµ„æºè¯»å–å™¨ (åŸå§‹æ•°æ® + è§£ææ•°æ® - ä¼˜åŒ–ç‰ˆæœ¬)")
    print(f"ğŸ“‹ ä»»åŠ¡ç±»å‹: {args.task_type_or_job_id}")
    print(f"ğŸ“‹ è¾“å…¥å‚æ•°: {args.task_id_or_task_id}")
    
    # ä¿å­˜åŸå§‹è¾“å…¥å‚æ•°ç”¨äºæ˜ å°„
    original_input = args.task_id_or_task_id
    
    # ç¬¬ä¸€æ­¥ï¼šéªŒè¯å’Œè½¬æ¢ä»»åŠ¡IDï¼ŒåŒæ—¶è·å–analysis_response
    task_id = None
    analysis_response = None
    job_id = None
    
    if is_valid_task_id(args.task_id_or_task_id):
        # ç›´æ¥ä½¿ç”¨ä½œä¸ºä»»åŠ¡ID
        task_id = args.task_id_or_task_id
        print(f"âœ… æ£€æµ‹åˆ°æœ‰æ•ˆçš„ä»»åŠ¡ID: {task_id}")
    else:
        # éœ€è¦è½¬æ¢ä¸ºä»»åŠ¡IDå¹¶è·å–analysis_response
        job_id = args.task_id_or_task_id
        
        # å¦‚æœæ˜¯çº¯æ•°å­—ï¼Œæ·»åŠ SLå‰ç¼€
        if job_id.isdigit():
            job_id = f"SL{job_id}"
            print(f"ğŸ”„ æ·»åŠ SLå‰ç¼€: {job_id}")
        
        print(f"ğŸ” é€šè¿‡æ•°æ®åº“æŸ¥è¯¢è½¬æ¢ job_id: {job_id}")
        
        # ğŸ†• å°è¯•ä½¿ç”¨ä¼˜åŒ–å™¨è·å–task_idå’Œanalysis_response
        try:
            # å¯¼å…¥ä¼˜åŒ–å™¨æ¨¡å—
            from src.azure_resource_reader_optimizer import convert_job_id_to_task_info
            task_info = convert_job_id_to_task_info(job_id)
            
            if task_info is None:
                print(f"âŒ æ— æ³•æ‰¾åˆ°å¯¹åº”çš„ä»»åŠ¡IDï¼Œè¯·æ£€æŸ¥ job_id: {job_id}")
                return
            
            task_id, analysis_response = task_info
            print(f"âœ… æŸ¥è¯¢æˆåŠŸï¼Œè·å¾—ä»»åŠ¡ID: {task_id}")
            
            if analysis_response:
                print(f"âœ… è·å¾— analysis_response æ•°æ®ï¼Œé•¿åº¦: {len(str(analysis_response))} å­—ç¬¦")
                
                # æ£€æŸ¥æ˜¯å¦å¯ç”¨äº†analysis_responseè§£æ
                try:
                    from config.analysis_response_config import is_analysis_response_enabled
                    
                    if is_analysis_response_enabled(args.task_type_or_job_id):
                        print(f"âœ… ä»»åŠ¡ç±»å‹ {args.task_type_or_job_id} å·²å¯ç”¨ analysis_response è§£æ")
                    else:
                        print(f"âš ï¸  ä»»åŠ¡ç±»å‹ {args.task_type_or_job_id} æœªå¯ç”¨ analysis_response è§£æï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•")
                        analysis_response = None
                except ImportError:
                    print(f"âš ï¸  æ— æ³•å¯¼å…¥ analysis_response é…ç½®ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•")
                    analysis_response = None
            else:
                print(f"â„¹ï¸  æœªæ‰¾åˆ° analysis_response æ•°æ®")
                
        except ImportError:
            # å›é€€åˆ°åŸå§‹æ–¹æ³•
            print(f"âš ï¸  ä¼˜åŒ–å™¨æ¨¡å—ä¸å¯ç”¨ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•")
            task_id = convert_job_id_to_task_id(job_id)
            
            if task_id is None:
                print(f"âŒ æ— æ³•æ‰¾åˆ°å¯¹åº”çš„ä»»åŠ¡IDï¼Œè¯·æ£€æŸ¥ job_id: {job_id}")
                return
            
            print(f"âœ… æŸ¥è¯¢æˆåŠŸï¼Œè·å¾—ä»»åŠ¡ID: {task_id}")
    
    task_type = args.task_type_or_job_id
    print(f"ğŸ“ åŸå§‹æ•°æ®è·¯å¾„: yiya0110/download/compress/{task_type}/{task_id}/")
    print(f"ğŸ“ è§£ææ•°æ®è·¯å¾„: collector0109/parse/{task_type}/{task_id}/")
    print("=" * 80)
    
    # ç¬¬äºŒæ­¥ï¼šç¡®å®šè¦å¤„ç†çš„æ–‡ä»¶åˆ—è¡¨
    if args.files is None:
        # æ ¹æ®ä»»åŠ¡ç±»å‹è‡ªåŠ¨é€‰æ‹©é»˜è®¤æ–‡ä»¶
        files_to_process = get_default_files_for_task_type(task_type)
        print(f"ğŸ“„ æ ¹æ®ä»»åŠ¡ç±»å‹è‡ªåŠ¨é€‰æ‹©åŸå§‹æ–‡ä»¶: {', '.join(files_to_process)}")
    else:
        files_to_process = args.files
        print(f"ğŸ“„ ç”¨æˆ·æŒ‡å®šåŸå§‹æ–‡ä»¶: {', '.join(files_to_process)}")
    
    # åˆ›å»ºä¸»è¯»å–å™¨ï¼ˆyiya0110ï¼‰
    reader = AzureResourceReader('yiya0110')
    
    # ç¡®å®šæ˜¯å¦éœ€è¦è§£å‹ç¼©
    decompress = args.output_type in ['html', 'txt', 'json']
    
    # ç”¨äºè®°å½•æ˜¯å¦æœ‰æ–‡ä»¶æˆåŠŸä¸‹è½½
    successfully_downloaded_files = []
    parse_file_downloaded = False
    parse_result = None
    
    # ğŸ†• é¦–å…ˆå°è¯•ä½¿ç”¨ä¼˜åŒ–æ–¹æ³•è·å–è§£ææ–‡ä»¶
    if not args.info_only:
        print(f"\nğŸš€ æ­¥éª¤1: ä½¿ç”¨ä¼˜åŒ–æ–¹æ³•è·å–è§£ææ–‡ä»¶")
        print("-" * 60)
        
        try:
            from src.azure_resource_reader_optimizer import fetch_and_save_parse_files_optimized
            
            # åˆ›å»ºcollector0109è¯»å–å™¨ç”¨äºè§£ææ–‡ä»¶
            parse_reader = AzureResourceReader('collector0109')
            
            # ä½¿ç”¨ä¼˜åŒ–æ–¹æ³•è·å–è§£ææ–‡ä»¶
            parse_result = fetch_and_save_parse_files_optimized(
                reader=parse_reader,
                task_type=task_type,
                task_id=task_id,
                save_dir=args.save_dir,
                decompress=decompress,
                job_id=job_id,
                analysis_response=analysis_response
            )
            
            if parse_result['success']:
                print(f"âœ… è§£ææ–‡ä»¶è·å–æˆåŠŸ!")
                if 'method_used' in parse_result:
                    method_name = "analysis_responseé“¾æ¥" if parse_result['method_used'] == 'analysis_response' else "Azureå­˜å‚¨"
                    print(f"ğŸ“¡ è·å–æ–¹å¼: {method_name}")
                print(f"ğŸ“¥ ä¸‹è½½æ–‡ä»¶æ•°: {parse_result['total_files_downloaded']}")
                
                # æ˜¾ç¤ºæ–‡ä»¶è¯¦æƒ…
                if parse_result['files_downloaded']:
                    for file_info in parse_result['files_downloaded']:
                        print(f"  âœ… {file_info['saved_name']}")
                        if 'size' in file_info:
                            print(f"     ğŸ“Š å¤§å°: {file_info['size']} å­—èŠ‚")
                        print(f"     ğŸ“ è·¯å¾„: {file_info['local_path']}")
                
                parse_file_downloaded = True
            else:
                print(f"âŒ è§£ææ–‡ä»¶è·å–å¤±è´¥: {parse_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                
        except ImportError:
            print(f"âš ï¸  ä¼˜åŒ–å™¨æ¨¡å—ä¸å¯ç”¨ï¼Œå°†åœ¨å¤„ç†åŸå§‹æ–‡ä»¶æ—¶ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•")
    
    # å¤„ç†æ¯ä¸ªåŸå§‹æ–‡ä»¶
    print(f"\nğŸ“„ æ­¥éª¤2: è·å–åŸå§‹æ–‡ä»¶")
    print("-" * 60)
    
    for filename in files_to_process:
        print(f"\nğŸ“„ å¤„ç†åŸå§‹æ–‡ä»¶: {filename}")
        print("-" * 40)
        
        if args.info_only:
            # æ˜¾ç¤ºåŸå§‹æ–‡ä»¶ä¿¡æ¯
            blob_path = f"compress/{task_type}/{task_id}/{filename}"
            blob_info = reader.get_blob_info('download', blob_path)
            
            if blob_info:
                print(f"âœ… åŸå§‹æ–‡ä»¶ä¿¡æ¯:")
                print(f"  ğŸ“Š å¤§å°: {blob_info['size_mb']} MB")
                print(f"  ğŸ“… ä¿®æ”¹æ—¶é—´: {blob_info['last_modified']}")
                print(f"  ğŸ”— URL: {blob_info['url']}")
            else:
                print("âŒ åŸå§‹æ–‡ä»¶ä¸å­˜åœ¨æˆ–è·å–ä¿¡æ¯å¤±è´¥")
            continue
        
        # è¯»å–åŸå§‹æ–‡ä»¶
        content = reader.read_task_file(task_type, task_id, filename, decompress)
        
        if content is not None:
            print("âœ… åŸå§‹æ–‡ä»¶è¯»å–æˆåŠŸ!")
            if isinstance(content, str):
                print(f"ğŸ“ åŸå§‹æ–‡ä»¶é•¿åº¦: {len(content)} å­—ç¬¦")
            else:
                print(f"ğŸ“Š åŸå§‹æ–‡ä»¶å¤§å°: {len(content)} å­—èŠ‚")
            
            # ä¿å­˜åŸå§‹æ–‡ä»¶
            save_filename = _generate_save_filename(filename, task_id, args.output_type)
            local_path = f"{args.save_dir}/{task_type}/{task_id}/{save_filename}"
            
            success = _save_content_to_file(content, local_path)
            if success:
                print(f"ğŸ’¾ åŸå§‹æ–‡ä»¶å·²ä¿å­˜åˆ°: {local_path}")
                successfully_downloaded_files.append(filename)
        else:
            print("âŒ åŸå§‹æ–‡ä»¶è¯»å–å¤±è´¥æˆ–æ–‡ä»¶ä¸å­˜åœ¨")
    
    # å¦‚æœä¼˜åŒ–æ–¹æ³•æ²¡æœ‰æˆåŠŸè·å–è§£ææ–‡ä»¶ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•
    if not parse_file_downloaded and not args.info_only:
        print(f"\nğŸ”„ æ­¥éª¤3: ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•è·å–è§£ææ–‡ä»¶")
        print("-" * 60)
        
        # ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•è·å–è§£ææ–‡ä»¶ï¼ˆä»…ç¬¬ä¸€ä¸ªæ–‡ä»¶æ—¶æ‰§è¡Œï¼‰
        if files_to_process:
            filename = files_to_process[0]  # è§£ææ–‡ä»¶ä¸å…·ä½“åŸå§‹æ–‡ä»¶æ— å…³
            
            # ä½¿ç”¨æ–°æ–¹æ³•åŒæ—¶è¯»å–åŸå§‹æ–‡ä»¶å’Œè§£ææ–‡ä»¶ä¸­çš„è§£æéƒ¨åˆ†
            combined_content = reader.read_task_file_with_parse(task_type, task_id, filename, decompress)
            parse_content = combined_content.get('parse')
            
            if parse_content is not None:
                print("âœ… è§£ææ–‡ä»¶è¯»å–æˆåŠŸ!")
                if isinstance(parse_content, str):
                    print(f"ğŸ“ è§£ææ–‡ä»¶é•¿åº¦: {len(parse_content)} å­—ç¬¦")
                    
                    # JSONæ ¼å¼éªŒè¯å’Œæ£€æµ‹
                    if parse_content.strip().startswith('{') or parse_content.strip().startswith('['):
                        try:
                            parsed_data = json.loads(parse_content)
                            print(f"ğŸ“‹ JSONè§£ææˆåŠŸï¼Œç±»å‹: {type(parsed_data)}")
                        except json.JSONDecodeError:
                            print("âš ï¸  è§£ææ–‡ä»¶ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼")
                    
                    # æ˜¾ç¤ºé¢„è§ˆ
                    print(f"ğŸ” è§£ææ–‡ä»¶é¢„è§ˆ (å‰200å­—ç¬¦):")
                    print(parse_content[:200] + "..." if len(parse_content) > 200 else parse_content)
                else:
                    print(f"ğŸ“Š è§£ææ–‡ä»¶å¤§å°: {len(parse_content)} å­—èŠ‚")
                
                # ä¿å­˜è§£ææ–‡ä»¶
                parse_filename = _generate_save_filename("parse_result", task_id, "json")
                parse_local_path = f"{args.save_dir}/{task_type}/{task_id}/{parse_filename}"
                
                parse_success = _save_content_to_file(parse_content, parse_local_path)
                if parse_success:
                    print(f"ğŸ’¾ è§£ææ–‡ä»¶å·²ä¿å­˜åˆ°: {parse_local_path}")
                    parse_file_downloaded = True
            else:
                print("âŒ è§£ææ–‡ä»¶è¯»å–å¤±è´¥æˆ–æ–‡ä»¶ä¸å­˜åœ¨")
    
    # ç¬¬ä¸‰æ­¥ï¼šæ›´æ–°ä»»åŠ¡æ˜ å°„æ–‡ä»¶
    if (successfully_downloaded_files or parse_file_downloaded) and not args.info_only and not args.no_mapping:
        print("\n" + "=" * 80)
        print("æ­¥éª¤4ï¼šæ›´æ–°ä»»åŠ¡æ˜ å°„æ–‡ä»¶")
        print("=" * 80)
        
        # æ˜¾ç¤ºæ˜ å°„ä¿¡æ¯
        print_task_mapping_info(original_input, task_type, task_id, args.save_dir)
        
        # æ›´æ–°åŸå§‹æ–‡ä»¶æ˜ å°„
        mapping_success = update_task_mapping(original_input, task_type, task_id, args.save_dir)
        
        # åœ¨ --with-parse æ¨¡å¼ä¸‹ï¼Œè§£ææ–‡ä»¶å’ŒåŸå§‹æ–‡ä»¶åœ¨åŒä¸€ç›®å½•ï¼Œä¸éœ€è¦å•ç‹¬çš„æ˜ å°„
        if mapping_success:
            if parse_file_downloaded:
                print(f"âœ… æˆåŠŸä¸‹è½½åŸå§‹æ–‡ä»¶å’Œè§£ææ–‡ä»¶å¹¶æ›´æ–°æ˜ å°„")
                print(f"ğŸ“„ åŸå§‹æ–‡ä»¶æ•°é‡: {len(successfully_downloaded_files)}")
                print(f"ğŸ“„ è§£ææ–‡ä»¶: 1ä¸ª (ä¿å­˜åœ¨åŒä¸€ç›®å½•)")
                
                # æ˜¾ç¤ºä½¿ç”¨çš„æ–¹æ³•
                if parse_result and 'method_used' in parse_result:
                    method_name = "analysis_responseä¼˜åŒ–é“¾æ¥" if parse_result['method_used'] == 'analysis_response' else "Azureå­˜å‚¨"
                    print(f"ğŸš€ è§£ææ–‡ä»¶è·å–æ–¹å¼: {method_name}")
            else:
                print(f"âœ… æˆåŠŸä¸‹è½½åŸå§‹æ–‡ä»¶å¹¶æ›´æ–°æ˜ å°„")
                print(f"ğŸ“„ å·²ä¸‹è½½æ–‡ä»¶: {', '.join(successfully_downloaded_files)}")
                if not parse_file_downloaded:
                    print("âš ï¸  è§£ææ–‡ä»¶ä¸‹è½½å¤±è´¥")
    
    elif args.info_only:
        print(f"\nğŸ“‹ ä¿¡æ¯æŸ¥çœ‹æ¨¡å¼ï¼Œæœªä¸‹è½½æ–‡ä»¶")
        # æ˜¾ç¤ºè§£ææ–‡ä»¶ä¿¡æ¯ï¼ˆå¦‚æœæœªåœ¨ä¼˜åŒ–æ­¥éª¤ä¸­è·å–ï¼‰
        if not parse_file_downloaded:
            parse_reader = AzureResourceReader('collector0109')
            parse_files = parse_reader.list_parse_files(task_type, task_id)
            if parse_files:
                print(f"\nâœ… è§£ææ–‡ä»¶ä¿¡æ¯ (å…±{len(parse_files)}ä¸ª):")
                for parse_file in parse_files:
                    print(f"  ğŸ“„ {parse_file['name']}")
                    print(f"     ğŸ“Š å¤§å°: {parse_file['size']} å­—èŠ‚")
            else:
                print("\nâŒ æœªæ‰¾åˆ°è§£ææ–‡ä»¶")
                
    elif args.no_mapping:
        print(f"\nğŸ“‹ å·²ç¦ç”¨æ˜ å°„æ–‡ä»¶ç”Ÿæˆ")
    elif not successfully_downloaded_files and not parse_file_downloaded:
        print(f"\nâš ï¸  æ²¡æœ‰æ–‡ä»¶æˆåŠŸä¸‹è½½ï¼Œæœªæ›´æ–°æ˜ å°„")


def handle_parse_mode(reader: AzureResourceReader, job_id: str, task_id: str, 
                     args, original_input: str) -> None:
    """
    å¤„ç†è§£ææ¨¡å¼çš„æ–‡ä»¶è¯»å–
    
    Args:
        reader: Azureèµ„æºè¯»å–å™¨å®ä¾‹
        job_id: è¯·æ±‚åºåˆ—å·
        task_id: ä»»åŠ¡ID
        args: å‘½ä»¤è¡Œå‚æ•°
        original_input: åŸå§‹è¾“å…¥å‚æ•°
    """
    
    # å¦‚æœåªæ˜¯åˆ—å‡ºæ–‡ä»¶
    if args.list_jobs:
        print(f"ğŸ“‹ åˆ—å‡ºè§£ææ–‡ä»¶:")
        files = reader.list_parse_files(job_id, task_id)
        
        if files:
            print(f"âœ… æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶:")
            for file_info in files:
                print(f"  ğŸ“„ æ–‡ä»¶: {file_info['name']}")
                print(f"     ğŸ“Š å¤§å°: {file_info['size']} å­—èŠ‚")
                print(f"     ğŸ“… ä¿®æ”¹: {file_info['last_modified']}")
                print()
        else:
            print("âŒ æœªæ‰¾åˆ°æ–‡ä»¶")
        return
    
    # ç¡®å®šæ˜¯å¦éœ€è¦è§£å‹ç¼©
    decompress = args.output_type in ['html', 'txt', 'json']
    
    # ç”¨äºè®°å½•æ˜¯å¦æœ‰æ–‡ä»¶æˆåŠŸä¸‹è½½
    successfully_downloaded_files = []
    
    # ç¡®å®šè¦å¤„ç†çš„æ–‡ä»¶åˆ—è¡¨
    if args.files is None:
        # è§£ææ¨¡å¼é»˜è®¤è‡ªåŠ¨æŸ¥æ‰¾JSONæ–‡ä»¶
        files_to_process = [None]  # Noneè¡¨ç¤ºè‡ªåŠ¨æŸ¥æ‰¾
        print(f"ğŸ“„ è§£ææ¨¡å¼ï¼šè‡ªåŠ¨æŸ¥æ‰¾JSONæ–‡ä»¶")
    else:
        files_to_process = args.files
        print(f"ğŸ“„ ç”¨æˆ·æŒ‡å®šæ–‡ä»¶: {', '.join(files_to_process)}")
    
    # å¤„ç†æ¯ä¸ªæ–‡ä»¶
    for filename in files_to_process:
        if filename is None:
            print(f"\nğŸ“„ è‡ªåŠ¨æŸ¥æ‰¾è§£ææ–‡ä»¶:")
        else:
            print(f"\nğŸ“„ å¤„ç†æ–‡ä»¶: {filename}")
        print("-" * 60)
        
        # ä»…æ˜¾ç¤ºä¿¡æ¯
        if args.info_only:
            if filename is None:
                files = reader.list_parse_files(job_id, task_id)
                if files:
                    print(f"âœ… æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶:")
                    for file_info in files:
                        print(f"  ğŸ“„ {file_info['name']}")
                        print(f"     ğŸ“Š å¤§å°: {file_info['size']} å­—èŠ‚")
                else:
                    print("âŒ æœªæ‰¾åˆ°æ–‡ä»¶")
            else:
                # æ„é€ å®Œæ•´è·¯å¾„ç”¨äºè·å–ä¿¡æ¯
                blob_path = f"parse/{job_id}/{task_id}/{filename}"
                blob_info = reader.get_blob_info('parse', blob_path)
                
                if blob_info:
                    print(f"âœ… æ–‡ä»¶ä¿¡æ¯:")
                    print(f"  ğŸ“Š å¤§å°: {blob_info['size_mb']} MB")
                    print(f"  ğŸ“… ä¿®æ”¹æ—¶é—´: {blob_info['last_modified']}")
                    print(f"  ğŸ”— URL: {blob_info['url']}")
                else:
                    print("âŒ æ–‡ä»¶ä¸å­˜åœ¨æˆ–è·å–ä¿¡æ¯å¤±è´¥")
            continue
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        content = reader.read_parse_file(job_id, task_id, filename, decompress)
        
        if content is None:
            print("âŒ è¯»å–å¤±è´¥æˆ–æ–‡ä»¶ä¸å­˜åœ¨")
            continue
        
        print("âœ… è¯»å–æˆåŠŸ!")
        
        # æ˜¾ç¤ºå†…å®¹ä¿¡æ¯
        if isinstance(content, str):
            print(f"ğŸ“ å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
            
            # å¦‚æœæ˜¯JSONç±»å‹ï¼Œå°è¯•è§£æå’Œæ ¼å¼åŒ–
            if args.output_type == 'json':
                try:
                    if content.strip().startswith('{') or content.strip().startswith('['):
                        parsed_data = json.loads(content)
                        print(f"ğŸ“‹ JSONè§£ææˆåŠŸï¼Œç±»å‹: {type(parsed_data)}")
                        if isinstance(parsed_data, dict):
                            print(f"ğŸ”‘ JSONé”®: {list(parsed_data.keys())}")
                        elif isinstance(parsed_data, list):
                            print(f"ğŸ“Š JSONæ•°ç»„é•¿åº¦: {len(parsed_data)}")
                except json.JSONDecodeError:
                    print("âš ï¸  å†…å®¹ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼")
            
            # æ˜¾ç¤ºé¢„è§ˆ
            print(f"ğŸ” å†…å®¹é¢„è§ˆ (å‰200å­—ç¬¦):")
            print(content[:200] + "..." if len(content) > 200 else content)
            
        else:
            print(f"ğŸ“Š æ•°æ®é•¿åº¦: {len(content)} å­—èŠ‚")
        
        # ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶
        actual_filename = filename if filename else "auto_found"
        
        # æ£€æµ‹å†…å®¹æ˜¯å¦ä¸ºJSONæ ¼å¼ï¼Œå¦‚æœæ˜¯åˆ™å¼ºåˆ¶ä½¿ç”¨jsonæ‰©å±•å
        output_type_to_use = args.output_type
        if isinstance(content, str) and (content.strip().startswith('{') or content.strip().startswith('[')):
            try:
                json.loads(content)  # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆJSON
                output_type_to_use = "json"  # å¼ºåˆ¶ä½¿ç”¨jsonæ ¼å¼
                print("ğŸ” æ£€æµ‹åˆ°JSONå†…å®¹ï¼Œå°†ä¿å­˜ä¸º.jsonæ–‡ä»¶")
            except json.JSONDecodeError:
                pass  # ä¸æ˜¯æœ‰æ•ˆJSONï¼Œä¿æŒåŸè¾“å‡ºç±»å‹
        
        save_filename = _generate_save_filename(actual_filename, task_id, output_type_to_use)
        local_path = f"{args.save_dir}/parse/{job_id}/{task_id}/{save_filename}"
        
        success = _save_content_to_file(content, local_path)
        if success:
            print(f"ğŸ’¾ æ–‡ä»¶å·²ä¿å­˜åˆ°: {local_path}")
            successfully_downloaded_files.append(actual_filename)
        else:
            print("âŒ ä¿å­˜å¤±è´¥")
    
    # æ›´æ–°ä»»åŠ¡æ˜ å°„æ–‡ä»¶ï¼ˆå¦‚æœæœ‰æ–‡ä»¶æˆåŠŸä¸‹è½½ä¸”æœªç¦ç”¨æ˜ å°„ï¼‰
    if successfully_downloaded_files and not args.info_only and not args.no_mapping:
        print("\n" + "=" * 80)
        print("ç¬¬ä¸‰æ­¥ï¼šæ›´æ–°ä»»åŠ¡æ˜ å°„æ–‡ä»¶")
        print("=" * 80)
        
        # æ˜¾ç¤ºæ˜ å°„ä¿¡æ¯
        print_parse_mapping_info(original_input, job_id, task_id, args.save_dir)
        
        # æ›´æ–°æ˜ å°„
        mapping_success = update_parse_mapping(original_input, job_id, task_id, args.save_dir)
        
        if mapping_success:
            print(f"âœ… æˆåŠŸä¸‹è½½ {len(successfully_downloaded_files)} ä¸ªæ–‡ä»¶å¹¶æ›´æ–°æ˜ å°„")
            print(f"ğŸ“„ å·²ä¸‹è½½æ–‡ä»¶: {', '.join(successfully_downloaded_files)}")
        else:
            print("âš ï¸  æ–‡ä»¶ä¸‹è½½æˆåŠŸä½†æ˜ å°„æ›´æ–°å¤±è´¥")
    
    elif args.info_only:
        print(f"\nğŸ“‹ ä¿¡æ¯æŸ¥çœ‹æ¨¡å¼ï¼Œæœªä¸‹è½½æ–‡ä»¶")
    elif args.no_mapping:
        print(f"\nğŸ“‹ å·²ç¦ç”¨æ˜ å°„æ–‡ä»¶ç”Ÿæˆ")
    elif not successfully_downloaded_files:
        print(f"\nâš ï¸  æ²¡æœ‰æ–‡ä»¶æˆåŠŸä¸‹è½½ï¼Œæœªæ›´æ–°æ˜ å°„")


def print_parse_mapping_info(original_input: str, job_id: str, task_id: str, 
                            save_dir: str = 'data/output') -> None:
    """
    æ‰“å°è§£ææ–‡ä»¶æ˜ å°„ä¿¡æ¯
    
    Args:
        original_input: åŸå§‹è¾“å…¥å‚æ•°
        job_id: è¯·æ±‚åºåˆ—å·ï¼ˆå®é™…æ˜¯ä»»åŠ¡ç±»å‹ï¼‰
        task_id: ä»»åŠ¡ID
        save_dir: ä¿å­˜ç›®å½•
    """
    relative_path = f"./parse/{job_id}/{task_id}/"
    full_path = f"{save_dir}/parse/{job_id}/{task_id}/"
    
    print(f"\nğŸ“‹ è§£ææ–‡ä»¶æ˜ å°„ä¿¡æ¯:")
    print(f"  ğŸ” è¾“å…¥å‚æ•°: {original_input}")
    print(f"  ğŸ“ ç›¸å¯¹è·¯å¾„: {relative_path}")
    print(f"  ğŸ“„ æ˜ å°„æ–‡ä»¶: {save_dir}/task_mapping.json")


def update_parse_mapping(original_input: str, job_id: str, task_id: str, 
                        save_dir: str = 'data/output') -> bool:
    """
    æ›´æ–°è§£ææ–‡ä»¶æ˜ å°„
    
    Args:
        original_input: åŸå§‹è¾“å…¥å‚æ•°
        job_id: è¯·æ±‚åºåˆ—å·ï¼ˆå®é™…æ˜¯ä»»åŠ¡ç±»å‹ï¼‰  
        task_id: ä»»åŠ¡ID
        save_dir: ä¿å­˜ç›®å½•
        
    Returns:
        bool: æ›´æ–°æˆåŠŸè¿”å›True
    """
    try:
        # æ˜ å°„æ–‡ä»¶è·¯å¾„
        map_file_path = f"{save_dir}/task_mapping.json"
        
        # è¯»å–ç°æœ‰æ˜ å°„
        mapping = {}
        if os.path.exists(map_file_path):
            try:
                with open(map_file_path, 'r', encoding='utf-8') as f:
                    mapping = json.load(f)
            except (json.JSONDecodeError, FileNotFoundError):
                logger.warning(f"æ˜ å°„æ–‡ä»¶æ ¼å¼é”™è¯¯æˆ–ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°çš„æ˜ å°„æ–‡ä»¶")
                mapping = {}
        
        # ç”Ÿæˆç›¸å¯¹è·¯å¾„
        relative_path = f"./parse/{job_id}/{task_id}/"
        
        # æ›´æ–°æ˜ å°„
        mapping[original_input] = {
            'relative_path': relative_path,
            'task_type': 'parse',
            'job_id': job_id,
            'task_id': task_id,
            'last_updated': datetime.now().isoformat()
        }
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(save_dir, exist_ok=True)
        
        # ä¿å­˜æ›´æ–°åçš„æ˜ å°„
        with open(map_file_path, 'w', encoding='utf-8') as f:
            json.dump(mapping, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ… è§£ææ–‡ä»¶æ˜ å°„å·²æ›´æ–°: {original_input} -> {relative_path}")
        return True
        
    except Exception as e:
        logger.error(f"âŒ æ›´æ–°è§£ææ–‡ä»¶æ˜ å°„å¤±è´¥: {str(e)}")
        return False


if __name__ == '__main__':
    main() 