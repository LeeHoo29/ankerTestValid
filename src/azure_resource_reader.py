#!/usr/bin/env python3
"""
Azure Storage èµ„æºè¯»å–å™¨
ä¸“é—¨ç”¨äºè¯»å–çº¿ä¸ŠAzure Storageä¸­çš„èµ„æºæ–‡ä»¶
"""
import os
import sys
import gzip
import logging
import json
import argparse
import re
from pathlib import Path
from typing import Dict, List, Optional, Union, BinaryIO
from datetime import datetime
import io

# Azure Storage SDK imports
from azure.identity import ClientSecretCredential, DefaultAzureCredential
from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient
from azure.core.exceptions import AzureError, ResourceNotFoundError

# å¯¼å…¥é¡¹ç›®é…ç½®
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))
from config.azure_storage_config import (
    AZURE_STORAGE_CONFIG,
    YIYA0110_STORAGE_CONFIG,
    COLLECTOR0109_STORAGE_CONFIG,
    set_azure_environment_variables,
    get_storage_account_url
)

# å¯¼å…¥æ•°æ®åº“è¿æ¥å™¨
from src.db.connector import DatabaseConnector
from config.db_config import DB_CONFIG

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def setup_logging(verbose: bool = False):
    """
    è®¾ç½®æ—¥å¿—é…ç½®
    
    Args:
        verbose: æ˜¯å¦å¯ç”¨è¯¦ç»†æ—¥å¿—è¾“å‡º
    """
    if verbose:
        # è¯¦ç»†æ¨¡å¼ï¼šæ˜¾ç¤ºæ‰€æœ‰æ—¥å¿—ï¼ŒåŒ…æ‹¬Azure SDKçš„HTTPè¯·æ±‚
        logging.getLogger().setLevel(logging.INFO)
        logging.getLogger('azure.core.pipeline.policies.http_logging_policy').setLevel(logging.INFO)
        logging.getLogger('azure.identity').setLevel(logging.INFO)
        print("ğŸ”§ å·²å¯ç”¨è¯¦ç»†æ—¥å¿—æ¨¡å¼")
    else:
        # ç®€åŒ–æ¨¡å¼ï¼šéšè—Azure SDKçš„è¯¦ç»†HTTPæ—¥å¿—
        logging.getLogger().setLevel(logging.INFO)
        logging.getLogger('azure.core.pipeline.policies.http_logging_policy').setLevel(logging.WARNING)
        logging.getLogger('azure.identity').setLevel(logging.WARNING)
        # ä¿æŒæˆ‘ä»¬è‡ªå·±çš„æ—¥å¿—
        logging.getLogger(__name__).setLevel(logging.INFO)
        logging.getLogger('src.azure_resource_reader_optimizer').setLevel(logging.INFO)
        logging.getLogger('src.db.connector').setLevel(logging.INFO)


class AzureResourceReader:
    """Azure Storage èµ„æºè¯»å–å™¨"""
    
    def __init__(self, account_name: str = 'yiya0110', use_default_credential: bool = False):
        """
        åˆå§‹åŒ–Azureèµ„æºè¯»å–å™¨
        
        Args:
            account_name: Azureå­˜å‚¨è´¦æˆ·åï¼Œæ”¯æŒ 'yiya0110' (åŸå§‹æ•°æ®) æˆ– 'collector0109' (è§£ææ•°æ®)
            use_default_credential: æ˜¯å¦ä½¿ç”¨é»˜è®¤å‡­æ®ï¼ŒFalseåˆ™ä½¿ç”¨æœåŠ¡ä¸»ä½“è®¤è¯
        """
        self.account_name = account_name
        self.account_url = get_storage_account_url(account_name, 'blob')
        
        # æ ¹æ®è´¦æˆ·åè®¾ç½®å­˜å‚¨é…ç½®
        if account_name == 'yiya0110':
            self.storage_config = YIYA0110_STORAGE_CONFIG
        elif account_name == 'collector0109':
            self.storage_config = COLLECTOR0109_STORAGE_CONFIG
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å­˜å‚¨è´¦æˆ·: {account_name}ã€‚æ”¯æŒçš„è´¦æˆ·: 'yiya0110', 'collector0109'")
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        set_azure_environment_variables()
        
        # åˆ›å»ºè®¤è¯å‡­æ®
        if use_default_credential:
            self.credential = DefaultAzureCredential()
        else:
            self.credential = ClientSecretCredential(
                tenant_id=AZURE_STORAGE_CONFIG['tenant_id'],
                client_id=AZURE_STORAGE_CONFIG['client_id'],
                client_secret=AZURE_STORAGE_CONFIG['client_secret']
            )
        
        # åˆå§‹åŒ–BlobæœåŠ¡å®¢æˆ·ç«¯
        self.blob_service_client = BlobServiceClient(
            account_url=self.account_url,
            credential=self.credential
        )
        
        logger.info(f"Azureèµ„æºè¯»å–å™¨åˆå§‹åŒ–æˆåŠŸ: {account_name} ({self.storage_config['container_name']})")
    
    def read_amazon_listing_job_file(self, job_id: str, filename: str = 'login.gz', 
                                    decompress: bool = True) -> Union[str, bytes, None]:
        """
        è¯»å–Amazon Listing Jobæ–‡ä»¶
        
        Args:
            job_id: ä»»åŠ¡IDï¼Œå¦‚ '1925464883027513344'
            filename: æ–‡ä»¶åï¼Œé»˜è®¤ä¸º 'login.gz'ï¼Œä¹Ÿæ”¯æŒ 'normal.gz'
            decompress: æ˜¯å¦è‡ªåŠ¨è§£å‹ç¼©ï¼Œé»˜è®¤ä¸ºTrue
            
        Returns:
            Union[str, bytes, None]: æ–‡ä»¶å†…å®¹ï¼Œå¦‚æœè§£å‹ç¼©åˆ™è¿”å›strï¼Œå¦åˆ™è¿”å›bytes
        """
        return self.read_task_file('AmazonListingJob', job_id, filename, decompress)
    
    def read_task_file(self, task_type: str, job_id: str, filename: str, 
                      decompress: bool = True) -> Union[str, bytes, None]:
        """
        è¯»å–ä»»åŠ¡æ–‡ä»¶ï¼ˆé€šç”¨æ–¹æ³•ï¼‰
        
        Args:
            task_type: ä»»åŠ¡ç±»å‹ï¼Œå¦‚ 'AmazonListingJob'
            job_id: ä»»åŠ¡IDï¼Œå¦‚ '1925464883027513344'
            filename: æ–‡ä»¶åï¼Œå¦‚ 'login.gz' æˆ– 'normal.gz'
            decompress: æ˜¯å¦è‡ªåŠ¨è§£å‹ç¼©ï¼Œé»˜è®¤ä¸ºTrue
            
        Returns:
            Union[str, bytes, None]: æ–‡ä»¶å†…å®¹
        """
        container_name = self.storage_config['container_name']
        blob_path = f"{self.storage_config['blob_base_path']}/{task_type}/{job_id}/{filename}"
        
        return self.read_blob_content(container_name, blob_path, decompress=decompress)
    
    def read_amazon_listing_job_both_files(self, job_id: str, 
                                         decompress: bool = True) -> Dict[str, Union[str, bytes, None]]:
        """
        è¯»å–Amazon Listing Jobçš„ä¸¤ä¸ªé»˜è®¤æ–‡ä»¶ï¼šlogin.gz å’Œ normal.gz
        
        Args:
            job_id: ä»»åŠ¡IDï¼Œå¦‚ '1925464883027513344'
            decompress: æ˜¯å¦è‡ªåŠ¨è§£å‹ç¼©ï¼Œé»˜è®¤ä¸ºTrue
            
        Returns:
            Dict[str, Union[str, bytes, None]]: åŒ…å«ä¸¤ä¸ªæ–‡ä»¶å†…å®¹çš„å­—å…¸
        """
        result = {}
        
        # è¯»å–login.gz
        logger.info(f"æ­£åœ¨è¯»å– login.gz æ–‡ä»¶...")
        result['login'] = self.read_amazon_listing_job_file(job_id, 'login.gz', decompress)
        
        # è¯»å–normal.gz
        logger.info(f"æ­£åœ¨è¯»å– normal.gz æ–‡ä»¶...")
        result['normal'] = self.read_amazon_listing_job_file(job_id, 'normal.gz', decompress)
        
        return result
    
    def read_blob_content(self, container_name: str, blob_path: str, 
                         decompress: bool = True) -> Union[str, bytes, None]:
        """
        è¯»å–Blobå†…å®¹
        
        Args:
            container_name: å®¹å™¨åç§°
            blob_path: Blobè·¯å¾„
            decompress: æ˜¯å¦è‡ªåŠ¨è§£å‹ç¼©.gzæ–‡ä»¶ï¼Œé»˜è®¤ä¸ºTrue
            
        Returns:
            Union[str, bytes, None]: æ–‡ä»¶å†…å®¹
        """
        try:
            blob_client = self.blob_service_client.get_blob_client(
                container=container_name,
                blob=blob_path
            )
            
            logger.info(f"æ­£åœ¨è¯»å– Blob: {container_name}/{blob_path}")
            
            # ä¸‹è½½Blobæ•°æ®
            blob_data = blob_client.download_blob().readall()
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦è§£å‹ç¼©
            if decompress and blob_path.endswith('.gz'):
                logger.info("æ£€æµ‹åˆ°.gzæ–‡ä»¶ï¼Œæ­£åœ¨è§£å‹ç¼©...")
                try:
                    # ä½¿ç”¨gzipè§£å‹ç¼©
                    decompressed_data = gzip.decompress(blob_data)
                    # å°è¯•è§£ç ä¸ºUTF-8å­—ç¬¦ä¸²
                    try:
                        content = decompressed_data.decode('utf-8')
                        logger.info(f"âœ… æˆåŠŸè¯»å–å¹¶è§£å‹ç¼©æ–‡ä»¶ï¼Œå†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
                        return content
                    except UnicodeDecodeError:
                        logger.warning("æ— æ³•è§£ç ä¸ºUTF-8ï¼Œè¿”å›åŸå§‹å­—èŠ‚æ•°æ®")
                        logger.info(f"âœ… æˆåŠŸè¯»å–å¹¶è§£å‹ç¼©æ–‡ä»¶ï¼Œæ•°æ®é•¿åº¦: {len(decompressed_data)} å­—èŠ‚")
                        return decompressed_data
                        
                except gzip.BadGzipFile:
                    logger.warning("æ–‡ä»¶ä¸æ˜¯æœ‰æ•ˆçš„gzipæ ¼å¼ï¼Œè¿”å›åŸå§‹æ•°æ®")
                    logger.info(f"âœ… æˆåŠŸè¯»å–æ–‡ä»¶ï¼Œæ•°æ®é•¿åº¦: {len(blob_data)} å­—èŠ‚")
                    return blob_data
            else:
                logger.info(f"âœ… æˆåŠŸè¯»å–æ–‡ä»¶ï¼Œæ•°æ®é•¿åº¦: {len(blob_data)} å­—èŠ‚")
                return blob_data
                
        except ResourceNotFoundError:
            logger.error(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {container_name}/{blob_path}")
            return None
        except Exception as e:
            logger.error(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}")
            return None
    
    def save_blob_to_file(self, container_name: str, blob_path: str, 
                         local_file_path: str, decompress: bool = True) -> bool:
        """
        ä¸‹è½½Blobå¹¶ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶
        
        Args:
            container_name: å®¹å™¨åç§°
            blob_path: Blobè·¯å¾„
            local_file_path: æœ¬åœ°æ–‡ä»¶ä¿å­˜è·¯å¾„
            decompress: æ˜¯å¦è‡ªåŠ¨è§£å‹ç¼©.gzæ–‡ä»¶ï¼Œé»˜è®¤ä¸ºTrue
            
        Returns:
            bool: ä¸‹è½½æˆåŠŸè¿”å›True
        """
        content = self.read_blob_content(container_name, blob_path, decompress=decompress)
        
        if content is None:
            return False
        
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            local_path = Path(local_file_path)
            local_path.parent.mkdir(parents=True, exist_ok=True)
            
            # å†™å…¥æ–‡ä»¶
            if isinstance(content, str):
                with open(local_file_path, 'w', encoding='utf-8') as f:
                    f.write(content)
            else:
                with open(local_file_path, 'wb') as f:
                    f.write(content)
            
            logger.info(f"âœ… æ–‡ä»¶ä¿å­˜æˆåŠŸ: {local_file_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {str(e)}")
            return False
    
    def list_amazon_listing_jobs(self, limit: int = 100) -> List[Dict]:
        """
        åˆ—å‡ºAmazon Listing Jobç›®å½•
        
        Args:
            limit: é™åˆ¶è¿”å›çš„æ•°é‡ï¼Œé»˜è®¤100
            
        Returns:
            List[Dict]: ä»»åŠ¡ä¿¡æ¯åˆ—è¡¨
        """
        container_name = self.storage_config['container_name']
        prefix = f"{self.storage_config['blob_base_path']}/AmazonListingJob/"
        
        return self.list_blobs_with_prefix(container_name, prefix, limit)
    
    def list_task_jobs(self, task_type: str, limit: int = 100) -> List[Dict]:
        """
        åˆ—å‡ºæŒ‡å®šä»»åŠ¡ç±»å‹çš„æ‰€æœ‰ä»»åŠ¡
        
        Args:
            task_type: ä»»åŠ¡ç±»å‹ï¼Œå¦‚ 'AmazonListingJob'
            limit: é™åˆ¶è¿”å›çš„æ•°é‡ï¼Œé»˜è®¤100
            
        Returns:
            List[Dict]: ä»»åŠ¡ä¿¡æ¯åˆ—è¡¨
        """
        container_name = self.storage_config['container_name']
        prefix = f"{self.storage_config['blob_base_path']}/{task_type}/"
        
        return self.list_blobs_with_prefix(container_name, prefix, limit)
    
    def list_blobs_with_prefix(self, container_name: str, prefix: str, 
                              limit: int = 100) -> List[Dict]:
        """
        åˆ—å‡ºæŒ‡å®šå‰ç¼€çš„Blob
        
        Args:
            container_name: å®¹å™¨åç§°
            prefix: Blobè·¯å¾„å‰ç¼€
            limit: é™åˆ¶è¿”å›çš„æ•°é‡
            
        Returns:
            List[Dict]: Blobä¿¡æ¯åˆ—è¡¨
        """
        try:
            container_client = self.blob_service_client.get_container_client(container_name)
            blobs = []
            
            count = 0
            for blob in container_client.list_blobs(name_starts_with=prefix):
                if count >= limit:
                    break
                    
                blobs.append({
                    'name': blob.name,
                    'size': blob.size,
                    'last_modified': blob.last_modified,
                    'content_type': blob.content_settings.content_type if blob.content_settings else None,
                    'path_parts': blob.name.split('/'),
                    'job_id': self._extract_job_id_from_path(blob.name)
                })
                count += 1
            
            logger.info(f"åˆ—å‡º {len(blobs)} ä¸ªBlob (å‰ç¼€: {prefix})")
            return blobs
            
        except Exception as e:
            logger.error(f"åˆ—å‡ºBlobå¤±è´¥: {str(e)}")
            return []
    
    def _extract_job_id_from_path(self, blob_path: str) -> Optional[str]:
        """
        ä»Blobè·¯å¾„ä¸­æå–Job ID
        
        Args:
            blob_path: Blobè·¯å¾„ï¼Œå¦‚ "compress/AmazonListingJob/1925464883027513344/login.gz"
            
        Returns:
            Optional[str]: Job ID
        """
        try:
            parts = blob_path.split('/')
            # æ–°è·¯å¾„ç»“æ„: compress/TaskType/JobID/filename
            if len(parts) >= 4 and parts[0] == 'compress':
                return parts[2]  # JobIDåœ¨ç¬¬3ä¸ªä½ç½®ï¼ˆç´¢å¼•2ï¼‰
        except:
            pass
        return None
    
    def get_blob_info(self, container_name: str, blob_path: str) -> Optional[Dict]:
        """
        è·å–Blobè¯¦ç»†ä¿¡æ¯
        
        Args:
            container_name: å®¹å™¨åç§°
            blob_path: Blobè·¯å¾„
            
        Returns:
            Optional[Dict]: Blobä¿¡æ¯å­—å…¸
        """
        try:
            blob_client = self.blob_service_client.get_blob_client(
                container=container_name,
                blob=blob_path
            )
            
            properties = blob_client.get_blob_properties()
            
            return {
                'name': blob_path,
                'size': properties.size,
                'size_mb': round(properties.size / (1024 * 1024), 2),
                'content_type': properties.content_settings.content_type,
                'last_modified': properties.last_modified,
                'etag': properties.etag,
                'metadata': properties.metadata or {},
                'creation_time': properties.creation_time,
                'blob_type': properties.blob_type,
                'url': f"{self.account_url}/{container_name}/{blob_path}"
            }
            
        except Exception as e:
            logger.error(f"è·å–Blobä¿¡æ¯å¤±è´¥: {str(e)}")
            return None
    
    def read_parse_file(self, task_type: str, task_id: str, filename: str = None, 
                       decompress: bool = True) -> Union[str, bytes, None]:
        """
        è¯»å–è§£ææ–‡ä»¶ï¼ˆcollector0109è´¦æˆ·ï¼‰
        
        Args:
            task_type: ä»»åŠ¡ç±»å‹ï¼ˆå¦‚: AmazonListingJobï¼‰
            task_id: ä»»åŠ¡IDï¼ˆå¦‚: 1910599147004108800ï¼‰
            filename: æ–‡ä»¶åï¼Œå¦‚æœä¸æŒ‡å®šåˆ™å°è¯•æŸ¥æ‰¾JSONæ–‡ä»¶
            decompress: æ˜¯å¦è‡ªåŠ¨è§£å‹ç¼©ï¼Œé»˜è®¤ä¸ºTrue
            
        Returns:
            Union[str, bytes, None]: æ–‡ä»¶å†…å®¹
        """
        if self.account_name != 'collector0109':
            logger.error("è¯»å–è§£ææ–‡ä»¶éœ€è¦ä½¿ç”¨collector0109å­˜å‚¨è´¦æˆ·")
            return None
        
        container_name = self.storage_config['container_name']
        
        # è·¯å¾„ç»“æ„ï¼šparse/{task_type}/{task_id}/*
        blob_prefix = f"{self.storage_config['blob_base_path']}/{task_type}/{task_id}/"
        
        if filename:
            # æŒ‡å®šæ–‡ä»¶å
            blob_path = f"{blob_prefix}{filename}"
            return self.read_blob_content(container_name, blob_path, decompress=decompress)
        else:
            # è‡ªåŠ¨æŸ¥æ‰¾JSONæ–‡ä»¶
            return self._auto_find_parse_file(container_name, blob_prefix, decompress)
    
    def _auto_find_parse_file(self, container_name: str, blob_prefix: str, 
                             decompress: bool = True) -> Union[str, bytes, None]:
        """
        è‡ªåŠ¨æŸ¥æ‰¾è§£ææ–‡ä»¶ï¼ˆä¼˜å…ˆJSONæ–‡ä»¶ï¼‰
        
        Args:
            container_name: å®¹å™¨å
            blob_prefix: Blobè·¯å¾„å‰ç¼€
            decompress: æ˜¯å¦è§£å‹ç¼©
            
        Returns:
            Union[str, bytes, None]: æ–‡ä»¶å†…å®¹
        """
        try:
            container_client = self.blob_service_client.get_container_client(container_name)
            
            # æŸ¥æ‰¾æ‰€æœ‰æ–‡ä»¶
            blobs = list(container_client.list_blobs(name_starts_with=blob_prefix))
            
            if not blobs:
                logger.warning(f"æœªæ‰¾åˆ°ä»»ä½•æ–‡ä»¶ï¼Œè·¯å¾„å‰ç¼€: {blob_prefix}")
                return None
            
            # ä¼˜å…ˆçº§ï¼š.json > .json.gz > å…¶ä»–
            json_files = [b for b in blobs if b.name.endswith('.json')]
            json_gz_files = [b for b in blobs if b.name.endswith('.json.gz')]
            other_files = [b for b in blobs if not (b.name.endswith('.json') or b.name.endswith('.json.gz'))]
            
            target_blob = None
            if json_files:
                target_blob = json_files[0]
                logger.info(f"æ‰¾åˆ°JSONæ–‡ä»¶: {target_blob.name}")
            elif json_gz_files:
                target_blob = json_gz_files[0]
                logger.info(f"æ‰¾åˆ°å‹ç¼©JSONæ–‡ä»¶: {target_blob.name}")
            elif other_files:
                target_blob = other_files[0]
                logger.info(f"æ‰¾åˆ°å…¶ä»–æ–‡ä»¶: {target_blob.name}")
            
            if target_blob:
                return self.read_blob_content(container_name, target_blob.name, decompress=decompress)
            else:
                logger.warning("æœªæ‰¾åˆ°åˆé€‚çš„è§£ææ–‡ä»¶")
                return None
                
        except Exception as e:
            logger.error(f"è‡ªåŠ¨æŸ¥æ‰¾è§£ææ–‡ä»¶å¤±è´¥: {str(e)}")
            return None
    
    def list_parse_files(self, task_type: str, task_id: str) -> List[Dict]:
        """
        åˆ—å‡ºè§£ææ–‡ä»¶ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
        
        Args:
            task_type: ä»»åŠ¡ç±»å‹ï¼ˆå¦‚: AmazonListingJobï¼‰
            task_id: ä»»åŠ¡ID
            
        Returns:
            List[Dict]: æ–‡ä»¶ä¿¡æ¯åˆ—è¡¨
        """
        if self.account_name != 'collector0109':
            logger.error("åˆ—å‡ºè§£ææ–‡ä»¶éœ€è¦ä½¿ç”¨collector0109å­˜å‚¨è´¦æˆ·")
            return []
        
        container_name = self.storage_config['container_name']
        prefix = f"{self.storage_config['blob_base_path']}/{task_type}/{task_id}/"
        
        return self.list_blobs_with_prefix(container_name, prefix, limit=100)

    def read_task_file_with_parse(self, task_type: str, task_id: str, filename: str, 
                                 decompress: bool = True) -> Dict[str, Union[str, bytes, None]]:
        """
        åŒæ—¶è¯»å–åŸå§‹ä»»åŠ¡æ–‡ä»¶å’Œè§£ææ–‡ä»¶
        
        Args:
            task_type: ä»»åŠ¡ç±»å‹ï¼Œå¦‚ 'AmazonListingJob'
            task_id: ä»»åŠ¡IDï¼Œå¦‚ '1925464883027513344'
            filename: åŸå§‹æ–‡ä»¶åï¼Œå¦‚ 'login.gz' æˆ– 'normal.gz'
            decompress: æ˜¯å¦è‡ªåŠ¨è§£å‹ç¼©ï¼Œé»˜è®¤ä¸ºTrue
            
        Returns:
            Dict[str, Union[str, bytes, None]]: åŒ…å«åŸå§‹æ–‡ä»¶å’Œè§£ææ–‡ä»¶å†…å®¹çš„å­—å…¸
        """
        result = {}
        
        # è¯»å–åŸå§‹æ–‡ä»¶ï¼ˆyiya0110ï¼‰
        if self.account_name == 'yiya0110':
            logger.info(f"æ­£åœ¨è¯»å–åŸå§‹æ–‡ä»¶: {filename}")
            result['original'] = self.read_task_file(task_type, task_id, filename, decompress)
            
            # åˆ›å»ºcollector0109è¯»å–å™¨æ¥è¯»å–è§£ææ–‡ä»¶
            parse_reader = AzureResourceReader('collector0109')
            logger.info(f"æ­£åœ¨è¯»å–è§£ææ–‡ä»¶...")
            result['parse'] = parse_reader.read_parse_file(task_type, task_id, None, decompress)
            
        else:
            logger.warning("æ­¤æ–¹æ³•éœ€è¦ä½¿ç”¨yiya0110è´¦æˆ·ä½œä¸ºä¸»è¯»å–å™¨")
            result['original'] = None
            result['parse'] = None
            
        return result

    def fetch_and_save_parse_files(self, task_type: str, task_id: str, 
                                  save_dir: str = 'data/output', 
                                  decompress: bool = True) -> Dict[str, any]:
        """
        ä»collector0109è·å–è§£ææ–‡ä»¶å¹¶ä¿å­˜åˆ°æŒ‡å®šæ–‡ä»¶å¤¹
        æ ¹æ®æµ‹è¯•ç»“æœä¼˜åŒ–ï¼Œä»…ä½¿ç”¨æ­£ç¡®çš„è·¯å¾„ï¼šparse/{task_type}/{task_id}/
        
        Args:
            task_type: ä»»åŠ¡ç±»å‹ï¼ˆå¦‚: AmazonReviewStarJobï¼‰
            task_id: ä»»åŠ¡IDï¼ˆå¦‚: 1887037115222994944ï¼‰
            save_dir: ä¿å­˜ç›®å½•ï¼Œé»˜è®¤: data/output
            decompress: æ˜¯å¦è‡ªåŠ¨è§£å‹ç¼©ï¼Œé»˜è®¤ä¸ºTrue
            
        Returns:
            Dict: åŒ…å«æ“ä½œç»“æœçš„å­—å…¸
        """
        if self.account_name != 'collector0109':
            logger.error("è·å–è§£ææ–‡ä»¶éœ€è¦ä½¿ç”¨collector0109å­˜å‚¨è´¦æˆ·")
            return {
                'success': False,
                'error': 'éœ€è¦ä½¿ç”¨collector0109å­˜å‚¨è´¦æˆ·',
                'files_downloaded': [],
                'save_path': None
            }
        
        logger.info(f"ğŸ” æ­£åœ¨ä»collector0109è·å–è§£ææ–‡ä»¶")
        logger.info(f"ğŸ“‹ ä»»åŠ¡ç±»å‹: {task_type}")
        logger.info(f"ğŸ“‹ ä»»åŠ¡ID: {task_id}")
        
        container_name = self.storage_config['container_name']
        # æ ¹æ®æµ‹è¯•ç»“æœï¼Œä½¿ç”¨æ­£ç¡®çš„è·¯å¾„ç»“æ„ï¼šparse/{task_type}/{task_id}/
        blob_prefix = f"{self.storage_config['blob_base_path']}/{task_type}/{task_id}/"
        
        logger.info(f"ğŸ“ æœç´¢è·¯å¾„: {blob_prefix}")
        
        try:
            container_client = self.blob_service_client.get_container_client(container_name)
            
            # æŸ¥æ‰¾æ‰€æœ‰æ–‡ä»¶
            blobs = list(container_client.list_blobs(name_starts_with=blob_prefix))
            
            if not blobs:
                logger.warning(f"âŒ æœªæ‰¾åˆ°ä»»ä½•è§£ææ–‡ä»¶ï¼Œè·¯å¾„: {blob_prefix}")
                return {
                    'success': False,
                    'error': f'æœªæ‰¾åˆ°è§£ææ–‡ä»¶ï¼Œè·¯å¾„: {blob_prefix}',
                    'files_downloaded': [],
                    'save_path': None
                }
            
            logger.info(f"âœ… æ‰¾åˆ° {len(blobs)} ä¸ªè§£ææ–‡ä»¶")
            
            # åˆ›å»ºä¿å­˜ç›®å½•
            save_path = Path(save_dir) / 'parse' / task_type / task_id
            save_path.mkdir(parents=True, exist_ok=True)
            logger.info(f"ğŸ“‚ ä¿å­˜ç›®å½•: {save_path}")
            
            downloaded_files = []
            
            # ä¼˜å…ˆçº§å¤„ç†ï¼šJSON > JSON.GZ > å…¶ä»–
            json_files = [b for b in blobs if b.name.endswith('.json')]
            json_gz_files = [b for b in blobs if b.name.endswith('.json.gz')]
            other_files = [b for b in blobs if not (b.name.endswith('.json') or b.name.endswith('.json.gz'))]
            
            # æŒ‰ä¼˜å…ˆçº§å¤„ç†æ–‡ä»¶
            files_to_process = []
            if json_files:
                files_to_process.extend(json_files)
                logger.info(f"ğŸ“„ æ‰¾åˆ° {len(json_files)} ä¸ªJSONæ–‡ä»¶")
            if json_gz_files:
                files_to_process.extend(json_gz_files)
                logger.info(f"ğŸ“„ æ‰¾åˆ° {len(json_gz_files)} ä¸ªå‹ç¼©JSONæ–‡ä»¶")
            if other_files:
                files_to_process.extend(other_files)
                logger.info(f"ğŸ“„ æ‰¾åˆ° {len(other_files)} ä¸ªå…¶ä»–æ–‡ä»¶")
            
            # ä¸‹è½½æ‰€æœ‰æ–‡ä»¶
            for blob in files_to_process:
                try:
                    logger.info(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½: {blob.name}")
                    
                    # è¯»å–æ–‡ä»¶å†…å®¹
                    content = self.read_blob_content(container_name, blob.name, decompress=decompress)
                    
                    if content is not None:
                        # ç”Ÿæˆä¿å­˜æ–‡ä»¶åï¼Œç¡®ä¿JSONæ–‡ä»¶ä¿æŒ.jsonæ‰©å±•å
                        file_name = Path(blob.name).name
                        if decompress and blob.name.endswith('.gz'):
                            # å¦‚æœè§£å‹ç¼©äº†ï¼Œç§»é™¤.gzæ‰©å±•å
                            file_name = file_name.replace('.gz', '')
                        
                        # ç¡®ä¿JSONæ–‡ä»¶å§‹ç»ˆä¿æŒ.jsonæ‰©å±•å
                        if file_name.endswith('.json') or blob.name.endswith('.json') or blob.name.endswith('.json.gz'):
                            if not file_name.endswith('.json'):
                                file_name = file_name.rsplit('.', 1)[0] + '.json'
                        
                        local_file_path = save_path / file_name
                        
                        # ä¿å­˜æ–‡ä»¶
                        success = self._save_content_to_file(content, str(local_file_path))
                        
                        if success:
                            downloaded_files.append({
                                'original_name': blob.name,
                                'saved_name': file_name,
                                'local_path': str(local_file_path),
                                'size': blob.size,
                                'content_length': len(content) if isinstance(content, str) else len(content)
                            })
                            logger.info(f"âœ… å·²ä¿å­˜: {local_file_path}")
                            
                            # å¦‚æœæ˜¯JSONæ–‡ä»¶ï¼Œæ˜¾ç¤ºé¢„è§ˆ
                            if file_name.endswith('.json') and isinstance(content, str):
                                try:
                                    import json as json_module
                                    parsed_data = json_module.loads(content)
                                    logger.info(f"ğŸ“‹ JSONè§£ææˆåŠŸï¼Œç±»å‹: {type(parsed_data)}")
                                    if isinstance(parsed_data, dict):
                                        logger.info(f"ğŸ”‘ JSONé”®: {list(parsed_data.keys())[:5]}...")
                                    elif isinstance(parsed_data, list):
                                        logger.info(f"ğŸ“Š JSONæ•°ç»„é•¿åº¦: {len(parsed_data)}")
                                except:
                                    logger.info(f"ğŸ“„ JSONæ–‡ä»¶å·²ä¿å­˜ï¼Œä½†æ— æ³•è§£æå†…å®¹")
                        else:
                            logger.error(f"âŒ ä¿å­˜å¤±è´¥: {blob.name}")
                    else:
                        logger.error(f"âŒ è¯»å–å¤±è´¥: {blob.name}")
                        
                except Exception as e:
                    logger.error(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ {blob.name}: {str(e)}")
            
            # è¿”å›ç»“æœ
            result = {
                'success': len(downloaded_files) > 0,
                'files_downloaded': downloaded_files,
                'save_path': str(save_path),
                'total_files_found': len(blobs),
                'total_files_downloaded': len(downloaded_files)
            }
            
            if downloaded_files:
                logger.info(f"âœ… æˆåŠŸä¸‹è½½ {len(downloaded_files)} ä¸ªè§£ææ–‡ä»¶åˆ°: {save_path}")
            else:
                logger.warning(f"âš ï¸  æœªèƒ½ä¸‹è½½ä»»ä½•æ–‡ä»¶")
                result['error'] = 'æœªèƒ½ä¸‹è½½ä»»ä½•æ–‡ä»¶'
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ è·å–è§£ææ–‡ä»¶å¤±è´¥: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'files_downloaded': [],
                'save_path': None
            }
    
    def _save_content_to_file(self, content: Union[str, bytes], file_path: str) -> bool:
        """
        ä¿å­˜å†…å®¹åˆ°æ–‡ä»¶ï¼ˆå†…éƒ¨æ–¹æ³•ï¼‰
        
        Args:
            content: æ–‡ä»¶å†…å®¹
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: ä¿å­˜æˆåŠŸè¿”å›True
        """
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            local_path = Path(file_path)
            local_path.parent.mkdir(parents=True, exist_ok=True)
            
            # å†™å…¥æ–‡ä»¶
            if isinstance(content, str):
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(content)
            else:
                with open(file_path, 'wb') as f:
                    f.write(content)
            
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {str(e)}")
            return False


def main():
    """ä¸»å‡½æ•°ï¼šå¤„ç†å‘½ä»¤è¡Œå‚æ•°å¹¶æ‰§è¡Œç›¸åº”æ“ä½œ"""
    parser = argparse.ArgumentParser(
        description='Azure Storage èµ„æºè¯»å–å™¨',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # è¯»å–åŸå§‹æ•°æ®æ–‡ä»¶ï¼ˆyiya0110è´¦æˆ·ï¼‰
  python3 src/azure_resource_reader.py AmazonListingJob 1925464883027513344 html
  python3 src/azure_resource_reader.py AmazonReviewStarJob 2796867471 html
  
  # ğŸ†• ç›´æ¥è·å–è§£ææ–‡ä»¶ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼Œæ¨èï¼‰
  python3 src/azure_resource_reader.py AmazonReviewStarJob 1887037115222994944 --fetch-parse
  python3 src/azure_resource_reader.py AmazonListingJob 1925464883027513344 --fetch-parse
  
  # ğŸ†• åŒæ—¶è¯»å–åŸå§‹æ•°æ®å’Œè§£ææ•°æ®
  python3 src/azure_resource_reader.py AmazonReviewStarJob 2796867471 html --with-parse
  python3 src/azure_resource_reader.py AmazonListingJob 1925464883027513344 json --with-parse
  
  # è¯»å–è§£ææ–‡ä»¶ï¼ˆä¼ ç»Ÿæ–¹å¼ï¼‰
  python3 src/azure_resource_reader.py --account collector0109 --parse-mode SL2796867471 1910599147004108800 json
  python3 src/azure_resource_reader.py --account collector0109 --parse-mode SL2796867471 1910599147004108800 json --files result.json
  
  # æŸ¥çœ‹å½“å‰çš„ä»»åŠ¡æ˜ å°„
  python3 src/azure_resource_reader.py --show-mapping
  
  # ç¦ç”¨æ˜ å°„æ–‡ä»¶ç”Ÿæˆ
  python3 src/azure_resource_reader.py AmazonListingJob 2796867471 html --no-mapping
  
  # å¯ç”¨è¯¦ç»†æ—¥å¿—è¾“å‡ºï¼ˆåŒ…æ‹¬HTTPè¯·æ±‚è¯¦æƒ…ï¼‰
  python3 src/azure_resource_reader.py AmazonListingJob 2834468425 html --with-parse --verbose
  
æ˜ å°„åŠŸèƒ½è¯´æ˜:
  - æ¯æ¬¡æˆåŠŸä¸‹è½½æ–‡ä»¶åï¼Œä¼šåœ¨ data/output/task_mapping.json ä¸­è®°å½•æ˜ å°„å…³ç³»
  - æ˜ å°„æ ¼å¼: è¾“å…¥å‚æ•° -> å®é™…ä¸‹è½½è·¯å¾„
  - ä¾‹å¦‚: 2796867471 -> ./AmazonReviewStarJob/1910599147004108800/*
  - ğŸ†• --with-parseæ¨¡å¼: åŸå§‹æ–‡ä»¶å’Œè§£ææ–‡ä»¶ä¿å­˜åœ¨åŒä¸€ç›®å½•ï¼Œä½¿ç”¨ç»Ÿä¸€æ˜ å°„
  - ğŸ†• --fetch-parseæ¨¡å¼: ä»…è§£ææ–‡ä»¶ï¼Œå•ç‹¬æ˜ å°„åˆ° ./parse/{ä»»åŠ¡ç±»å‹}/{ä»»åŠ¡ID}/
  - è§£ææ¨¡å¼: SL2796867471 -> ./parse/SL2796867471/1910599147004108800/*
        """
    )
    
    parser.add_argument('task_type_or_job_id', 
                       nargs='?',
                       help='ä»»åŠ¡ç±»å‹ï¼ˆå¦‚: AmazonListingJobï¼‰æˆ–job_idï¼ˆå¦‚: 2841227686 æˆ– SL2796867471ï¼‰')
    parser.add_argument('task_id_or_output_type', 
                       nargs='?',
                       help='ä»»åŠ¡IDï¼ˆé•¿æ•°å­—ä¸²ï¼‰æˆ–è¾“å‡ºç±»å‹ï¼ˆå½“ç¬¬ä¸€ä¸ªå‚æ•°ä¸ºjob_idæ—¶ï¼‰')
    parser.add_argument('output_type_or_extra', 
                       nargs='?',
                       choices=['html', 'txt', 'json', 'raw'],
                       help='è¾“å‡ºæ–‡ä»¶ç±»å‹: html(è‡ªåŠ¨è§£å‹), txt(è‡ªåŠ¨è§£å‹), json(è‡ªåŠ¨è§£å‹), raw(ä¸è§£å‹)')
    
    parser.add_argument('--account', '-a',
                       choices=['yiya0110', 'collector0109'],
                       default='yiya0110',
                       help='Azureå­˜å‚¨è´¦æˆ·: yiya0110(åŸå§‹æ•°æ®), collector0109(è§£ææ•°æ®)')
    parser.add_argument('--parse-mode', '-p',
                       action='store_true',
                       help='è§£ææ–‡ä»¶æ¨¡å¼ï¼ˆéœ€è¦é…åˆ--account collector0109ä½¿ç”¨ï¼‰')
    parser.add_argument('--with-parse', 
                       action='store_true',
                       help='åŒæ—¶è·å–åŸå§‹æ•°æ®å’Œè§£ææ•°æ®ï¼ˆè‡ªåŠ¨ä½¿ç”¨yiya0110è¯»å–åŸå§‹æ•°æ®ï¼Œcollector0109è¯»å–è§£ææ•°æ®ï¼‰')
    parser.add_argument('--fetch-parse', 
                       action='store_true',
                       help='ğŸ†• ç›´æ¥ä»collector0109è·å–è§£ææ–‡ä»¶ï¼ˆä¼˜åŒ–è·¯å¾„ï¼ŒåŸºäºæµ‹è¯•ç»“æœï¼‰')
    parser.add_argument('--files', '-f',
                       nargs='+',
                       default=None,
                       help='è¦è¯»å–çš„æ–‡ä»¶åˆ—è¡¨ï¼Œè§£ææ¨¡å¼ä¸‹é»˜è®¤è‡ªåŠ¨æŸ¥æ‰¾JSONæ–‡ä»¶')
    parser.add_argument('--save-dir', '-s',
                       default='data/output',
                       help='ä¿å­˜ç›®å½•ï¼Œé»˜è®¤: data/output')
    parser.add_argument('--info-only', '-i',
                       action='store_true',
                       help='ä»…æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯ï¼Œä¸ä¸‹è½½å†…å®¹')
    parser.add_argument('--list-jobs', '-l',
                       action='store_true',
                       help='åˆ—å‡ºæŒ‡å®šä»»åŠ¡ç±»å‹çš„æ‰€æœ‰ä»»åŠ¡')
    parser.add_argument('--no-mapping', 
                       action='store_true',
                       help='ä¸ç”Ÿæˆä»»åŠ¡æ˜ å°„æ–‡ä»¶')
    parser.add_argument('--show-mapping', 
                       action='store_true',
                       help='æ˜¾ç¤ºå½“å‰çš„ä»»åŠ¡æ˜ å°„æ–‡ä»¶å†…å®¹')
    parser.add_argument('--verbose', '-v',
                       action='store_true',
                       help='å¯ç”¨è¯¦ç»†æ—¥å¿—è¾“å‡ºï¼ˆåŒ…æ‹¬HTTPè¯·æ±‚è¯¦æƒ…ï¼‰')
    
    args = parser.parse_args()
    
    # æ ¹æ®verboseå‚æ•°è®¾ç½®æ—¥å¿—çº§åˆ«
    setup_logging(verbose=args.verbose)
    
    # å¦‚æœåªæ˜¯æ˜¾ç¤ºæ˜ å°„æ–‡ä»¶å†…å®¹
    if args.show_mapping:
        show_task_mapping(args.save_dir)
        return
    
    # ğŸ†• æ™ºèƒ½å‚æ•°è§£æï¼šè‡ªåŠ¨æ£€æµ‹æ˜¯å¦ä¸ºç®€åŒ–æ¨¡å¼
    is_smart_mode = False
    actual_task_type = None
    actual_task_id = None
    actual_output_type = None
    
    # æ£€æŸ¥ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯å¦ä¸ºçº¯æ•°å­—ï¼ˆjob_idæ¨¡å¼ï¼‰
    if args.task_type_or_job_id and args.task_type_or_job_id.isdigit():
        # ç®€åŒ–æ¨¡å¼ï¼špython3 script.py 2841227686 html --with-parse
        is_smart_mode = True
        job_id = args.task_type_or_job_id
        actual_output_type = args.task_id_or_output_type or 'html'
        
        print(f"ğŸš€ æ™ºèƒ½æ¨¡å¼ï¼šè‡ªåŠ¨è¯†åˆ«ä»»åŠ¡ç±»å‹")
        print(f"ğŸ“‹ Job ID: {job_id}")
        print(f"ğŸ“‹ è¾“å‡ºç±»å‹: {actual_output_type}")
        print("ğŸ” æ­£åœ¨æŸ¥è¯¢ä»»åŠ¡ç±»å‹...")
        
        # æŸ¥è¯¢ä»»åŠ¡ç±»å‹
        actual_task_type = get_task_type_by_job_id(job_id)
        if not actual_task_type:
            print(f"âŒ æ— æ³•æ‰¾åˆ° job_id {job_id} å¯¹åº”çš„ä»»åŠ¡ç±»å‹")
            return
            
        print(f"âœ… å·²è¯†åˆ«ä»»åŠ¡ç±»å‹: {actual_task_type}")
        
        # è½¬æ¢ä¸ºä»»åŠ¡ID
        if job_id.isdigit():
            prefixed_job_id = f"SL{job_id}"
        else:
            prefixed_job_id = job_id
            
        actual_task_id = convert_job_id_to_task_id(prefixed_job_id)
        if not actual_task_id:
            print(f"âŒ æ— æ³•æ‰¾åˆ°å¯¹åº”çš„ä»»åŠ¡IDï¼Œè¯·æ£€æŸ¥ job_id: {prefixed_job_id}")
            return
            
        print(f"âœ… å·²è·å–ä»»åŠ¡ID: {actual_task_id}")
        print("=" * 80)
        
        # é‡æ–°ç»„ç»‡argså¯¹è±¡ä»¥å…¼å®¹ç°æœ‰é€»è¾‘
        args.task_type_or_job_id = actual_task_type
        args.task_id_or_task_id = actual_task_id
        args.output_type = actual_output_type
    else:
        # ä¼ ç»Ÿæ¨¡å¼ï¼špython3 script.py AmazonListingJob 1234567890123456 html --with-parse
        args.task_id_or_task_id = args.task_id_or_output_type
        args.output_type = args.output_type_or_extra

    # ğŸ†• å¤„ç† --fetch-parse æ¨¡å¼
    if args.fetch_parse:
        if not args.task_type_or_job_id or not args.task_id_or_task_id:
            parser.error("--fetch-parse æ¨¡å¼éœ€è¦æä¾› task_type å’Œ task_id å‚æ•°")
        
        task_type = args.task_type_or_job_id
        task_id = args.task_id_or_task_id
        
        print(f"ğŸ” Azure Storage è§£ææ–‡ä»¶è·å–å™¨ (ä¼˜åŒ–ç‰ˆæœ¬)")
        print(f"ğŸ“‹ ä»»åŠ¡ç±»å‹: {task_type}")
        print(f"ğŸ“‹ ä»»åŠ¡ID: {task_id}")
        print(f"ğŸ“ æœç´¢è·¯å¾„: collector0109/parse/{task_type}/{task_id}/")
        print("=" * 80)
        
        # åˆ›å»ºcollector0109è¯»å–å™¨
        reader = AzureResourceReader('collector0109')
        
        # ä½¿ç”¨ä¼˜åŒ–çš„æ–¹æ³•è·å–è§£ææ–‡ä»¶
        result = reader.fetch_and_save_parse_files(
            task_type=task_type,
            task_id=task_id,
            save_dir=args.save_dir,
            decompress=True
        )
        
        # æ˜¾ç¤ºç»“æœ
        if result['success']:
            print(f"\nâœ… è§£ææ–‡ä»¶è·å–æˆåŠŸ!")
            print(f"ğŸ“‚ ä¿å­˜è·¯å¾„: {result['save_path']}")
            print(f"ğŸ“Š æ‰¾åˆ°æ–‡ä»¶æ•°: {result['total_files_found']}")
            print(f"ğŸ“¥ ä¸‹è½½æ–‡ä»¶æ•°: {result['total_files_downloaded']}")
            
            if result['files_downloaded']:
                print(f"\nğŸ“„ å·²ä¸‹è½½çš„æ–‡ä»¶:")
                for file_info in result['files_downloaded']:
                    print(f"  âœ… {file_info['saved_name']}")
                    print(f"     ğŸ“Š å¤§å°: {file_info['size']} å­—èŠ‚")
                    print(f"     ğŸ“ è·¯å¾„: {file_info['local_path']}")
                    print()
        else:
            print(f"\nâŒ è§£ææ–‡ä»¶è·å–å¤±è´¥!")
            print(f"ğŸ” é”™è¯¯ä¿¡æ¯: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        
        return
    
    # æ£€æŸ¥å¿…éœ€å‚æ•°
    if not args.task_type_or_job_id or not args.task_id_or_task_id or not args.output_type:
        if is_smart_mode:
            parser.error("æ™ºèƒ½æ¨¡å¼éœ€è¦æä¾›: job_id output_type [--with-parse]")
        else:
            parser.error("ä¼ ç»Ÿæ¨¡å¼éœ€è¦æä¾›: task_type task_id output_type [--with-parse]")
    
    # ä¿å­˜åŸå§‹è¾“å…¥å‚æ•°ç”¨äºæ˜ å°„
    if is_smart_mode:
        original_input = job_id  # æ™ºèƒ½æ¨¡å¼ä½¿ç”¨job_idä½œä¸ºåŸå§‹è¾“å…¥
    else:
        original_input = args.task_id_or_task_id
    
    # æ£€æŸ¥è§£ææ¨¡å¼çš„å‚æ•°ä¸€è‡´æ€§
    if args.parse_mode and args.account != 'collector0109':
        parser.error("è§£ææ¨¡å¼å¿…é¡»ä½¿ç”¨ --account collector0109")
    
    if args.account == 'collector0109' and not args.parse_mode:
        print("âš ï¸  ä½¿ç”¨collector0109è´¦æˆ·ä½†æœªå¯ç”¨è§£ææ¨¡å¼ï¼Œå°†è‡ªåŠ¨å¯ç”¨è§£ææ¨¡å¼")
        args.parse_mode = True
    
    # æ£€æŸ¥--with-parseå‚æ•°
    if args.with_parse:
        if args.account != 'yiya0110':
            print("âš ï¸  --with-parse æ¨¡å¼å°†è‡ªåŠ¨ä½¿ç”¨yiya0110ä½œä¸ºä¸»è´¦æˆ·")
            args.account = 'yiya0110'
        
        # å¤„ç†åŒæ—¶è·å–åŸå§‹æ•°æ®å’Œè§£ææ•°æ®çš„æ¨¡å¼
        handle_with_parse_mode(args)
        return
    
    # è§£ææ¨¡å¼ç‰¹æ®Šå¤„ç†
    if args.parse_mode:
        print(f"ğŸ” Azure Storage èµ„æºè¯»å–å™¨ (è§£ææ¨¡å¼)")
        print(f"ğŸ“‹ Job ID: {args.task_type_or_job_id}")
        print(f"ğŸ“‹ Task ID: {args.task_id_or_task_id}")
        
        job_id = args.task_type_or_job_id
        task_id = args.task_id_or_task_id
        original_input = f"{job_id}:{task_id}"
        
        print(f"ğŸ“ è§£ææ•°æ®è·¯å¾„: collector0109/parse/{job_id}/{task_id}/")
        print("=" * 80)
        
        # åˆ›å»ºèµ„æºè¯»å–å™¨
        reader = AzureResourceReader(args.account)
        
        # å¤„ç†è§£ææ–‡ä»¶
        handle_parse_mode(reader, job_id, task_id, args, original_input)
        return
    
    # åŸå§‹æ¨¡å¼å¤„ç†
    # ç¬¬ä¸€æ­¥ï¼šéªŒè¯å’Œè½¬æ¢ä»»åŠ¡ID
    if not is_smart_mode:
        print(f"ğŸ” Azure Storage èµ„æºè¯»å–å™¨")
        print(f"ğŸ“‹ ä»»åŠ¡ç±»å‹: {args.task_type_or_job_id}")
        print(f"ğŸ“‹ è¾“å…¥å‚æ•°: {args.task_id_or_task_id}")
    
    # æ™ºèƒ½æ¨¡å¼å·²ç»å®Œæˆäº†ä»»åŠ¡IDçš„è½¬æ¢ï¼Œä¼ ç»Ÿæ¨¡å¼éœ€è¦è¿›è¡Œè½¬æ¢
    if is_smart_mode:
        task_id = actual_task_id
    else:
        # æ£€æŸ¥è¾“å…¥æ˜¯å¦ä¸ºæœ‰æ•ˆçš„ä»»åŠ¡ID
        if is_valid_task_id(args.task_id_or_task_id):
            # ç›´æ¥ä½¿ç”¨ä½œä¸ºä»»åŠ¡ID
            task_id = args.task_id_or_task_id
            print(f"âœ… æ£€æµ‹åˆ°æœ‰æ•ˆçš„ä»»åŠ¡ID: {task_id}")
        else:
            # éœ€è¦è½¬æ¢ä¸ºä»»åŠ¡ID
            job_id = args.task_id_or_task_id
            
            # å¦‚æœæ˜¯çº¯æ•°å­—ï¼Œæ·»åŠ SLå‰ç¼€
            if job_id.isdigit():
                job_id = f"SL{job_id}"
                print(f"ğŸ”„ æ·»åŠ SLå‰ç¼€: {job_id}")
            
            print(f"ğŸ” é€šè¿‡æ•°æ®åº“æŸ¥è¯¢è½¬æ¢ job_id: {job_id}")
            task_id = convert_job_id_to_task_id(job_id)
            
            if task_id is None:
                print(f"âŒ æ— æ³•æ‰¾åˆ°å¯¹åº”çš„ä»»åŠ¡IDï¼Œè¯·æ£€æŸ¥ job_id: {job_id}")
                return
            
            print(f"âœ… æŸ¥è¯¢æˆåŠŸï¼Œè·å¾—ä»»åŠ¡ID: {task_id}")
    
    print(f"ğŸ“ è·¯å¾„ç»“æ„: {args.account if hasattr(args, 'account') else 'yiya0110'}/{args.task_type_or_job_id}/{task_id}/")
    print("=" * 80)
    
    # ç¬¬äºŒæ­¥ï¼šç¡®å®šè¦å¤„ç†çš„æ–‡ä»¶åˆ—è¡¨
    if args.files is None:
        # æ ¹æ®ä»»åŠ¡ç±»å‹è‡ªåŠ¨é€‰æ‹©é»˜è®¤æ–‡ä»¶
        files_to_process = get_default_files_for_task_type(args.task_type_or_job_id)
        print(f"ğŸ“„ æ ¹æ®ä»»åŠ¡ç±»å‹è‡ªåŠ¨é€‰æ‹©æ–‡ä»¶: {', '.join(files_to_process)}")
    else:
        files_to_process = args.files
        print(f"ğŸ“„ ç”¨æˆ·æŒ‡å®šæ–‡ä»¶: {', '.join(files_to_process)}")
    
    # åˆ›å»ºèµ„æºè¯»å–å™¨
    reader = AzureResourceReader(args.account)
    
    # å¦‚æœåªæ˜¯åˆ—å‡ºä»»åŠ¡
    if args.list_jobs:
        print(f"ğŸ“‹ åˆ—å‡º {args.task_type_or_job_id} ä»»åŠ¡ (å‰10ä¸ª):")
        jobs = reader.list_task_jobs(args.task_type_or_job_id, limit=10)
        
        if jobs:
            print(f"âœ… æ‰¾åˆ° {len(jobs)} ä¸ªä»»åŠ¡:")
            for job in jobs:
                job_id_extracted = job.get('job_id', 'Unknown')
                print(f"  ğŸ“‹ ä»»åŠ¡ID: {job_id_extracted}")
                print(f"     ğŸ“„ æ–‡ä»¶: {job['name']}")
                print(f"     ğŸ“Š å¤§å°: {job['size']} å­—èŠ‚")
                print(f"     ğŸ“… ä¿®æ”¹: {job['last_modified']}")
                print()
        else:
            print("âŒ æœªæ‰¾åˆ°ä»»åŠ¡")
        return
    
    # ç¡®å®šæ˜¯å¦éœ€è¦è§£å‹ç¼©
    decompress = args.output_type in ['html', 'txt', 'json']
    
    # ç”¨äºè®°å½•æ˜¯å¦æœ‰æ–‡ä»¶æˆåŠŸä¸‹è½½
    successfully_downloaded_files = []
    
    # ğŸ†• é¦–å…ˆæ£€æŸ¥æ˜¯å¦å­˜åœ¨å•ä¸€å‹ç¼©æ–‡ä»¶æ ¼å¼: compress/{task_type}/{task_id}.gz
    single_file_path = f"compress/{args.task_type_or_job_id}/{task_id}.gz"
    container_name = reader.storage_config['container_name']
    
    print(f"\nğŸ” æ£€æŸ¥å•ä¸€å‹ç¼©æ–‡ä»¶: {single_file_path}")
    print("-" * 60)
    
    # æ£€æŸ¥å•ä¸€å‹ç¼©æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    single_file_info = reader.get_blob_info(container_name, single_file_path)
    
    if single_file_info is not None:
        print(f"âœ… å‘ç°å•ä¸€å‹ç¼©æ–‡ä»¶!")
        print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {single_file_info['size_mb']} MB")
        print(f"ğŸ“… ä¿®æ”¹æ—¶é—´: {single_file_info['last_modified']}")
        
        if not args.info_only:
            # è¯»å–å•ä¸€å‹ç¼©æ–‡ä»¶
            print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½å•ä¸€å‹ç¼©æ–‡ä»¶...")
            content = reader.read_blob_content(container_name, single_file_path, decompress=decompress)
            
            if content is not None:
                print("âœ… å•ä¸€å‹ç¼©æ–‡ä»¶è¯»å–æˆåŠŸ!")
                
                # æ˜¾ç¤ºå†…å®¹ä¿¡æ¯
                if isinstance(content, str):
                    print(f"ğŸ“ å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
                    
                    # å¦‚æœæ˜¯JSONç±»å‹ï¼Œå°è¯•è§£æ
                    if args.output_type == 'json':
                        try:
                            if content.strip().startswith('{') or content.strip().startswith('['):
                                parsed_data = json.loads(content)
                                print(f"ğŸ“‹ JSONè§£ææˆåŠŸï¼Œç±»å‹: {type(parsed_data)}")
                                if isinstance(parsed_data, dict):
                                    print(f"ğŸ”‘ JSONé”®: {list(parsed_data.keys())}")
                                elif isinstance(parsed_data, list):
                                    print(f"ğŸ“Š JSONæ•°ç»„é•¿åº¦: {len(parsed_data)}")
                        except json.JSONDecodeError:
                            print("âš ï¸  å†…å®¹ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼")
                    
                    # æ˜¾ç¤ºé¢„è§ˆ
                    print(f"ğŸ” å†…å®¹é¢„è§ˆ (å‰200å­—ç¬¦):")
                    print(content[:200] + "..." if len(content) > 200 else content)
                    
                else:
                    print(f"ğŸ“Š æ•°æ®é•¿åº¦: {len(content)} å­—èŠ‚")
                
                # ä¿å­˜å•ä¸€å‹ç¼©æ–‡ä»¶
                # æ£€æµ‹å†…å®¹æ˜¯å¦ä¸ºJSONæ ¼å¼ï¼Œå¦‚æœæ˜¯åˆ™å¼ºåˆ¶ä½¿ç”¨jsonæ‰©å±•å
                output_type_to_use = args.output_type
                if isinstance(content, str) and (content.strip().startswith('{') or content.strip().startswith('[')):
                    try:
                        json.loads(content)  # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆJSON
                        output_type_to_use = "json"  # å¼ºåˆ¶ä½¿ç”¨jsonæ ¼å¼
                        print("ğŸ” æ£€æµ‹åˆ°JSONå†…å®¹ï¼Œå°†ä¿å­˜ä¸º.jsonæ–‡ä»¶")
                    except json.JSONDecodeError:
                        pass  # ä¸æ˜¯æœ‰æ•ˆJSONï¼Œä¿æŒåŸè¾“å‡ºç±»å‹
                
                save_filename = _generate_save_filename(f"{task_id}", task_id, output_type_to_use)
                local_path = f"{args.save_dir}/{args.task_type_or_job_id}/{task_id}/{save_filename}"
                
                success = _save_content_to_file(content, local_path)
                if success:
                    print(f"ğŸ’¾ å•ä¸€å‹ç¼©æ–‡ä»¶å·²ä¿å­˜åˆ°: {local_path}")
                    successfully_downloaded_files.append(f"{task_id}.gz")
                    
                    # å¦‚æœæˆåŠŸä¸‹è½½äº†å•ä¸€å‹ç¼©æ–‡ä»¶ï¼Œè·³è¿‡åç»­çš„å¤šæ–‡ä»¶å¤„ç†
                    print(f"\nâœ… å•ä¸€å‹ç¼©æ–‡ä»¶å¤„ç†å®Œæˆï¼Œè·³è¿‡å¤šæ–‡ä»¶å¤„ç†")
                else:
                    print("âŒ å•ä¸€å‹ç¼©æ–‡ä»¶ä¿å­˜å¤±è´¥")
            else:
                print("âŒ å•ä¸€å‹ç¼©æ–‡ä»¶è¯»å–å¤±è´¥")
    else:
        print(f"âŒ æœªæ‰¾åˆ°å•ä¸€å‹ç¼©æ–‡ä»¶ï¼Œä½¿ç”¨åŸå§‹é€»è¾‘å¤„ç†å¤šä¸ªæ–‡ä»¶")
    
    # å¦‚æœå•ä¸€å‹ç¼©æ–‡ä»¶ä¸å­˜åœ¨æˆ–å¤„ç†å¤±è´¥ï¼ŒæŒ‰åŸæ¥çš„é€»è¾‘å¤„ç†æ¯ä¸ªæ–‡ä»¶
    if not successfully_downloaded_files:
        print(f"\nğŸ“„ æŒ‰åŸå§‹é€»è¾‘å¤„ç†å¤šä¸ªæ–‡ä»¶")
        print("=" * 40)
        
        for filename in files_to_process:
            print(f"\nğŸ“„ å¤„ç†æ–‡ä»¶: {filename}")
            print("-" * 40)
            
            if args.info_only:
                # æ˜¾ç¤ºåŸå§‹æ–‡ä»¶ä¿¡æ¯
                blob_path = f"compress/{args.task_type_or_job_id}/{task_id}/{filename}"
                blob_info = reader.get_blob_info('download', blob_path)
                
                if blob_info:
                    print(f"âœ… åŸå§‹æ–‡ä»¶ä¿¡æ¯:")
                    print(f"  ğŸ“Š å¤§å°: {blob_info['size_mb']} MB")
                    print(f"  ğŸ“… ä¿®æ”¹æ—¶é—´: {blob_info['last_modified']}")
                    print(f"  ğŸ”— URL: {blob_info['url']}")
                else:
                    print("âŒ åŸå§‹æ–‡ä»¶ä¸å­˜åœ¨æˆ–è·å–ä¿¡æ¯å¤±è´¥")
                continue
            
            # è¯»å–åŸå§‹æ–‡ä»¶
            content = reader.read_task_file(args.task_type_or_job_id, task_id, filename, decompress)
            
            if content is not None:
                print("âœ… åŸå§‹æ–‡ä»¶è¯»å–æˆåŠŸ!")
                if isinstance(content, str):
                    print(f"ğŸ“ åŸå§‹æ–‡ä»¶é•¿åº¦: {len(content)} å­—ç¬¦")
                else:
                    print(f"ğŸ“Š åŸå§‹æ–‡ä»¶å¤§å°: {len(content)} å­—èŠ‚")
                
                # ä¿å­˜åŸå§‹æ–‡ä»¶
                save_filename = _generate_save_filename(filename, task_id, args.output_type)
                local_path = f"{args.save_dir}/{args.task_type_or_job_id}/{task_id}/{save_filename}"
                
                success = _save_content_to_file(content, local_path)
                if success:
                    print(f"ğŸ’¾ åŸå§‹æ–‡ä»¶å·²ä¿å­˜åˆ°: {local_path}")
                    successfully_downloaded_files.append(filename)
            else:
                print("âŒ åŸå§‹æ–‡ä»¶è¯»å–å¤±è´¥æˆ–æ–‡ä»¶ä¸å­˜åœ¨")
    
    # ç¬¬ä¸‰æ­¥ï¼šæ›´æ–°ä»»åŠ¡æ˜ å°„æ–‡ä»¶ï¼ˆå¦‚æœæœ‰æ–‡ä»¶æˆåŠŸä¸‹è½½ä¸”æœªç¦ç”¨æ˜ å°„ï¼‰
    if successfully_downloaded_files and not args.info_only and not args.no_mapping:
        print("\n" + "=" * 80)
        print("ç¬¬ä¸‰æ­¥ï¼šæ›´æ–°ä»»åŠ¡æ˜ å°„æ–‡ä»¶")
        print("=" * 80)
        
        # æ˜¾ç¤ºæ˜ å°„ä¿¡æ¯
        print_task_mapping_info(original_input, args.task_type_or_job_id, task_id, args.save_dir)
        
        # æ›´æ–°æ˜ å°„
        mapping_success = update_task_mapping(original_input, args.task_type_or_job_id, task_id, args.save_dir)
        
        if mapping_success:
            print(f"âœ… æˆåŠŸä¸‹è½½ {len(successfully_downloaded_files)} ä¸ªæ–‡ä»¶å¹¶æ›´æ–°æ˜ å°„")
            print(f"ğŸ“„ å·²ä¸‹è½½æ–‡ä»¶: {', '.join(successfully_downloaded_files)}")
        else:
            print("âš ï¸  æ–‡ä»¶ä¸‹è½½æˆåŠŸä½†æ˜ å°„æ›´æ–°å¤±è´¥")
    
    elif args.info_only:
        print(f"\nğŸ“‹ ä¿¡æ¯æŸ¥çœ‹æ¨¡å¼ï¼Œæœªä¸‹è½½æ–‡ä»¶")
    elif args.no_mapping:
        print(f"\nğŸ“‹ å·²ç¦ç”¨æ˜ å°„æ–‡ä»¶ç”Ÿæˆ")
    elif not successfully_downloaded_files:
        print(f"\nâš ï¸  æ²¡æœ‰æ–‡ä»¶æˆåŠŸä¸‹è½½ï¼Œæœªæ›´æ–°æ˜ å°„")


def _generate_save_filename(original_filename: str, task_id: str, output_type: str) -> str:
    """
    ç”Ÿæˆä¿å­˜æ–‡ä»¶åï¼ˆä¸åŒ…å«è·¯å¾„ï¼Œåªæ˜¯æ–‡ä»¶åï¼‰
    
    Args:
        original_filename: åŸå§‹æ–‡ä»¶åï¼Œå¦‚ 'login.gz'
        task_id: ä»»åŠ¡ID
        output_type: è¾“å‡ºç±»å‹
        
    Returns:
        str: ç”Ÿæˆçš„æ–‡ä»¶å
    """
    base_name = original_filename.replace('.gz', '').replace('.', '_')
    
    if output_type == 'raw':
        return original_filename
    else:
        extension = output_type if output_type in ['html', 'txt', 'json'] else 'txt'
        return f"{base_name}.{extension}"


def _save_content_to_file(content: Union[str, bytes], file_path: str) -> bool:
    """
    ä¿å­˜å†…å®¹åˆ°æ–‡ä»¶
    
    Args:
        content: æ–‡ä»¶å†…å®¹
        file_path: æ–‡ä»¶è·¯å¾„
        
    Returns:
        bool: ä¿å­˜æˆåŠŸè¿”å›True
    """
    try:
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        local_path = Path(file_path)
        local_path.parent.mkdir(parents=True, exist_ok=True)
        
        # å†™å…¥æ–‡ä»¶
        if isinstance(content, str):
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)
        else:
            with open(file_path, 'wb') as f:
                f.write(content)
        
        return True
        
    except Exception as e:
        logger.error(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {str(e)}")
        return False


def demo_read_amazon_listing_job():
    """æ¼”ç¤ºè¯»å–Amazon Listing Jobæ–‡ä»¶ï¼ˆä¿ç•™ç”¨äºæµ‹è¯•ï¼‰"""
    # åˆ›å»ºèµ„æºè¯»å–å™¨
    reader = AzureResourceReader('yiya0110')
    
    # æŒ‡å®šè¦è¯»å–çš„ä»»åŠ¡ID
    job_id = '1925464883027513344'
    
    print(f"ğŸ” æ­£åœ¨è¯»å–Amazon Listing Jobæ–‡ä»¶...")
    print(f"ğŸ“‹ ä»»åŠ¡ç±»å‹: AmazonListingJob")
    print(f"ğŸ“‹ ä»»åŠ¡ID: {job_id}")
    print(f"ğŸ“ è·¯å¾„ç»“æ„: yiya0110/download/compress/AmazonListingJob/{job_id}/")
    print("=" * 80)
    
    # æ–¹æ³•1ï¼šè¯»å–é»˜è®¤çš„ä¸¤ä¸ªæ–‡ä»¶
    print("æ–¹æ³•1ï¼šè¯»å–ä¸¤ä¸ªé»˜è®¤æ–‡ä»¶ (login.gz + normal.gz)")
    both_files = reader.read_amazon_listing_job_both_files(job_id)
    
    for file_type, content in both_files.items():
        print(f"\nğŸ“„ {file_type}.gz æ–‡ä»¶:")
        if content:
            print(f"âœ… è¯»å–æˆåŠŸ!")
            if isinstance(content, str):
                print(f"ğŸ“ å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
                print(f"ğŸ” å†…å®¹é¢„è§ˆ (å‰200å­—ç¬¦):")
                print(content[:200] + "..." if len(content) > 200 else content)
            else:
                print(f"ğŸ“Š æ•°æ®é•¿åº¦: {len(content)} å­—èŠ‚")
        else:
            print("âŒ è¯»å–å¤±è´¥æˆ–æ–‡ä»¶ä¸å­˜åœ¨")
    
    print("\n" + "=" * 80)
    
    # æ–¹æ³•2ï¼šä½¿ç”¨é€šç”¨æ–¹æ³•è¯»å–æŒ‡å®šæ–‡ä»¶
    print("æ–¹æ³•2ï¼šä½¿ç”¨é€šç”¨æ–¹æ³•è¯»å–login.gz")
    login_content = reader.read_task_file('AmazonListingJob', job_id, 'login.gz')
    
    if login_content:
        print("âœ… é€šç”¨æ–¹æ³•è¯»å–æˆåŠŸ!")
        if isinstance(login_content, str):
            print(f"ğŸ“ å†…å®¹é•¿åº¦: {len(login_content)} å­—ç¬¦")
        else:
            print(f"ğŸ“Š æ•°æ®é•¿åº¦: {len(login_content)} å­—èŠ‚")
    else:
        print("âŒ é€šç”¨æ–¹æ³•è¯»å–å¤±è´¥")
    
    print("\n" + "=" * 80)
    
    # æ–¹æ³•3ï¼šè·å–æ–‡ä»¶ä¿¡æ¯
    print("æ–¹æ³•3ï¼šè·å–æ–‡ä»¶è¯¦ç»†ä¿¡æ¯")
    for filename in ['login.gz', 'normal.gz']:
        blob_path = f"compress/AmazonListingJob/{job_id}/{filename}"
        blob_info = reader.get_blob_info('download', blob_path)
        
        print(f"\nğŸ“„ {filename}:")
        if blob_info:
            print(f"  ğŸ“Š å¤§å°: {blob_info['size_mb']} MB")
            print(f"  ğŸ“… ä¿®æ”¹æ—¶é—´: {blob_info['last_modified']}")
            print(f"  ğŸ”— URL: {blob_info['url']}")
        else:
            print("  âŒ æ–‡ä»¶ä¸å­˜åœ¨æˆ–è·å–ä¿¡æ¯å¤±è´¥")
    
    print("\n" + "=" * 80)
    
    # æ–¹æ³•4ï¼šä¿å­˜æ–‡ä»¶åˆ°æœ¬åœ°
    print("æ–¹æ³•4ï¼šä¸‹è½½å¹¶ä¿å­˜åˆ°æœ¬åœ°")
    for filename in ['login.gz', 'normal.gz']:
        blob_path = f"compress/AmazonListingJob/{job_id}/{filename}"
        local_filename = filename.replace('.gz', '.txt')
        local_path = f"data/output/amazon_listing_{job_id}_{local_filename}"
        
        success = reader.save_blob_to_file('download', blob_path, local_path)
        if success:
            print(f"âœ… {filename} å·²ä¿å­˜åˆ°: {local_path}")
        else:
            print(f"âŒ {filename} ä¿å­˜å¤±è´¥")
    
    print("\n" + "=" * 80)
    
    # æ–¹æ³•5ï¼šåˆ—å‡ºAmazon Listing Jobs
    print("æ–¹æ³•5ï¼šåˆ—å‡ºæœ€è¿‘çš„Amazon Listing Jobs (å‰5ä¸ª)")
    jobs = reader.list_amazon_listing_jobs(limit=5)
    
    if jobs:
        print(f"âœ… æ‰¾åˆ° {len(jobs)} ä¸ªä»»åŠ¡:")
        for job in jobs:
            job_id_extracted = job.get('job_id', 'Unknown')
            print(f"  ğŸ“‹ ä»»åŠ¡ID: {job_id_extracted}")
            print(f"     ğŸ“„ æ–‡ä»¶: {job['name']}")
            print(f"     ğŸ“Š å¤§å°: {job['size']} å­—èŠ‚")
            print(f"     ğŸ“… ä¿®æ”¹: {job['last_modified']}")
            print()
    else:
        print("âŒ æœªæ‰¾åˆ°ä»»åŠ¡æˆ–åˆ—å–å¤±è´¥")


def is_valid_task_id(task_id: str) -> bool:
    """
    æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆçš„ä»»åŠ¡IDæ ¼å¼ï¼ˆé•¿æ•°å­—ä¸²ï¼‰
    
    Args:
        task_id: è¦æ£€æŸ¥çš„ä»»åŠ¡ID
        
    Returns:
        bool: å¦‚æœæ˜¯æœ‰æ•ˆçš„ä»»åŠ¡IDè¿”å›True
    """
    # æ£€æŸ¥æ˜¯å¦ä¸º18-20ä½çš„çº¯æ•°å­—
    return re.match(r'^\d{18,20}$', task_id) is not None


def map_db_task_type_to_system_type(db_task_type: str) -> str:
    """
    å°†æ•°æ®åº“ä¸­çš„ä»»åŠ¡ç±»å‹æ˜ å°„åˆ°ç³»ç»Ÿä½¿ç”¨çš„ä»»åŠ¡ç±»å‹
    
    Args:
        db_task_type: æ•°æ®åº“ä¸­çš„ä»»åŠ¡ç±»å‹ï¼Œå¦‚ 'amazon_product', 'amazon_review'
        
    Returns:
        str: ç³»ç»Ÿä½¿ç”¨çš„ä»»åŠ¡ç±»å‹ï¼Œå¦‚ 'AmazonListingJob', 'AmazonReviewStarJob'
    """
    mapping = {
        'amazon_product': 'AmazonListingJob',
        'amazon_review': 'AmazonReviewStarJob',
        'amazon_listing': 'AmazonListingJob',
        'amazon_review_star': 'AmazonReviewStarJob'
    }
    
    return mapping.get(db_task_type.lower(), db_task_type)


def get_task_type_by_job_id(job_id: str) -> Optional[str]:
    """
    é€šè¿‡æ•°æ®åº“æŸ¥è¯¢è·å–ä»»åŠ¡ç±»å‹
    
    Args:
        job_id: è¯·æ±‚åºåˆ—å·ï¼Œå¦‚ 'SL2796867471' æˆ– '2796867471'
        
    Returns:
        Optional[str]: æ‰¾åˆ°çš„ä»»åŠ¡ç±»å‹ (type)ï¼Œæœªæ‰¾åˆ°è¿”å›None
    """
    # å¦‚æœæ˜¯çº¯æ•°å­—ï¼Œæ·»åŠ SLå‰ç¼€
    if job_id.isdigit():
        job_id = f"SL{job_id}"
    
    # åˆ›å»ºæ•°æ®åº“è¿æ¥
    db_config = DB_CONFIG.copy()
    db_config['database'] = 'shulex_collector_prod'
    
    db = DatabaseConnector(db_config)
    if not db.connect():
        logger.error("æ— æ³•è¿æ¥åˆ°æ•°æ®åº“ shulex_collector_prod")
        return None
    
    try:
        # å®šä¹‰è¦æŸ¥è¯¢çš„è¡¨
        tables_to_check = ['log_a', 'log_b', 'log_c', 'log_d']
        
        logger.info(f"æ­£åœ¨æŸ¥è¯¢ job_id: {job_id} çš„ä»»åŠ¡ç±»å‹")
        
        all_results = []
        
        # æŸ¥è¯¢å„ä¸ªè¡¨
        for table_name in tables_to_check:
            query = f"SELECT type FROM {table_name} WHERE req_ssn = %s"
            try:
                records = db.execute_query(query, (job_id,))
                if records:
                    all_results.extend(records)
                    logger.info(f"åœ¨è¡¨ {table_name} ä¸­æ‰¾åˆ° {len(records)} æ¡è®°å½•")
                    
            except Exception as e:
                logger.error(f"æŸ¥è¯¢è¡¨ {table_name} å¤±è´¥: {str(e)}")
        
        # åˆ†ææŸ¥è¯¢ç»“æœ
        if len(all_results) == 0:
            logger.warning(f"åœ¨æ‰€æœ‰è¡¨ä¸­éƒ½æ²¡æœ‰æ‰¾åˆ° job_id: {job_id}")
            return None
            
        # è·å–å”¯ä¸€çš„ä»»åŠ¡ç±»å‹
        unique_types = set()
        for record in all_results:
            task_type = record.get('type', '')
            if task_type:
                unique_types.add(task_type)
        
        if len(unique_types) == 1:
            db_task_type = list(unique_types)[0]
            # æ˜ å°„åˆ°ç³»ç»Ÿä½¿ç”¨çš„ä»»åŠ¡ç±»å‹
            system_task_type = map_db_task_type_to_system_type(db_task_type)
            logger.info(f"æ‰¾åˆ°ä»»åŠ¡ç±»å‹: {db_task_type} -> {system_task_type}")
            return system_task_type
        elif len(unique_types) > 1:
            logger.warning(f"æ‰¾åˆ°å¤šä¸ªä»»åŠ¡ç±»å‹: {unique_types}ï¼Œè¿”å›ç¬¬ä¸€ä¸ª")
            db_task_type = list(unique_types)[0]
            system_task_type = map_db_task_type_to_system_type(db_task_type)
            return system_task_type
        else:
            logger.warning(f"æ‰¾åˆ°è®°å½•ä½†ä»»åŠ¡ç±»å‹ä¸ºç©º")
            return None
    
    finally:
        db.disconnect()


def convert_task_id_to_job_id(task_id: str) -> Optional[str]:
    """
    åå‘æŸ¥è¯¢ï¼šé€šè¿‡task_idè·å–å¯¹åº”çš„job_idï¼ˆreq_ssnï¼‰
    
    Args:
        task_id: ä»»åŠ¡ID
        
    Returns:
        Optional[str]: å¯¹åº”çš„job_idï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™è¿”å›None
    """
    try:
        # å¯¼å…¥æ•°æ®åº“é…ç½®å¹¶æŒ‡å®šæ•°æ®åº“
        from config.db_config import DB_CONFIG
        config_with_db = DB_CONFIG.copy()
        config_with_db['database'] = 'shulex_collector_prod'
        
        # ä½¿ç”¨æ•°æ®åº“è¿æ¥å™¨æŸ¥è¯¢
        db = DatabaseConnector(config_with_db)
        
        # ç¡®ä¿è¿æ¥æˆåŠŸ
        if not db.connect():
            logger.error("åå‘æŸ¥è¯¢: æ•°æ®åº“è¿æ¥å¤±è´¥")
            return None
        
        # æŸ¥è¯¢æ‰€æœ‰å¯èƒ½çš„è¡¨
        tables = ['log_a', 'log_b', 'log_c', 'log_d']
        logger.info(f"æ­£åœ¨åå‘æŸ¥è¯¢ task_id: {task_id} å¯¹åº”çš„ job_id")
        
        for table in tables:
            query = f"""
            SELECT req_ssn 
            FROM {table} 
            WHERE ext_ssn = %s 
            LIMIT 1
            """
            
            result = db.execute_query(query, (task_id,))
            
            if result and len(result) > 0:
                job_id = str(result[0]['req_ssn'])
                logger.info(f"åœ¨è¡¨ {table} ä¸­æ‰¾åˆ°å¯¹åº”çš„ job_id: {job_id}")
                
                # å¦‚æœjob_idä»¥SLå¼€å¤´ï¼Œå»æ‰SLå‰ç¼€è¿”å›åŸå§‹çš„req_ssn
                if job_id.startswith('SL') and job_id[2:].isdigit():
                    original_job_id = job_id[2:]
                    logger.info(f"å»æ‰SLå‰ç¼€ï¼Œè¿”å›åŸå§‹ job_id: {original_job_id}")
                    db.disconnect()
                    return original_job_id
                else:
                    db.disconnect()
                    return job_id
        
        logger.warning(f"åœ¨æ‰€æœ‰è¡¨ä¸­éƒ½æ²¡æœ‰æ‰¾åˆ° task_id: {task_id} å¯¹åº”çš„ job_id")
        db.disconnect()
        return None
        
    except Exception as e:
        logger.error(f"åå‘æŸ¥è¯¢å¤±è´¥: {str(e)}")
        return None


def convert_job_id_to_task_id(job_id: str) -> Optional[str]:
    """
    é€šè¿‡æ•°æ®åº“æŸ¥è¯¢å°† job_id è½¬æ¢ä¸º task_id
    
    Args:
        job_id: è¯·æ±‚åºåˆ—å·ï¼Œå¦‚ 'SL2796867471'
        
    Returns:
        Optional[str]: æ‰¾åˆ°çš„ task_id (ext_ssn)ï¼Œæœªæ‰¾åˆ°è¿”å›None
    """
    # åˆ›å»ºæ•°æ®åº“è¿æ¥
    db_config = DB_CONFIG.copy()
    db_config['database'] = 'shulex_collector_prod'
    
    db = DatabaseConnector(db_config)
    if not db.connect():
        logger.error("æ— æ³•è¿æ¥åˆ°æ•°æ®åº“ shulex_collector_prod")
        return None
    
    try:
        # å®šä¹‰è¦æŸ¥è¯¢çš„è¡¨
        tables_to_check = ['log_a', 'log_b', 'log_c', 'log_d']
        
        logger.info(f"æ­£åœ¨æŸ¥è¯¢ job_id: {job_id}")
        
        all_results = []
        
        # æŸ¥è¯¢å„ä¸ªè¡¨
        for table_name in tables_to_check:
            query = f"SELECT * FROM {table_name} WHERE req_ssn = %s"
            try:
                records = db.execute_query(query, (job_id,))
                if records:
                    all_results.extend(records)
                    logger.info(f"åœ¨è¡¨ {table_name} ä¸­æ‰¾åˆ° {len(records)} æ¡è®°å½•")
                    
            except Exception as e:
                logger.error(f"æŸ¥è¯¢è¡¨ {table_name} å¤±è´¥: {str(e)}")
        
        # åˆ†ææŸ¥è¯¢ç»“æœ
        if len(all_results) == 0:
            logger.warning(f"åœ¨æ‰€æœ‰è¡¨ä¸­éƒ½æ²¡æœ‰æ‰¾åˆ° job_id: {job_id}")
            return None
            
        elif len(all_results) == 1:
            # æ‰¾åˆ°å”¯ä¸€è®°å½•
            record = all_results[0]
            ext_ssn = record.get('ext_ssn', '')
            if ext_ssn:
                logger.info(f"æ‰¾åˆ°å”¯ä¸€è®°å½•ï¼Œtask_id (ext_ssn): {ext_ssn}")
                return ext_ssn
            else:
                logger.warning(f"æ‰¾åˆ°è®°å½•ä½† ext_ssn ä¸ºç©º")
                return None
                
        else:
            # æ‰¾åˆ°å¤šæ¡è®°å½•ï¼Œéœ€è¦å»é‡
            unique_ext_ssns = set()
            for record in all_results:
                ext_ssn = record.get('ext_ssn', '')
                if ext_ssn:
                    unique_ext_ssns.add(ext_ssn)
            
            if len(unique_ext_ssns) == 1:
                task_id = list(unique_ext_ssns)[0]
                logger.info(f"æ‰¾åˆ° {len(all_results)} æ¡è®°å½•ï¼Œå»é‡åå¾—åˆ°å”¯ä¸€ task_id: {task_id}")
                return task_id
            else:
                logger.warning(f"æ‰¾åˆ° {len(all_results)} æ¡è®°å½•ï¼Œä½†åŒ…å« {len(unique_ext_ssns)} ä¸ªä¸åŒçš„ ext_ssn")
                return None
    
    finally:
        db.disconnect()


def get_default_files_for_task_type(task_type: str) -> List[str]:
    """
    æ ¹æ®ä»»åŠ¡ç±»å‹è·å–é»˜è®¤çš„æ–‡ä»¶åˆ—è¡¨
    
    Args:
        task_type: ä»»åŠ¡ç±»å‹ï¼Œå¦‚ 'AmazonListingJob' æˆ– 'AmazonReviewStarJob'
        
    Returns:
        List[str]: é»˜è®¤æ–‡ä»¶åˆ—è¡¨
    """
    if task_type == 'AmazonListingJob':
        return ['login.gz', 'normal.gz']
    elif task_type == 'AmazonReviewStarJob':
        return ['page_1.gz', 'page_2.gz', 'page_3.gz', 'page_4.gz', 'page_5.gz']
    else:
        # å¯¹äºæœªçŸ¥ä»»åŠ¡ç±»å‹ï¼Œå°è¯•é»˜è®¤æ–‡ä»¶
        return ['login.gz', 'normal.gz']


def update_task_mapping(input_param: str, task_type: str, actual_task_id: str, 
                       save_dir: str = 'data/output', **kwargs) -> bool:
    """
    æ›´æ–°ä»»åŠ¡æ˜ å°„è®°å½•åˆ°æœ¬åœ°æ•°æ®åº“ï¼ˆä»…ä½¿ç”¨æ•°æ®åº“ï¼‰
    
    Args:
        input_param: ç”¨æˆ·è¾“å…¥çš„å‚æ•°ï¼ˆtask_idæˆ–job_idï¼‰
        task_type: ä»»åŠ¡ç±»å‹
        actual_task_id: å®é™…çš„ä»»åŠ¡ID
        save_dir: ä¿å­˜ç›®å½•
        **kwargs: å…¶ä»–å¯é€‰å‚æ•°
        
    Returns:
        bool: æ›´æ–°æˆåŠŸè¿”å›True
    """
    try:
        from src.db.local_connector import LocalDatabaseConnector
        
        # ç”Ÿæˆç›¸å¯¹è·¯å¾„å’Œå®Œæ•´è·¯å¾„
        relative_path = f"./{task_type}/{actual_task_id}/"
        full_path = f"{save_dir}/{task_type}/{actual_task_id}/"
        
        # ç»Ÿè®¡æ–‡ä»¶ä¿¡æ¯
        file_count = 0
        has_parse_file = False
        files_info = []
        
        if os.path.exists(full_path):
            for file_path in Path(full_path).iterdir():
                if file_path.is_file():
                    file_count += 1
                    file_type = 'parse' if file_path.name == 'parse_result.json' else 'original'
                    if file_type == 'parse':
                        has_parse_file = True
                    
                    files_info.append({
                        'file_name': file_path.name,
                        'file_type': file_type,
                        'file_size': file_path.stat().st_size,
                        'file_path': str(file_path),
                        'download_success': True
                    })
        
        # è·å–ä¸‹è½½æ–¹å¼
        download_method = kwargs.get('download_method', 'azure_storage')
        status = kwargs.get('status', 'success')
        
        # æ’å…¥æˆ–æ›´æ–°æ•°æ®åº“è®°å½•
        db = LocalDatabaseConnector()
        mapping_id = db.insert_task_mapping(
            job_id=input_param,
            task_type=task_type,
            actual_task_id=actual_task_id,
            relative_path=relative_path,
            full_path=full_path,
            file_count=file_count,
            has_parse_file=has_parse_file,
            download_method=download_method,
            status=status
        )
        
        if mapping_id and files_info:
            db.insert_file_details(mapping_id, files_info)
        
        db.disconnect()
        
        if mapping_id:
            logger.info(f"âœ… ä»»åŠ¡æ˜ å°„å·²æ›´æ–°åˆ°æ•°æ®åº“: {input_param} -> {relative_path} (ID: {mapping_id})")
            return True
        else:
            logger.error(f"âŒ æ•°æ®åº“æ›´æ–°å¤±è´¥: {input_param}")
            return False
            
    except ImportError:
        logger.error(f"âŒ æœ¬åœ°æ•°æ®åº“æ¨¡å—ä¸å¯ç”¨ï¼Œæ— æ³•ä¿å­˜ä»»åŠ¡æ˜ å°„")
        return False
    except Exception as e:
        logger.error(f"âŒ æ•°æ®åº“æ“ä½œå¤±è´¥: {str(e)}")
        return False


def print_task_mapping_info(input_param: str, task_type: str, actual_task_id: str, 
                           save_dir: str = 'data/output') -> None:
    """
    æ‰“å°ä»»åŠ¡æ˜ å°„ä¿¡æ¯
    
    Args:
        input_param: ç”¨æˆ·è¾“å…¥çš„å‚æ•°
        task_type: ä»»åŠ¡ç±»å‹
        actual_task_id: å®é™…çš„ä»»åŠ¡ID
        save_dir: ä¿å­˜ç›®å½•
    """
    relative_path = f"./{task_type}/{actual_task_id}/"
    full_path = f"{save_dir}/{task_type}/{actual_task_id}/"
    
    print(f"\nğŸ“‹ ä»»åŠ¡æ˜ å°„ä¿¡æ¯:")
    print(f"  ğŸ” è¾“å…¥å‚æ•°: {input_param}")
    print(f"  ğŸ“ ç›¸å¯¹è·¯å¾„: {relative_path}")
    print(f"  ğŸ“„ æ˜ å°„æ–‡ä»¶: {save_dir}/task_mapping.json")


def show_task_mapping(save_dir: str = 'data/output') -> None:
    """
    æ˜¾ç¤ºå½“å‰çš„ä»»åŠ¡æ˜ å°„æ–‡ä»¶å†…å®¹
    
    Args:
        save_dir: ä¿å­˜ç›®å½•
    """
    map_file_path = f"{save_dir}/task_mapping.json"
    
    print("ğŸ” Azure Storage ä»»åŠ¡æ˜ å°„æŸ¥çœ‹å™¨")
    print("=" * 80)
    
    if not os.path.exists(map_file_path):
        print(f"âŒ æ˜ å°„æ–‡ä»¶ä¸å­˜åœ¨: {map_file_path}")
        return
    
    try:
        with open(map_file_path, 'r', encoding='utf-8') as f:
            mapping = json.load(f)
        
        if not mapping:
            print("ğŸ“‹ æ˜ å°„æ–‡ä»¶ä¸ºç©º")
            return
        
        print(f"ğŸ“„ æ˜ å°„æ–‡ä»¶è·¯å¾„: {map_file_path}")
        print(f"ğŸ“Š æ€»è®¡æ˜ å°„æ•°é‡: {len(mapping)}")
        print("\nğŸ“‹ ä»»åŠ¡æ˜ å°„åˆ—è¡¨:")
        print("-" * 80)
        
        # æŒ‰æœ€åæ›´æ–°æ—¶é—´æ’åº
        sorted_mappings = sorted(
            mapping.items(),
            key=lambda x: x[1].get('last_updated', ''),
            reverse=True
        )
        
        for input_param, info in sorted_mappings:
            print(f"\nğŸ” è¾“å…¥å‚æ•°: {input_param}")
            task_type = info.get('task_type', 'Unknown')
            
            if task_type == 'parse':
                # è§£ææ–‡ä»¶æ˜ å°„æ˜¾ç¤º
                print(f"  ğŸ“‚ ç±»å‹: è§£ææ–‡ä»¶")
                print(f"  ğŸ“‹ Job ID: {info.get('job_id', 'Unknown')}")
                print(f"  ğŸ“‹ Task ID: {info.get('task_id', 'Unknown')}")
            else:
                # åŸå§‹æ–‡ä»¶æ˜ å°„æ˜¾ç¤º
                print(f"  ğŸ“‚ ä»»åŠ¡ç±»å‹: {task_type}")
                print(f"  ğŸ“‹ å®é™…ä»»åŠ¡ID: {info.get('actual_task_id', 'Unknown')}")
            
            print(f"  ğŸ“ ç›¸å¯¹è·¯å¾„: {info.get('relative_path', 'Unknown')}")
            print(f"  ğŸ“… æœ€åæ›´æ–°: {info.get('last_updated', 'Unknown')}")
            
            # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨ï¼Œé€šè¿‡relative_pathæ„å»ºå®Œæ•´è·¯å¾„
            relative_path = info.get('relative_path', '')
            if relative_path:
                # ç§»é™¤ç›¸å¯¹è·¯å¾„å‰ç¼€ './' å¹¶æ„å»ºå®Œæ•´è·¯å¾„
                clean_path = relative_path.lstrip('./')
                full_path = os.path.join(save_dir, clean_path)
                
                if os.path.exists(full_path):
                    file_count = len([f for f in os.listdir(full_path) if os.path.isfile(os.path.join(full_path, f))])
                    print(f"  ğŸ“„ æ–‡ä»¶æ•°é‡: {file_count}")
                else:
                    print(f"  âš ï¸  ç›®å½•ä¸å­˜åœ¨")
            else:
                print(f"  âš ï¸  ç›¸å¯¹è·¯å¾„ä¿¡æ¯ç¼ºå¤±")
        
        print("\n" + "=" * 80)
        print("ğŸ’¡ ä½¿ç”¨æ–¹å¼:")
        print(f"   æ ¹æ®æ˜ å°„ï¼Œè¾“å…¥å‚æ•°å¯ä»¥ç›´æ¥å®šä½åˆ°å¯¹åº”çš„ä¸‹è½½æ–‡ä»¶å¤¹")
        print(f"   ä¾‹å¦‚ï¼šè¾“å…¥ '{list(mapping.keys())[0]}' å¯¹åº”æ–‡ä»¶å¤¹ '{list(mapping.values())[0].get('relative_path', '')}*'")
        
    except json.JSONDecodeError:
        print(f"âŒ æ˜ å°„æ–‡ä»¶æ ¼å¼é”™è¯¯: {map_file_path}")
    except Exception as e:
        print(f"âŒ è¯»å–æ˜ å°„æ–‡ä»¶å¤±è´¥: {str(e)}")


def handle_with_parse_mode(args) -> None:
    """
    å¤„ç†åŒæ—¶è·å–åŸå§‹æ•°æ®å’Œè§£ææ•°æ®çš„æ¨¡å¼ï¼ˆé›†æˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
    """
    print(f"ğŸ” Azure Storage èµ„æºè¯»å–å™¨ (åŸå§‹æ•°æ® + è§£ææ•°æ® - ä¼˜åŒ–ç‰ˆæœ¬)")
    print(f"ğŸ“‹ ä»»åŠ¡ç±»å‹: {args.task_type_or_job_id}")
    print(f"ğŸ“‹ è¾“å…¥å‚æ•°: {args.task_id_or_task_id}")
    
    # ç¬¬ä¸€æ­¥ï¼šéªŒè¯å’Œè½¬æ¢ä»»åŠ¡IDï¼ŒåŒæ—¶è·å–analysis_response
    task_id = None
    analysis_response = None
    job_id = None
    original_input = None  # ç”¨äºæ˜ å°„çš„åŸå§‹job_id
    
    if is_valid_task_id(args.task_id_or_task_id):
        # ç›´æ¥ä½¿ç”¨ä½œä¸ºä»»åŠ¡ID - éœ€è¦åå‘æŸ¥è¯¢è·å–job_id
        task_id = args.task_id_or_task_id
        print(f"âœ… æ£€æµ‹åˆ°æœ‰æ•ˆçš„ä»»åŠ¡ID: {task_id}")
        
        # é€šè¿‡task_idåå‘æŸ¥è¯¢job_idï¼ˆç”¨äºæ˜ å°„ç´¢å¼•ï¼‰
        job_id_from_task = convert_task_id_to_job_id(task_id)
        if job_id_from_task:
            original_input = job_id_from_task
            print(f"âœ… åå‘æŸ¥è¯¢æˆåŠŸï¼Œæ˜ å°„ç´¢å¼•ä½¿ç”¨ job_id: {original_input}")
        else:
            original_input = args.task_id_or_task_id
            print(f"âš ï¸  åå‘æŸ¥è¯¢å¤±è´¥ï¼Œä½¿ç”¨ä»»åŠ¡IDä½œä¸ºæ˜ å°„ç´¢å¼•: {original_input}")
    else:
        # éœ€è¦è½¬æ¢ä¸ºä»»åŠ¡IDå¹¶è·å–analysis_response
        original_input = args.task_id_or_task_id  # ä¿å­˜åŸå§‹è¾“å…¥çš„job_idç”¨äºæ˜ å°„
        job_id = args.task_id_or_task_id
        
        # å¦‚æœæ˜¯çº¯æ•°å­—ï¼Œæ·»åŠ SLå‰ç¼€
        if job_id.isdigit():
            job_id = f"SL{job_id}"
            print(f"ğŸ”„ æ·»åŠ SLå‰ç¼€: {job_id}")
        
        print(f"ğŸ” é€šè¿‡æ•°æ®åº“æŸ¥è¯¢è½¬æ¢ job_id: {job_id}")
        print(f"ğŸ“‹ æ˜ å°„ç´¢å¼•å°†ä½¿ç”¨åŸå§‹job_id: {original_input}")
        
        # ğŸ†• å°è¯•ä½¿ç”¨ä¼˜åŒ–å™¨è·å–task_idå’Œanalysis_response
        try:
            # å¯¼å…¥ä¼˜åŒ–å™¨æ¨¡å—
            from src.azure_resource_reader_optimizer import convert_job_id_to_task_info
            task_info = convert_job_id_to_task_info(job_id)
            
            if task_info is None:
                print(f"âŒ æ— æ³•æ‰¾åˆ°å¯¹åº”çš„ä»»åŠ¡IDï¼Œè¯·æ£€æŸ¥ job_id: {job_id}")
                return
            
            task_id, analysis_response = task_info
            print(f"âœ… æŸ¥è¯¢æˆåŠŸï¼Œè·å¾—ä»»åŠ¡ID: {task_id}")
            
            if analysis_response:
                print(f"âœ… è·å¾— analysis_response æ•°æ®ï¼Œé•¿åº¦: {len(str(analysis_response))} å­—ç¬¦")
                
                # æ£€æŸ¥æ˜¯å¦å¯ç”¨äº†analysis_responseè§£æ
                try:
                    from config.analysis_response_config import is_analysis_response_enabled
                    
                    if is_analysis_response_enabled(args.task_type_or_job_id):
                        print(f"âœ… ä»»åŠ¡ç±»å‹ {args.task_type_or_job_id} å·²å¯ç”¨ analysis_response è§£æ")
                    else:
                        print(f"âš ï¸  ä»»åŠ¡ç±»å‹ {args.task_type_or_job_id} æœªå¯ç”¨ analysis_response è§£æï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•")
                        analysis_response = None
                except ImportError:
                    print(f"âš ï¸  æ— æ³•å¯¼å…¥ analysis_response é…ç½®ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•")
                    analysis_response = None
            else:
                print(f"â„¹ï¸  æœªæ‰¾åˆ° analysis_response æ•°æ®")
                
        except ImportError:
            # å›é€€åˆ°åŸå§‹æ–¹æ³•
            print(f"âš ï¸  ä¼˜åŒ–å™¨æ¨¡å—ä¸å¯ç”¨ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•")
            task_id = convert_job_id_to_task_id(job_id)
            
            if task_id is None:
                print(f"âŒ æ— æ³•æ‰¾åˆ°å¯¹åº”çš„ä»»åŠ¡IDï¼Œè¯·æ£€æŸ¥ job_id: {job_id}")
                return
            
            print(f"âœ… æŸ¥è¯¢æˆåŠŸï¼Œè·å¾—ä»»åŠ¡ID: {task_id}")
    
    task_type = args.task_type_or_job_id
    print(f"ğŸ“ åŸå§‹æ•°æ®è·¯å¾„: yiya0110/download/compress/{task_type}/{task_id}/")
    print(f"ğŸ“ è§£ææ•°æ®è·¯å¾„: collector0109/parse/{task_type}/{task_id}/")
    print("=" * 80)
    
    # ç¬¬äºŒæ­¥ï¼šç¡®å®šè¦å¤„ç†çš„æ–‡ä»¶åˆ—è¡¨
    if args.files is None:
        # æ ¹æ®ä»»åŠ¡ç±»å‹è‡ªåŠ¨é€‰æ‹©é»˜è®¤æ–‡ä»¶
        files_to_process = get_default_files_for_task_type(task_type)
        print(f"ğŸ“„ æ ¹æ®ä»»åŠ¡ç±»å‹è‡ªåŠ¨é€‰æ‹©åŸå§‹æ–‡ä»¶: {', '.join(files_to_process)}")
    else:
        files_to_process = args.files
        print(f"ğŸ“„ ç”¨æˆ·æŒ‡å®šåŸå§‹æ–‡ä»¶: {', '.join(files_to_process)}")
    
    # åˆ›å»ºä¸»è¯»å–å™¨ï¼ˆyiya0110ï¼‰
    reader = AzureResourceReader('yiya0110')
    
    # ç¡®å®šæ˜¯å¦éœ€è¦è§£å‹ç¼©
    decompress = args.output_type in ['html', 'txt', 'json']
    
    # ç”¨äºè®°å½•æ˜¯å¦æœ‰æ–‡ä»¶æˆåŠŸä¸‹è½½
    successfully_downloaded_files = []
    parse_file_downloaded = False
    parse_result = None
    
    # ğŸ†• é¦–å…ˆå°è¯•ä½¿ç”¨ä¼˜åŒ–æ–¹æ³•è·å–è§£ææ–‡ä»¶
    if not args.info_only:
        print(f"\nğŸš€ æ­¥éª¤1: ä½¿ç”¨ä¼˜åŒ–æ–¹æ³•è·å–è§£ææ–‡ä»¶")
        print("-" * 60)
        
        try:
            from src.azure_resource_reader_optimizer import fetch_and_save_parse_files_optimized
            
            # åˆ›å»ºcollector0109è¯»å–å™¨ç”¨äºè§£ææ–‡ä»¶
            parse_reader = AzureResourceReader('collector0109')
            
            # ä½¿ç”¨ä¼˜åŒ–æ–¹æ³•è·å–è§£ææ–‡ä»¶
            parse_result = fetch_and_save_parse_files_optimized(
                reader=parse_reader,
                task_type=task_type,
                task_id=task_id,
                save_dir=args.save_dir,
                decompress=decompress,
                job_id=job_id,
                analysis_response=analysis_response
            )
            
            if parse_result['success']:
                print(f"âœ… è§£ææ–‡ä»¶è·å–æˆåŠŸ!")
                if 'method_used' in parse_result:
                    method_name = "analysis_responseé“¾æ¥" if parse_result['method_used'] == 'analysis_response' else "Azureå­˜å‚¨"
                    print(f"ğŸ“¡ è·å–æ–¹å¼: {method_name}")
                print(f"ğŸ“¥ ä¸‹è½½æ–‡ä»¶æ•°: {parse_result['total_files_downloaded']}")
                
                # æ˜¾ç¤ºæ–‡ä»¶è¯¦æƒ…
                if parse_result['files_downloaded']:
                    for file_info in parse_result['files_downloaded']:
                        print(f"  âœ… {file_info['saved_name']}")
                        if 'size' in file_info:
                            print(f"     ğŸ“Š å¤§å°: {file_info['size']} å­—èŠ‚")
                        print(f"     ğŸ“ è·¯å¾„: {file_info['local_path']}")
                
                parse_file_downloaded = True
            else:
                print(f"âŒ è§£ææ–‡ä»¶è·å–å¤±è´¥: {parse_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                
        except ImportError:
            print(f"âš ï¸  ä¼˜åŒ–å™¨æ¨¡å—ä¸å¯ç”¨ï¼Œå°†åœ¨å¤„ç†åŸå§‹æ–‡ä»¶æ—¶ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•")
    
    # ğŸ†• é¦–å…ˆæ£€æŸ¥æ˜¯å¦å­˜åœ¨å•ä¸€å‹ç¼©æ–‡ä»¶æ ¼å¼: compress/{task_type}/{task_id}.gz
    print(f"\nğŸ“„ æ­¥éª¤2: è·å–åŸå§‹æ–‡ä»¶")
    print("-" * 60)
    
    single_file_path = f"compress/{task_type}/{task_id}.gz"
    container_name = reader.storage_config['container_name']
    
    print(f"\nğŸ” æ£€æŸ¥å•ä¸€å‹ç¼©æ–‡ä»¶: {single_file_path}")
    print("-" * 40)
    
    # æ£€æŸ¥å•ä¸€å‹ç¼©æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    single_file_info = reader.get_blob_info(container_name, single_file_path)
    
    if single_file_info is not None:
        print(f"âœ… å‘ç°å•ä¸€å‹ç¼©æ–‡ä»¶!")
        print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {single_file_info['size_mb']} MB")
        print(f"ğŸ“… ä¿®æ”¹æ—¶é—´: {single_file_info['last_modified']}")
        
        if not args.info_only:
            # è¯»å–å•ä¸€å‹ç¼©æ–‡ä»¶
            print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½å•ä¸€å‹ç¼©æ–‡ä»¶...")
            content = reader.read_blob_content(container_name, single_file_path, decompress=decompress)
            
            if content is not None:
                print("âœ… å•ä¸€å‹ç¼©æ–‡ä»¶è¯»å–æˆåŠŸ!")
                
                # æ˜¾ç¤ºå†…å®¹ä¿¡æ¯
                if isinstance(content, str):
                    print(f"ğŸ“ å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
                    
                    # å¦‚æœæ˜¯JSONç±»å‹ï¼Œå°è¯•è§£æ
                    if args.output_type == 'json':
                        try:
                            if content.strip().startswith('{') or content.strip().startswith('['):
                                parsed_data = json.loads(content)
                                print(f"ğŸ“‹ JSONè§£ææˆåŠŸï¼Œç±»å‹: {type(parsed_data)}")
                                if isinstance(parsed_data, dict):
                                    print(f"ğŸ”‘ JSONé”®: {list(parsed_data.keys())}")
                                elif isinstance(parsed_data, list):
                                    print(f"ğŸ“Š JSONæ•°ç»„é•¿åº¦: {len(parsed_data)}")
                        except json.JSONDecodeError:
                            print("âš ï¸  å†…å®¹ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼")
                    
                    # æ˜¾ç¤ºé¢„è§ˆ
                    print(f"ğŸ” å†…å®¹é¢„è§ˆ (å‰200å­—ç¬¦):")
                    print(content[:200] + "..." if len(content) > 200 else content)
                    
                else:
                    print(f"ğŸ“Š æ•°æ®é•¿åº¦: {len(content)} å­—èŠ‚")
                
                # ä¿å­˜å•ä¸€å‹ç¼©æ–‡ä»¶
                # æ£€æµ‹å†…å®¹æ˜¯å¦ä¸ºJSONæ ¼å¼ï¼Œå¦‚æœæ˜¯åˆ™å¼ºåˆ¶ä½¿ç”¨jsonæ‰©å±•å
                output_type_to_use = args.output_type
                if isinstance(content, str) and (content.strip().startswith('{') or content.strip().startswith('[')):
                    try:
                        json.loads(content)  # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆJSON
                        output_type_to_use = "json"  # å¼ºåˆ¶ä½¿ç”¨jsonæ ¼å¼
                        print("ğŸ” æ£€æµ‹åˆ°JSONå†…å®¹ï¼Œå°†ä¿å­˜ä¸º.jsonæ–‡ä»¶")
                    except json.JSONDecodeError:
                        pass  # ä¸æ˜¯æœ‰æ•ˆJSONï¼Œä¿æŒåŸè¾“å‡ºç±»å‹
                
                save_filename = _generate_save_filename(f"{task_id}", task_id, output_type_to_use)
                local_path = f"{args.save_dir}/{task_type}/{task_id}/{save_filename}"
                
                success = _save_content_to_file(content, local_path)
                if success:
                    print(f"ğŸ’¾ å•ä¸€å‹ç¼©æ–‡ä»¶å·²ä¿å­˜åˆ°: {local_path}")
                    successfully_downloaded_files.append(f"{task_id}.gz")
                    
                    # å¦‚æœæˆåŠŸä¸‹è½½äº†å•ä¸€å‹ç¼©æ–‡ä»¶ï¼Œè·³è¿‡åç»­çš„å¤šæ–‡ä»¶å¤„ç†
                    print(f"\nâœ… å•ä¸€å‹ç¼©æ–‡ä»¶å¤„ç†å®Œæˆï¼Œè·³è¿‡å¤šæ–‡ä»¶å¤„ç†")
                else:
                    print("âŒ å•ä¸€å‹ç¼©æ–‡ä»¶ä¿å­˜å¤±è´¥")
            else:
                print("âŒ å•ä¸€å‹ç¼©æ–‡ä»¶è¯»å–å¤±è´¥")
    else:
        print(f"âŒ æœªæ‰¾åˆ°å•ä¸€å‹ç¼©æ–‡ä»¶ï¼Œä½¿ç”¨åŸå§‹é€»è¾‘å¤„ç†å¤šä¸ªæ–‡ä»¶")
    
    # å¦‚æœå•ä¸€å‹ç¼©æ–‡ä»¶ä¸å­˜åœ¨æˆ–å¤„ç†å¤±è´¥ï¼ŒæŒ‰åŸæ¥çš„é€»è¾‘å¤„ç†æ¯ä¸ªæ–‡ä»¶
    if not successfully_downloaded_files:
        print(f"\nğŸ“„ æŒ‰åŸå§‹é€»è¾‘å¤„ç†å¤šä¸ªæ–‡ä»¶")
        print("=" * 40)
        
        for filename in files_to_process:
            print(f"\nğŸ“„ å¤„ç†æ–‡ä»¶: {filename}")
            print("-" * 40)
            
            if args.info_only:
                # æ˜¾ç¤ºåŸå§‹æ–‡ä»¶ä¿¡æ¯
                blob_path = f"compress/{task_type}/{task_id}/{filename}"
                blob_info = reader.get_blob_info('download', blob_path)
                
                if blob_info:
                    print(f"âœ… åŸå§‹æ–‡ä»¶ä¿¡æ¯:")
                    print(f"  ğŸ“Š å¤§å°: {blob_info['size_mb']} MB")
                    print(f"  ğŸ“… ä¿®æ”¹æ—¶é—´: {blob_info['last_modified']}")
                    print(f"  ğŸ”— URL: {blob_info['url']}")
                else:
                    print("âŒ åŸå§‹æ–‡ä»¶ä¸å­˜åœ¨æˆ–è·å–ä¿¡æ¯å¤±è´¥")
                continue
            
            # è¯»å–åŸå§‹æ–‡ä»¶
            content = reader.read_task_file(task_type, task_id, filename, decompress)
            
            if content is not None:
                print("âœ… åŸå§‹æ–‡ä»¶è¯»å–æˆåŠŸ!")
                if isinstance(content, str):
                    print(f"ğŸ“ åŸå§‹æ–‡ä»¶é•¿åº¦: {len(content)} å­—ç¬¦")
                else:
                    print(f"ğŸ“Š åŸå§‹æ–‡ä»¶å¤§å°: {len(content)} å­—èŠ‚")
                
                # ä¿å­˜åŸå§‹æ–‡ä»¶
                save_filename = _generate_save_filename(filename, task_id, args.output_type)
                local_path = f"{args.save_dir}/{task_type}/{task_id}/{save_filename}"
                
                success = _save_content_to_file(content, local_path)
                if success:
                    print(f"ğŸ’¾ åŸå§‹æ–‡ä»¶å·²ä¿å­˜åˆ°: {local_path}")
                    successfully_downloaded_files.append(filename)
            else:
                print("âŒ åŸå§‹æ–‡ä»¶è¯»å–å¤±è´¥æˆ–æ–‡ä»¶ä¸å­˜åœ¨")
    
    # å¦‚æœä¼˜åŒ–æ–¹æ³•æ²¡æœ‰æˆåŠŸè·å–è§£ææ–‡ä»¶ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•
    if not parse_file_downloaded and not args.info_only:
        print(f"\nğŸ”„ æ­¥éª¤3: ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•è·å–è§£ææ–‡ä»¶")
        print("-" * 60)
        
        # ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•è·å–è§£ææ–‡ä»¶ï¼ˆä»…ç¬¬ä¸€ä¸ªæ–‡ä»¶æ—¶æ‰§è¡Œï¼‰
        if files_to_process:
            filename = files_to_process[0]  # è§£ææ–‡ä»¶ä¸å…·ä½“åŸå§‹æ–‡ä»¶æ— å…³
            
            # ä½¿ç”¨æ–°æ–¹æ³•åŒæ—¶è¯»å–åŸå§‹æ–‡ä»¶å’Œè§£ææ–‡ä»¶ä¸­çš„è§£æéƒ¨åˆ†
            combined_content = reader.read_task_file_with_parse(task_type, task_id, filename, decompress)
            parse_content = combined_content.get('parse')
            
            if parse_content is not None:
                print("âœ… è§£ææ–‡ä»¶è¯»å–æˆåŠŸ!")
                if isinstance(parse_content, str):
                    print(f"ğŸ“ è§£ææ–‡ä»¶é•¿åº¦: {len(parse_content)} å­—ç¬¦")
                    
                    # JSONæ ¼å¼éªŒè¯å’Œæ£€æµ‹
                    if parse_content.strip().startswith('{') or parse_content.strip().startswith('['):
                        try:
                            parsed_data = json.loads(parse_content)
                            print(f"ğŸ“‹ JSONè§£ææˆåŠŸï¼Œç±»å‹: {type(parsed_data)}")
                        except json.JSONDecodeError:
                            print("âš ï¸  è§£ææ–‡ä»¶ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼")
                    
                    # æ˜¾ç¤ºé¢„è§ˆ
                    print(f"ğŸ” è§£ææ–‡ä»¶é¢„è§ˆ (å‰200å­—ç¬¦):")
                    print(parse_content[:200] + "..." if len(parse_content) > 200 else parse_content)
                else:
                    print(f"ğŸ“Š è§£ææ–‡ä»¶å¤§å°: {len(parse_content)} å­—èŠ‚")
                
                # ä¿å­˜è§£ææ–‡ä»¶
                parse_filename = _generate_save_filename("parse_result", task_id, "json")
                parse_local_path = f"{args.save_dir}/{task_type}/{task_id}/{parse_filename}"
                
                parse_success = _save_content_to_file(parse_content, parse_local_path)
                if parse_success:
                    print(f"ğŸ’¾ è§£ææ–‡ä»¶å·²ä¿å­˜åˆ°: {parse_local_path}")
                    parse_file_downloaded = True
            else:
                print("âŒ è§£ææ–‡ä»¶è¯»å–å¤±è´¥æˆ–æ–‡ä»¶ä¸å­˜åœ¨")
    
    # ç¬¬ä¸‰æ­¥ï¼šæ›´æ–°ä»»åŠ¡æ˜ å°„æ–‡ä»¶
    if (successfully_downloaded_files or parse_file_downloaded) and not args.info_only and not args.no_mapping:
        print("\n" + "=" * 80)
        print("æ­¥éª¤4ï¼šæ›´æ–°ä»»åŠ¡æ˜ å°„æ–‡ä»¶")
        print("=" * 80)
        
        # æ˜¾ç¤ºæ˜ å°„ä¿¡æ¯
        print_task_mapping_info(original_input, task_type, task_id, args.save_dir)
        
        # æ›´æ–°åŸå§‹æ–‡ä»¶æ˜ å°„
        mapping_success = update_task_mapping(original_input, task_type, task_id, args.save_dir)
        
        # åœ¨ --with-parse æ¨¡å¼ä¸‹ï¼Œè§£ææ–‡ä»¶å’ŒåŸå§‹æ–‡ä»¶åœ¨åŒä¸€ç›®å½•ï¼Œä¸éœ€è¦å•ç‹¬çš„æ˜ å°„
        if mapping_success:
            if parse_file_downloaded:
                print(f"âœ… æˆåŠŸä¸‹è½½åŸå§‹æ–‡ä»¶å’Œè§£ææ–‡ä»¶å¹¶æ›´æ–°æ˜ å°„")
                print(f"ğŸ“„ åŸå§‹æ–‡ä»¶æ•°é‡: {len(successfully_downloaded_files)}")
                print(f"ğŸ“„ è§£ææ–‡ä»¶: 1ä¸ª (ä¿å­˜åœ¨åŒä¸€ç›®å½•)")
                
                # æ˜¾ç¤ºä½¿ç”¨çš„æ–¹æ³•
                if parse_result and 'method_used' in parse_result:
                    method_name = "analysis_responseä¼˜åŒ–é“¾æ¥" if parse_result['method_used'] == 'analysis_response' else "Azureå­˜å‚¨"
                    print(f"ğŸš€ è§£ææ–‡ä»¶è·å–æ–¹å¼: {method_name}")
            else:
                print(f"âœ… æˆåŠŸä¸‹è½½åŸå§‹æ–‡ä»¶å¹¶æ›´æ–°æ˜ å°„")
                print(f"ğŸ“„ å·²ä¸‹è½½æ–‡ä»¶: {', '.join(successfully_downloaded_files)}")
                if not parse_file_downloaded:
                    print("âš ï¸  è§£ææ–‡ä»¶ä¸‹è½½å¤±è´¥")
    
    elif args.info_only:
        print(f"\nğŸ“‹ ä¿¡æ¯æŸ¥çœ‹æ¨¡å¼ï¼Œæœªä¸‹è½½æ–‡ä»¶")
        # æ˜¾ç¤ºè§£ææ–‡ä»¶ä¿¡æ¯ï¼ˆå¦‚æœæœªåœ¨ä¼˜åŒ–æ­¥éª¤ä¸­è·å–ï¼‰
        if not parse_file_downloaded:
            parse_reader = AzureResourceReader('collector0109')
            parse_files = parse_reader.list_parse_files(task_type, task_id)
            if parse_files:
                print(f"\nâœ… è§£ææ–‡ä»¶ä¿¡æ¯ (å…±{len(parse_files)}ä¸ª):")
                for parse_file in parse_files:
                    print(f"  ğŸ“„ {parse_file['name']}")
                    print(f"     ğŸ“Š å¤§å°: {parse_file['size']} å­—èŠ‚")
            else:
                print("\nâŒ æœªæ‰¾åˆ°è§£ææ–‡ä»¶")
                
    elif args.no_mapping:
        print(f"\nğŸ“‹ å·²ç¦ç”¨æ˜ å°„æ–‡ä»¶ç”Ÿæˆ")
    elif not successfully_downloaded_files and not parse_file_downloaded:
        print(f"\nâš ï¸  æ²¡æœ‰æ–‡ä»¶æˆåŠŸä¸‹è½½ï¼Œæœªæ›´æ–°æ˜ å°„")


def handle_parse_mode(reader: AzureResourceReader, job_id: str, task_id: str, 
                     args, original_input: str) -> None:
    """
    å¤„ç†è§£ææ¨¡å¼çš„æ–‡ä»¶è¯»å–
    
    Args:
        reader: Azureèµ„æºè¯»å–å™¨å®ä¾‹
        job_id: è¯·æ±‚åºåˆ—å·
        task_id: ä»»åŠ¡ID
        args: å‘½ä»¤è¡Œå‚æ•°
        original_input: åŸå§‹è¾“å…¥å‚æ•°
    """
    
    # å¦‚æœåªæ˜¯åˆ—å‡ºæ–‡ä»¶
    if args.list_jobs:
        print(f"ğŸ“‹ åˆ—å‡ºè§£ææ–‡ä»¶:")
        files = reader.list_parse_files(job_id, task_id)
        
        if files:
            print(f"âœ… æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶:")
            for file_info in files:
                print(f"  ğŸ“„ æ–‡ä»¶: {file_info['name']}")
                print(f"     ğŸ“Š å¤§å°: {file_info['size']} å­—èŠ‚")
                print(f"     ğŸ“… ä¿®æ”¹: {file_info['last_modified']}")
                print()
        else:
            print("âŒ æœªæ‰¾åˆ°æ–‡ä»¶")
        return
    
    # ç¡®å®šæ˜¯å¦éœ€è¦è§£å‹ç¼©
    decompress = args.output_type in ['html', 'txt', 'json']
    
    # ç”¨äºè®°å½•æ˜¯å¦æœ‰æ–‡ä»¶æˆåŠŸä¸‹è½½
    successfully_downloaded_files = []
    
    # ç¡®å®šè¦å¤„ç†çš„æ–‡ä»¶åˆ—è¡¨
    if args.files is None:
        # è§£ææ¨¡å¼é»˜è®¤è‡ªåŠ¨æŸ¥æ‰¾JSONæ–‡ä»¶
        files_to_process = [None]  # Noneè¡¨ç¤ºè‡ªåŠ¨æŸ¥æ‰¾
        print(f"ğŸ“„ è§£ææ¨¡å¼ï¼šè‡ªåŠ¨æŸ¥æ‰¾JSONæ–‡ä»¶")
    else:
        files_to_process = args.files
        print(f"ğŸ“„ ç”¨æˆ·æŒ‡å®šæ–‡ä»¶: {', '.join(files_to_process)}")
    
    # å¤„ç†æ¯ä¸ªæ–‡ä»¶
    for filename in files_to_process:
        if filename is None:
            print(f"\nğŸ“„ è‡ªåŠ¨æŸ¥æ‰¾è§£ææ–‡ä»¶:")
        else:
            print(f"\nğŸ“„ å¤„ç†æ–‡ä»¶: {filename}")
        print("-" * 60)
        
        # ä»…æ˜¾ç¤ºä¿¡æ¯
        if args.info_only:
            if filename is None:
                files = reader.list_parse_files(job_id, task_id)
                if files:
                    print(f"âœ… æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶:")
                    for file_info in files:
                        print(f"  ğŸ“„ {file_info['name']}")
                        print(f"     ğŸ“Š å¤§å°: {file_info['size']} å­—èŠ‚")
                else:
                    print("âŒ æœªæ‰¾åˆ°æ–‡ä»¶")
            else:
                # æ„é€ å®Œæ•´è·¯å¾„ç”¨äºè·å–ä¿¡æ¯
                blob_path = f"parse/{job_id}/{task_id}/{filename}"
                blob_info = reader.get_blob_info('parse', blob_path)
                
                if blob_info:
                    print(f"âœ… æ–‡ä»¶ä¿¡æ¯:")
                    print(f"  ğŸ“Š å¤§å°: {blob_info['size_mb']} MB")
                    print(f"  ğŸ“… ä¿®æ”¹æ—¶é—´: {blob_info['last_modified']}")
                    print(f"  ğŸ”— URL: {blob_info['url']}")
                else:
                    print("âŒ æ–‡ä»¶ä¸å­˜åœ¨æˆ–è·å–ä¿¡æ¯å¤±è´¥")
            continue
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        content = reader.read_parse_file(job_id, task_id, filename, decompress)
        
        if content is None:
            print("âŒ è¯»å–å¤±è´¥æˆ–æ–‡ä»¶ä¸å­˜åœ¨")
            continue
        
        print("âœ… è¯»å–æˆåŠŸ!")
        
        # æ˜¾ç¤ºå†…å®¹ä¿¡æ¯
        if isinstance(content, str):
            print(f"ğŸ“ å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
            
            # å¦‚æœæ˜¯JSONç±»å‹ï¼Œå°è¯•è§£æå’Œæ ¼å¼åŒ–
            if args.output_type == 'json':
                try:
                    if content.strip().startswith('{') or content.strip().startswith('['):
                        parsed_data = json.loads(content)
                        print(f"ğŸ“‹ JSONè§£ææˆåŠŸï¼Œç±»å‹: {type(parsed_data)}")
                        if isinstance(parsed_data, dict):
                            print(f"ğŸ”‘ JSONé”®: {list(parsed_data.keys())}")
                        elif isinstance(parsed_data, list):
                            print(f"ğŸ“Š JSONæ•°ç»„é•¿åº¦: {len(parsed_data)}")
                except json.JSONDecodeError:
                    print("âš ï¸  å†…å®¹ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼")
            
            # æ˜¾ç¤ºé¢„è§ˆ
            print(f"ğŸ” å†…å®¹é¢„è§ˆ (å‰200å­—ç¬¦):")
            print(content[:200] + "..." if len(content) > 200 else content)
            
        else:
            print(f"ğŸ“Š æ•°æ®é•¿åº¦: {len(content)} å­—èŠ‚")
        
        # ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶
        actual_filename = filename if filename else "auto_found"
        
        # æ£€æµ‹å†…å®¹æ˜¯å¦ä¸ºJSONæ ¼å¼ï¼Œå¦‚æœæ˜¯åˆ™å¼ºåˆ¶ä½¿ç”¨jsonæ‰©å±•å
        output_type_to_use = args.output_type
        if isinstance(content, str) and (content.strip().startswith('{') or content.strip().startswith('[')):
            try:
                json.loads(content)  # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆJSON
                output_type_to_use = "json"  # å¼ºåˆ¶ä½¿ç”¨jsonæ ¼å¼
                print("ğŸ” æ£€æµ‹åˆ°JSONå†…å®¹ï¼Œå°†ä¿å­˜ä¸º.jsonæ–‡ä»¶")
            except json.JSONDecodeError:
                pass  # ä¸æ˜¯æœ‰æ•ˆJSONï¼Œä¿æŒåŸè¾“å‡ºç±»å‹
        
        save_filename = _generate_save_filename(actual_filename, task_id, output_type_to_use)
        local_path = f"{args.save_dir}/parse/{job_id}/{task_id}/{save_filename}"
        
        success = _save_content_to_file(content, local_path)
        if success:
            print(f"ğŸ’¾ æ–‡ä»¶å·²ä¿å­˜åˆ°: {local_path}")
            successfully_downloaded_files.append(actual_filename)
        else:
            print("âŒ ä¿å­˜å¤±è´¥")
    
    # æ›´æ–°ä»»åŠ¡æ˜ å°„æ–‡ä»¶ï¼ˆå¦‚æœæœ‰æ–‡ä»¶æˆåŠŸä¸‹è½½ä¸”æœªç¦ç”¨æ˜ å°„ï¼‰
    if successfully_downloaded_files and not args.info_only and not args.no_mapping:
        print("\n" + "=" * 80)
        print("ç¬¬ä¸‰æ­¥ï¼šæ›´æ–°ä»»åŠ¡æ˜ å°„æ–‡ä»¶")
        print("=" * 80)
        
        # æ˜¾ç¤ºæ˜ å°„ä¿¡æ¯
        print_parse_mapping_info(original_input, job_id, task_id, args.save_dir)
        
        # æ›´æ–°æ˜ å°„
        mapping_success = update_parse_mapping(original_input, job_id, task_id, args.save_dir)
        
        if mapping_success:
            print(f"âœ… æˆåŠŸä¸‹è½½ {len(successfully_downloaded_files)} ä¸ªæ–‡ä»¶å¹¶æ›´æ–°æ˜ å°„")
            print(f"ğŸ“„ å·²ä¸‹è½½æ–‡ä»¶: {', '.join(successfully_downloaded_files)}")
        else:
            print("âš ï¸  æ–‡ä»¶ä¸‹è½½æˆåŠŸä½†æ˜ å°„æ›´æ–°å¤±è´¥")
    
    elif args.info_only:
        print(f"\nğŸ“‹ ä¿¡æ¯æŸ¥çœ‹æ¨¡å¼ï¼Œæœªä¸‹è½½æ–‡ä»¶")
    elif args.no_mapping:
        print(f"\nğŸ“‹ å·²ç¦ç”¨æ˜ å°„æ–‡ä»¶ç”Ÿæˆ")
    elif not successfully_downloaded_files:
        print(f"\nâš ï¸  æ²¡æœ‰æ–‡ä»¶æˆåŠŸä¸‹è½½ï¼Œæœªæ›´æ–°æ˜ å°„")


def print_parse_mapping_info(original_input: str, job_id: str, task_id: str, 
                            save_dir: str = 'data/output') -> None:
    """
    æ‰“å°è§£ææ–‡ä»¶æ˜ å°„ä¿¡æ¯
    
    Args:
        original_input: åŸå§‹è¾“å…¥å‚æ•°
        job_id: è¯·æ±‚åºåˆ—å·ï¼ˆå®é™…æ˜¯ä»»åŠ¡ç±»å‹ï¼‰
        task_id: ä»»åŠ¡ID
        save_dir: ä¿å­˜ç›®å½•
    """
    relative_path = f"./parse/{job_id}/{task_id}/"
    full_path = f"{save_dir}/parse/{job_id}/{task_id}/"
    
    print(f"\nğŸ“‹ è§£ææ–‡ä»¶æ˜ å°„ä¿¡æ¯:")
    print(f"  ğŸ” è¾“å…¥å‚æ•°: {original_input}")
    print(f"  ğŸ“ ç›¸å¯¹è·¯å¾„: {relative_path}")
    print(f"  ğŸ“„ æ˜ å°„æ–‡ä»¶: {save_dir}/task_mapping.json")


def update_parse_mapping(original_input: str, job_id: str, task_id: str, 
                        save_dir: str = 'data/output') -> bool:
    """
    æ›´æ–°è§£ææ–‡ä»¶æ˜ å°„åˆ°æ•°æ®åº“ï¼ˆä»…ä½¿ç”¨æ•°æ®åº“ï¼Œparseæ–‡ä»¶æš‚ä¸æ”¯æŒæ•°æ®åº“å­˜å‚¨ï¼‰
    
    Args:
        original_input: åŸå§‹è¾“å…¥å‚æ•°
        job_id: è¯·æ±‚åºåˆ—å·ï¼ˆå®é™…æ˜¯ä»»åŠ¡ç±»å‹ï¼‰  
        task_id: ä»»åŠ¡ID
        save_dir: ä¿å­˜ç›®å½•
        
    Returns:
        bool: æ›´æ–°æˆåŠŸè¿”å›True
    """
    logger.warning(f"âš ï¸  è§£ææ–‡ä»¶æ˜ å°„æš‚ä¸æ”¯æŒæ•°æ®åº“å­˜å‚¨ï¼Œå°†è·³è¿‡æ˜ å°„æ›´æ–°: {original_input}")
    logger.info(f"ğŸ” è§£ææ–‡ä»¶ä¿å­˜è·¯å¾„: {save_dir}/parse/{job_id}/{task_id}/")
    return True  # ç›´æ¥è¿”å›æˆåŠŸï¼Œä¸è¿›è¡Œæ˜ å°„æ“ä½œ


if __name__ == '__main__':
    main() 