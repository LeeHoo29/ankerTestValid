#!/usr/bin/env python3
"""
collector0109 å­˜å‚¨è´¦æˆ·å…¨é¢æµ‹è¯•
ä¿®å¤APIé—®é¢˜å¹¶å…¨é¢æœç´¢æŒ‡å®štask_idçš„æ–‡ä»¶
æ”¯æŒå‘½ä»¤è¡Œå‚æ•°è¾“å…¥ä»»åŠ¡ç±»å‹å’Œä»»åŠ¡ID
"""
import sys
import os
import argparse
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))

from src.azure_resource_reader import AzureResourceReader
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def test_collector0109_connection():
    """æµ‹è¯•collector0109è¿æ¥"""
    print("ğŸ” æµ‹è¯•collector0109å­˜å‚¨è´¦æˆ·è¿æ¥")
    print("=" * 60)
    
    try:
        reader = AzureResourceReader('collector0109')
        print("âœ… collector0109è¿æ¥æˆåŠŸ")
        return reader
    except Exception as e:
        print(f"âŒ collector0109è¿æ¥å¤±è´¥: {e}")
        return None


def test_specific_paths_fixed(reader, task_type, task_id):
    """æµ‹è¯•ç‰¹å®šè·¯å¾„ï¼ˆä¿®å¤ç‰ˆæœ¬ï¼‰"""
    print(f"\nğŸ” æµ‹è¯•ç‰¹å®šè·¯å¾„ï¼ˆä¿®å¤ç‰ˆæœ¬ï¼‰")
    print("=" * 60)
    
    path_variations = [
        f"parse/parse/{task_type}/{task_id}/",
        f"parse/parse/{task_type}/{task_id}",
        f"parse/{task_type}/{task_id}/",
        f"parse/{task_type}/{task_id}",
    ]
    
    container_client = reader.blob_service_client.get_container_client('parse')
    results = {}
    
    for prefix in path_variations:
        try:
            print(f"  æœç´¢è·¯å¾„: {prefix}")
            # ä¿®å¤ï¼šä¸ä½¿ç”¨max_resultså‚æ•°
            blobs = []
            blob_iter = container_client.list_blobs(name_starts_with=prefix)
            
            # æ‰‹åŠ¨é™åˆ¶ç»“æœæ•°é‡
            count = 0
            for blob in blob_iter:
                blobs.append(blob)
                count += 1
                if count >= 10:  # é™åˆ¶ä¸º10ä¸ªç»“æœ
                    break
            
            results[prefix] = blobs
            print(f"    ç»“æœ: {len(blobs)} ä¸ªæ–‡ä»¶")
            
            if blobs:
                for i, blob in enumerate(blobs[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
                    print(f"      ğŸ“„ {blob.name}")
                    print(f"         ğŸ“Š å¤§å°: {blob.size} å­—èŠ‚")
                if len(blobs) > 3:
                    print(f"      ... è¿˜æœ‰ {len(blobs) - 3} ä¸ªæ–‡ä»¶")
            print()
            
        except Exception as e:
            print(f"    é”™è¯¯: {e}")
            results[prefix] = []
    
    return results


def search_task_id_broadly(reader, task_id):
    """å¹¿æ³›æœç´¢åŒ…å«task_idçš„æ–‡ä»¶"""
    print(f"\nğŸ” å¹¿æ³›æœç´¢åŒ…å«task_id {task_id}çš„æ–‡ä»¶")
    print("=" * 60)
    
    try:
        container_client = reader.blob_service_client.get_container_client('parse')
        
        # æœç´¢ä¸åŒçš„å¯èƒ½è·¯å¾„å‰ç¼€
        search_prefixes = [
            "parse/parse/",
            "parse/",
            "",  # æœç´¢æ ¹ç›®å½•
        ]
        
        all_matches = []
        
        for prefix in search_prefixes:
            print(f"  æœç´¢å‰ç¼€: '{prefix}'")
            try:
                blob_iter = container_client.list_blobs(name_starts_with=prefix)
                
                matches = []
                count = 0
                for blob in blob_iter:
                    if task_id in blob.name:
                        matches.append(blob)
                        print(f"    âœ… æ‰¾åˆ°: {blob.name}")
                        print(f"       ğŸ“Š å¤§å°: {blob.size} å­—èŠ‚")
                        print(f"       ğŸ“… ä¿®æ”¹: {blob.last_modified}")
                        print()
                    
                    count += 1
                    # é™åˆ¶æœç´¢èŒƒå›´ï¼Œé¿å…è¶…æ—¶
                    if count >= 1000:
                        print(f"    âš ï¸  å·²æœç´¢ {count} ä¸ªæ–‡ä»¶ï¼Œåœæ­¢æœç´¢æ­¤å‰ç¼€")
                        break
                
                all_matches.extend(matches)
                print(f"    å‰ç¼€ '{prefix}' æ‰¾åˆ° {len(matches)} ä¸ªåŒ¹é…æ–‡ä»¶")
                print()
                
            except Exception as e:
                print(f"    å‰ç¼€ '{prefix}' æœç´¢å¤±è´¥: {e}")
                print()
        
        return all_matches
        
    except Exception as e:
        print(f"âŒ å¹¿æ³›æœç´¢å¤±è´¥: {e}")
        return []


def search_task_type_files(reader, task_type):
    """æœç´¢æŒ‡å®šä»»åŠ¡ç±»å‹ç›¸å…³çš„æ‰€æœ‰æ–‡ä»¶"""
    print(f"\nğŸ” æœç´¢{task_type}ç›¸å…³æ–‡ä»¶")
    print("=" * 60)
    
    try:
        container_client = reader.blob_service_client.get_container_client('parse')
        
        search_prefixes = [
            f"parse/parse/{task_type}/",
            f"parse/{task_type}/",
            f"{task_type}/",
        ]
        
        all_matches = []
        
        for prefix in search_prefixes:
            print(f"  æœç´¢å‰ç¼€: '{prefix}'")
            try:
                blob_iter = container_client.list_blobs(name_starts_with=prefix)
                
                matches = []
                count = 0
                for blob in blob_iter:
                    matches.append(blob)
                    if count < 5:  # åªæ˜¾ç¤ºå‰5ä¸ª
                        print(f"    ğŸ“„ {blob.name}")
                        print(f"       ğŸ“Š å¤§å°: {blob.size} å­—èŠ‚")
                        print(f"       ğŸ“… ä¿®æ”¹: {blob.last_modified}")
                        print()
                    
                    count += 1
                    # é™åˆ¶æœç´¢æ•°é‡
                    if count >= 20:
                        break
                
                all_matches.extend(matches)
                print(f"    å‰ç¼€ '{prefix}' æ‰¾åˆ° {len(matches)} ä¸ªæ–‡ä»¶")
                if len(matches) > 5:
                    print(f"    ï¼ˆåªæ˜¾ç¤ºäº†å‰5ä¸ªæ–‡ä»¶ï¼‰")
                print()
                
            except Exception as e:
                print(f"    å‰ç¼€ '{prefix}' æœç´¢å¤±è´¥: {e}")
                print()
        
        return all_matches
        
    except Exception as e:
        print(f"âŒ {task_type}æœç´¢å¤±è´¥: {e}")
        return []


def analyze_path_structure(reader):
    """åˆ†æparseå®¹å™¨çš„è·¯å¾„ç»“æ„"""
    print(f"\nğŸ” åˆ†æparseå®¹å™¨çš„è·¯å¾„ç»“æ„")
    print("=" * 60)
    
    try:
        container_client = reader.blob_service_client.get_container_client('parse')
        
        # è·å–æ ¹ç›®å½•ä¸‹çš„å‰å‡ ä¸ªæ–‡ä»¶æ¥åˆ†æç»“æ„
        blob_iter = container_client.list_blobs()
        
        print("  æ ¹ç›®å½•ä¸‹çš„æ–‡ä»¶ç»“æ„æ ·ä¾‹:")
        count = 0
        path_patterns = set()
        
        for blob in blob_iter:
            if count < 10:
                print(f"    ğŸ“„ {blob.name}")
                
                # åˆ†æè·¯å¾„æ¨¡å¼
                parts = blob.name.split('/')
                if len(parts) >= 3:
                    pattern = '/'.join(parts[:3]) + '/...'
                    path_patterns.add(pattern)
            
            count += 1
            if count >= 50:  # åªåˆ†æå‰50ä¸ªæ–‡ä»¶
                break
        
        print(f"\n  å‘ç°çš„è·¯å¾„æ¨¡å¼:")
        for pattern in sorted(path_patterns):
            print(f"    ğŸ” {pattern}")
        
        print(f"\n  æ€»å…±åˆ†æäº† {count} ä¸ªæ–‡ä»¶")
        
    except Exception as e:
        print(f"âŒ è·¯å¾„ç»“æ„åˆ†æå¤±è´¥: {e}")


def test_download_sample_file(reader, task_type, task_id, blobs):
    """æµ‹è¯•ä¸‹è½½ç¤ºä¾‹æ–‡ä»¶"""
    if not blobs:
        print("\nâŒ æ²¡æœ‰æ–‡ä»¶å¯ä¾›ä¸‹è½½æµ‹è¯•")
        return
        
    print(f"\nğŸ” æµ‹è¯•ä¸‹è½½ç¤ºä¾‹æ–‡ä»¶")
    print("=" * 60)
    
    # é€‰æ‹©ç¬¬ä¸€ä¸ªæ–‡ä»¶è¿›è¡Œä¸‹è½½æµ‹è¯•
    sample_blob = blobs[0]
    print(f"  é€‰æ‹©æ–‡ä»¶: {sample_blob.name}")
    
    try:
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path(f"data/test_output/{task_type}/{task_id}")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # è·å–æ–‡ä»¶å
        file_name = Path(sample_blob.name).name
        output_path = output_dir / file_name
        
        # ä¸‹è½½æ–‡ä»¶
        container_client = reader.blob_service_client.get_container_client('parse')
        blob_client = container_client.get_blob_client(sample_blob.name)
        
        with open(output_path, 'wb') as f:
            download_stream = blob_client.download_blob()
            f.write(download_stream.readall())
        
        print(f"âœ… æ–‡ä»¶ä¸‹è½½æˆåŠŸ: {output_path}")
        print(f"   æ–‡ä»¶å¤§å°: {output_path.stat().st_size} å­—èŠ‚")
        
        # æ˜¾ç¤ºæ–‡ä»¶å†…å®¹çš„å‰å‡ è¡Œï¼ˆå¦‚æœæ˜¯æ–‡æœ¬æ–‡ä»¶ï¼‰
        try:
            with open(output_path, 'r', encoding='utf-8') as f:
                content = f.read(500)  # è¯»å–å‰500ä¸ªå­—ç¬¦
                print(f"   æ–‡ä»¶å†…å®¹é¢„è§ˆ:")
                print("   " + "-" * 40)
                for line in content.split('\n')[:5]:  # æ˜¾ç¤ºå‰5è¡Œ
                    print(f"   {line}")
                if len(content) >= 500:
                    print("   ...")
                print("   " + "-" * 40)
        except:
            print("   æ–‡ä»¶å†…å®¹æ— æ³•ä»¥æ–‡æœ¬å½¢å¼æ˜¾ç¤ºï¼ˆå¯èƒ½æ˜¯äºŒè¿›åˆ¶æ–‡ä»¶ï¼‰")
            
        return output_path
        
    except Exception as e:
        print(f"âŒ ä¸‹è½½æ–‡ä»¶å¤±è´¥: {e}")
        return None


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='collector0109 å­˜å‚¨è´¦æˆ·å…¨é¢æµ‹è¯•',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # æµ‹è¯•æŒ‡å®šçš„ä»»åŠ¡ç±»å‹å’Œä»»åŠ¡ID
  python3 tests/test_collector0109_comprehensive.py AmazonReviewStarJob 1910598939612549120
  
  # æµ‹è¯•å…¶ä»–ä»»åŠ¡ç±»å‹
  python3 tests/test_collector0109_comprehensive.py AmazonListingJob 1925464883027513344
  
  # åªä¸‹è½½ç¤ºä¾‹æ–‡ä»¶ï¼ˆä¸è¿›è¡Œå¹¿æ³›æœç´¢ï¼‰
  python3 tests/test_collector0109_comprehensive.py AmazonReviewStarJob 1910598939612549120 --download-only
        """
    )
    
    parser.add_argument('task_type', 
                       help='ä»»åŠ¡ç±»å‹ï¼Œå¦‚: AmazonReviewStarJob, AmazonListingJob')
    parser.add_argument('task_id', 
                       help='ä»»åŠ¡IDï¼ˆé•¿æ•°å­—ä¸²ï¼‰ï¼Œå¦‚: 1910598939612549120')
    parser.add_argument('--download-only', '-d',
                       action='store_true',
                       help='åªä¸‹è½½æ‰¾åˆ°çš„æ–‡ä»¶ï¼Œä¸è¿›è¡Œå¹¿æ³›æœç´¢ï¼ˆæ›´å¿«ï¼‰')
    parser.add_argument('--no-download', '-n',
                       action='store_true',
                       help='ä¸ä¸‹è½½æ–‡ä»¶ï¼Œåªæ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯')
    
    args = parser.parse_args()
    
    task_type = args.task_type
    task_id = args.task_id
    
    print("ğŸ§ª collector0109 å­˜å‚¨è´¦æˆ·å…¨é¢æµ‹è¯•")
    print("=" * 80)
    print(f"ç›®æ ‡ä»»åŠ¡ç±»å‹: {task_type}")
    print(f"ç›®æ ‡ä»»åŠ¡ID: {task_id}")
    print(f"é¢„æœŸè·¯å¾„: parse/parse/{task_type}/{task_id}/")
    
    # 1. æµ‹è¯•è¿æ¥
    reader = test_collector0109_connection()
    if not reader:
        return
    
    # 2. åˆ†æè·¯å¾„ç»“æ„ï¼ˆå¦‚æœä¸æ˜¯åªä¸‹è½½æ¨¡å¼ï¼‰
    if not args.download_only:
        analyze_path_structure(reader)
    
    # 3. æµ‹è¯•ç‰¹å®šè·¯å¾„ï¼ˆä¿®å¤ç‰ˆæœ¬ï¼‰
    path_results = test_specific_paths_fixed(reader, task_type, task_id)
    
    # 4. å¹¿æ³›æœç´¢task_idï¼ˆå¦‚æœä¸æ˜¯åªä¸‹è½½æ¨¡å¼ï¼‰
    task_matches = []
    if not args.download_only:
        task_matches = search_task_id_broadly(reader, task_id)
    
    # 5. æœç´¢ä»»åŠ¡ç±»å‹ç›¸å…³æ–‡ä»¶ï¼ˆå¦‚æœä¸æ˜¯åªä¸‹è½½æ¨¡å¼ï¼‰
    type_matches = []
    if not args.download_only:
        type_matches = search_task_type_files(reader, task_type)
    
    # 6. å¦‚æœæ‰¾åˆ°æ–‡ä»¶ä¸”æœªç¦ç”¨ä¸‹è½½ï¼Œæµ‹è¯•ä¸‹è½½
    found_files = []
    for path, blobs in path_results.items():
        found_files.extend(blobs)
    
    if found_files and not args.no_download:
        test_download_sample_file(reader, task_type, task_id, found_files)
    
    # æ€»ç»“
    print("\n" + "=" * 80)
    print("ğŸ“Š æµ‹è¯•æ€»ç»“")
    print("=" * 80)
    print(f"ç›®æ ‡è·¯å¾„: parse/parse/{task_type}/{task_id}/")
    
    # æ˜¾ç¤ºå„è·¯å¾„å˜ä½“çš„ç»“æœ
    print("\nğŸ¯ ç‰¹å®šè·¯å¾„æµ‹è¯•ç»“æœ:")
    total_found = 0
    for path, blobs in path_results.items():
        print(f"  {path}: {len(blobs)} ä¸ªæ–‡ä»¶")
        total_found += len(blobs)
    
    if not args.download_only:
        print(f"\nğŸ” task_idæœç´¢ç»“æœ: {len(task_matches)} ä¸ªåŒ¹é…æ–‡ä»¶")
        print(f"ğŸ” {task_type}æœç´¢ç»“æœ: {len(type_matches)} ä¸ªç›¸å…³æ–‡ä»¶")
        
        if task_matches:
            print("\nâœ… æ‰¾åˆ°åŒ…å«task_idçš„æ–‡ä»¶:")
            for match in task_matches:
                print(f"  ğŸ“„ {match.name}")
    
    if total_found > 0 or task_matches:
        print("\nâœ… ç¡®è®¤ï¼šæ‰¾åˆ°äº†ç›¸å…³æ–‡ä»¶ï¼")
        if found_files:
            print("ğŸ¯ åœ¨ç‰¹å®šè·¯å¾„ä¸‹æ‰¾åˆ°çš„æ–‡ä»¶:")
            for blob in found_files[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"  ğŸ“„ {blob.name}")
            if len(found_files) > 5:
                print(f"  ... è¿˜æœ‰ {len(found_files) - 5} ä¸ªæ–‡ä»¶")
    else:
        print("\nâŒ æœªæ‰¾åˆ°æŒ‡å®štask_idçš„æ–‡ä»¶")
        print("ğŸ’¡ å»ºè®®æ£€æŸ¥:")
        print("   1. task_idæ˜¯å¦æ­£ç¡®")
        print("   2. æ–‡ä»¶æ˜¯å¦å­˜åœ¨äºå…¶ä»–è·¯å¾„ç»“æ„ä¸­")
        print("   3. æ˜¯å¦éœ€è¦æœç´¢å…¶ä»–å®¹å™¨")


if __name__ == '__main__':
    main() 